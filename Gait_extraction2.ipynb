{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/Dataset #8/'  #change dir to your project folder"
      ],
      "metadata": {
        "id": "B9CIstyEsI47"
      },
      "id": "B9CIstyEsI47",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/Dataset #8/Dataset #8/'  #change dir to your project folder"
      ],
      "metadata": {
        "id": "_z1nCj_D4DdM"
      },
      "id": "_z1nCj_D4DdM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "b56e19e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b56e19e8",
        "outputId": "164ae9d5-428d-4fba-87f4-0c2575750f1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1022, 6, 1024, 1)\n",
            "(1022, 1024)\n",
            "(332, 6, 1024, 1)\n",
            "(332, 1024)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import preprocessing\n",
        "from tensorflow.keras import layers ,models\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def read_data(data_path):\n",
        "    data = []\n",
        "    file_names = os.listdir(data_path)\n",
        "    file_names.sort(key=lambda x:int(x[:-4]))\n",
        "    for file_name in file_names:\n",
        "        file_path = os.path.join(data_path, file_name)\n",
        "        signal_data = np.loadtxt(file_path)\n",
        "        data.append(signal_data)\n",
        "    data = np.array(data).transpose(0, 2, 1)\n",
        "    d_shape = data.shape\n",
        "    return data.reshape(d_shape[0], 1, d_shape[1], d_shape[2])\n",
        "\n",
        "\n",
        "def read_label(data_path):\n",
        "    data = []\n",
        "    file_names = os.listdir(data_path)\n",
        "    file_names.sort(key=lambda x:int(x[:-10]))\n",
        "    for file_name in file_names:\n",
        "        file_path = os.path.join(data_path, file_name)\n",
        "        signal_data = np.loadtxt(file_path)\n",
        "        data.append(signal_data)\n",
        "    return np.array(data)\n",
        "\n",
        "\n",
        "\n",
        "train_data_path = 'gdrive/My Drive/Dataset #8/Dataset #8/train/train_data'\n",
        "train_label_path = 'gdrive/My Drive/Dataset #8/Dataset #8/train/train_label'\n",
        "test_data_path = 'gdrive/My Drive/Dataset #8/Dataset #8/test/test_data'\n",
        "test_label_path = 'gdrive/My Drive/Dataset #8/Dataset #8/test/test_label'\n",
        "\n",
        "train_data = read_data(train_data_path).transpose(0, 2, 3, 1) # 519\n",
        "train_label = read_label(train_label_path)\n",
        "test_data = read_data(test_data_path).transpose(0, 2, 3, 1) # 519\n",
        "test_label = read_label(test_label_path)\n",
        "\n",
        "print(train_data.shape)\n",
        "print(train_label.shape)\n",
        "print(test_data.shape)\n",
        "print(test_label.shape)\n",
        "\n",
        "num_classes = len(np.unique(train_label))\n",
        "num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "1e695b73",
      "metadata": {
        "id": "1e695b73"
      },
      "outputs": [],
      "source": [
        "idx = np.random.permutation(len(train_data))\n",
        "train_data = train_data[idx]\n",
        "train_label = train_label[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "9c4f5685",
      "metadata": {
        "id": "9c4f5685"
      },
      "outputs": [],
      "source": [
        "train_label[train_label == -1] = 0\n",
        "test_label[test_label == -1] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "8d0d00b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d0d00b9",
        "outputId": "7d0dda24-f0dc-4688-be23-d7f66311ab26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 6, 1024, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 6, 1024, 1)  4           ['input_3[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 6, 1024, 64)  1088        ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 6, 1024, 64)  256        ['conv2d_26[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " re_lu_24 (ReLU)                (None, 6, 1024, 64)  0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 6, 1024, 64)  8256        ['re_lu_24[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 6, 1024, 64)  256        ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " re_lu_25 (ReLU)                (None, 6, 1024, 64)  0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling2d_4 (MaxPooling2D)  (None, 6, 512, 64)  0           ['re_lu_25[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 6, 512, 64)  256         ['max_pooling2d_4[0][0]']        \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 6, 512, 128)  131200      ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 6, 512, 128)  512        ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " re_lu_26 (ReLU)                (None, 6, 512, 128)  0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 6, 512, 128)  262272      ['re_lu_26[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 6, 512, 128)  512        ['conv2d_29[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " re_lu_27 (ReLU)                (None, 6, 512, 128)  0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling2d_5 (MaxPooling2D)  (None, 6, 256, 128)  0          ['re_lu_27[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 6, 256, 128)  512        ['max_pooling2d_5[0][0]']        \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 6, 256, 256)  524544      ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 6, 256, 256)  1024       ['conv2d_30[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " re_lu_28 (ReLU)                (None, 6, 256, 256)  0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 6, 256, 256)  1048832     ['re_lu_28[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 6, 256, 256)  1024       ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " re_lu_29 (ReLU)                (None, 6, 256, 256)  0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 6, 256, 256)  1048832     ['re_lu_29[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 6, 256, 256)  1024       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " re_lu_30 (ReLU)                (None, 6, 256, 256)  0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_transpose_4 (Conv2DTran  (None, 6, 512, 128)  65664      ['re_lu_30[0][0]']               \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 6, 512, 256)  0           ['conv2d_transpose_4[0][0]',     \n",
            "                                                                  're_lu_27[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 6, 512, 256)  1024       ['concatenate_4[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 6, 512, 128)  524416      ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 6, 512, 128)  512        ['conv2d_33[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " re_lu_31 (ReLU)                (None, 6, 512, 128)  0           ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 6, 512, 128)  262272      ['re_lu_31[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 6, 512, 128)  512        ['conv2d_34[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " re_lu_32 (ReLU)                (None, 6, 512, 128)  0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_transpose_5 (Conv2DTran  (None, 6, 1024, 64)  16448      ['re_lu_32[0][0]']               \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 6, 1024, 128  0           ['re_lu_25[0][0]',               \n",
            "                                )                                 'conv2d_transpose_5[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_49 (BatchN  (None, 6, 1024, 128  512        ['concatenate_5[0][0]']          \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 6, 1024, 64)  131136      ['batch_normalization_49[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_50 (BatchN  (None, 6, 1024, 64)  256        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " re_lu_33 (ReLU)                (None, 6, 1024, 64)  0           ['batch_normalization_50[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 6, 1024, 64)  65600       ['re_lu_33[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_51 (BatchN  (None, 6, 1024, 64)  256        ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " re_lu_34 (ReLU)                (None, 6, 1024, 64)  0           ['batch_normalization_51[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1024, 256  98560       ['re_lu_34[0][0]']               \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_52 (BatchN  (None, 1, 1024, 256  1024       ['conv2d_37[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " re_lu_35 (ReLU)                (None, 1, 1024, 256  0           ['batch_normalization_52[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1024, 1)   257         ['re_lu_35[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_53 (BatchN  (None, 1, 1024, 1)  4           ['conv2d_38[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.reshape_2 (TFOpLambda)      (None, 1024)         0           ['batch_normalization_53[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,198,857\n",
            "Trainable params: 4,194,117\n",
            "Non-trainable params: 4,740\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def make_model(input_shape):\n",
        "    input_layer = keras.layers.Input(input_shape)\n",
        "    \n",
        "    conv = keras.layers.BatchNormalization()(input_layer)\n",
        "    conv1_1 = keras.layers.Conv2D(filters=64, kernel_size=[1, 16], padding=\"same\")(conv)\n",
        "    conv1_1 = keras.layers.BatchNormalization()(conv1_1)\n",
        "    conv1_1 = keras.layers.ReLU()(conv1_1)\n",
        "\n",
        "    conv1_2 = keras.layers.Conv2D(filters=64, kernel_size=[1, 2], padding=\"same\")(conv1_1)\n",
        "    conv1_2 = keras.layers.BatchNormalization()(conv1_2)\n",
        "    conv1_2 = keras.layers.ReLU()(conv1_2)\n",
        "    \n",
        "    conv2_1= keras.layers.MaxPooling2D(pool_size=[1, 16], strides=[1,2], padding=\"same\")(conv1_2)\n",
        "    conv2_1 = keras.layers.BatchNormalization()(conv2_1)\n",
        "    \n",
        "    conv2_2 = keras.layers.Conv2D(filters=128, kernel_size=[1, 16], padding=\"same\")(conv2_1)\n",
        "    conv2_2 = keras.layers.BatchNormalization()(conv2_2)\n",
        "    conv2_2 = keras.layers.ReLU()(conv2_2)\n",
        "    \n",
        "    conv2_3 = keras.layers.Conv2D(filters=128, kernel_size=[1, 16], padding=\"same\")(conv2_2)\n",
        "    conv2_3 = keras.layers.BatchNormalization()(conv2_3)\n",
        "    conv2_3 = keras.layers.ReLU()(conv2_3)\n",
        "    \n",
        "    conv3_1 = keras.layers.MaxPooling2D(pool_size=(1, 2), strides=[1,2], padding=\"same\", data_format=None)(conv2_3)\n",
        "    conv3_1 = keras.layers.BatchNormalization()(conv3_1)\n",
        "    \n",
        "    conv3_2 = keras.layers.Conv2D(filters=256, kernel_size=[1, 16], padding=\"same\")(conv3_1)\n",
        "    conv3_2 = keras.layers.BatchNormalization()(conv3_2)\n",
        "    conv3_2 = keras.layers.ReLU()(conv3_2)\n",
        "    \n",
        "    conv3_3 = keras.layers.Conv2D(filters=256, kernel_size=[1, 16], padding=\"same\")(conv3_2)\n",
        "    conv3_3 = keras.layers.BatchNormalization()(conv3_3)\n",
        "    conv3_3 = keras.layers.ReLU()(conv3_3)\n",
        "    \n",
        "    conv3_4 = keras.layers.Conv2D(filters=256, kernel_size=[1, 16], padding=\"same\")(conv3_3)\n",
        "    conv3_4 = keras.layers.BatchNormalization()(conv3_4)\n",
        "    conv3_4 = keras.layers.ReLU()(conv3_4)\n",
        "    \n",
        "    conv2_4_1 = keras.layers.Conv2DTranspose(filters=128, kernel_size=[1,2],strides=[1,2], padding=\"same\")(conv3_4)\n",
        "    conv2_4 = tf.keras.layers.Concatenate(axis =3)([conv2_4_1, conv2_3])\n",
        "    conv2_4 = keras.layers.BatchNormalization()(conv2_4)\n",
        "    \n",
        "    conv2_5 = keras.layers.Conv2D(filters=128, kernel_size=[1, 16], padding=\"same\")(conv2_4)\n",
        "    conv2_5 = keras.layers.BatchNormalization()(conv2_5)\n",
        "    conv2_5 = keras.layers.ReLU()(conv2_5)\n",
        "    \n",
        "    conv2_6 = keras.layers.Conv2D(filters=128, kernel_size=[1, 16], padding=\"same\")(conv2_5)\n",
        "    conv2_6 = keras.layers.BatchNormalization()(conv2_6)\n",
        "    conv2_6 = keras.layers.ReLU()(conv2_6)\n",
        "    \n",
        "    conv1_3_1 = keras.layers.Conv2DTranspose(filters=64, kernel_size=[1, 2],strides=[1,2], padding=\"same\")(conv2_6)\n",
        "    conv1_3 = tf.keras.layers.Concatenate(axis =3)([conv1_2, conv1_3_1])\n",
        "    conv1_3 = keras.layers.BatchNormalization()(conv1_3)\n",
        "    \n",
        "    conv1_4 = keras.layers.Conv2D(filters=64, kernel_size=[1, 16], padding=\"same\")(conv1_3)\n",
        "    conv1_4 = keras.layers.BatchNormalization()(conv1_4)\n",
        "    conv1_4 = keras.layers.ReLU()(conv1_4)\n",
        "    \n",
        "    conv1_4 = keras.layers.Conv2D(filters=64, kernel_size=[1, 16], padding=\"same\")(conv1_4)\n",
        "    conv1_4 = keras.layers.BatchNormalization()(conv1_4)\n",
        "    conv1_4 = keras.layers.ReLU()(conv1_4)\n",
        "    \n",
        "    conv1_5 = keras.layers.Conv2D(filters=256, kernel_size=[6,1], padding=\"valid\")(conv1_4)\n",
        "    conv1_5 = keras.layers.BatchNormalization()(conv1_5)\n",
        "    conv1_5 = keras.layers.ReLU()(conv1_5)\n",
        "    \n",
        "    #gap = keras.layers.GlobalAveragePooling2D()(conv1_5)\n",
        "\n",
        "    output_layer = keras.layers.Conv2D(filters=1, kernel_size=[1, 1], padding=\"same\",activation=\"sigmoid\")(conv1_5)\n",
        "    output_layer = keras.layers.BatchNormalization()(output_layer)\n",
        "    out= tf.reshape(output_layer,[-1,1024])\n",
        "    layers.Flatten()\n",
        "\n",
        "    return keras.models.Model(inputs=input_layer, outputs=out)\n",
        "    #out = tf.reshape(output_layer, [-1, 1024])\n",
        "    #return out\n",
        "     \n",
        "model = make_model(input_shape=train_data.shape[1:])\n",
        "keras.utils.plot_model(model, show_shapes=True)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "0f58dcc2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f58dcc2",
        "outputId": "7cab2c5c-3d40-4c91-b168-9d1c37dd1663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "26/26 [==============================] - 29s 1s/step - loss: 3625.5422 - accuracy: 0.0845 - val_loss: 3550.8689 - val_accuracy: 0.1073\n",
            "Epoch 2/150\n",
            "26/26 [==============================] - 26s 1s/step - loss: 3624.3853 - accuracy: 0.0796 - val_loss: 3555.9841 - val_accuracy: 0.1415\n",
            "Epoch 3/150\n",
            "26/26 [==============================] - 26s 1s/step - loss: 3621.9460 - accuracy: 0.1322 - val_loss: 3541.1763 - val_accuracy: 0.2390\n",
            "Epoch 4/150\n",
            "26/26 [==============================] - 26s 1s/step - loss: 3619.9084 - accuracy: 0.1200 - val_loss: 3552.9609 - val_accuracy: 0.3610\n",
            "Epoch 5/150\n",
            "26/26 [==============================] - 26s 1s/step - loss: 3617.1267 - accuracy: 0.1248 - val_loss: 3567.6875 - val_accuracy: 0.4244\n",
            "Epoch 6/150\n",
            "26/26 [==============================] - 26s 1s/step - loss: 3616.1670 - accuracy: 0.1248 - val_loss: 3547.2366 - val_accuracy: 0.1707\n",
            "Epoch 7/150\n",
            "26/26 [==============================] - 26s 1s/step - loss: 3616.4517 - accuracy: 0.1273 - val_loss: 3535.2046 - val_accuracy: 0.1951\n",
            "Epoch 8/150\n",
            "26/26 [==============================] - 26s 1s/step - loss: 3616.1892 - accuracy: 0.1151 - val_loss: 3554.9941 - val_accuracy: 0.1171\n",
            "Epoch 9/150\n",
            "26/26 [==============================] - 26s 1s/step - loss: 3614.8804 - accuracy: 0.0942 - val_loss: 3532.9399 - val_accuracy: 0.2195\n",
            "Epoch 10/150\n",
            "26/26 [==============================] - 26s 1s/step - loss: 3641.4446 - accuracy: 0.1346 - val_loss: 3795.4812 - val_accuracy: 0.0927\n",
            "Epoch 11/150\n",
            "26/26 [==============================] - 26s 1s/step - loss: 3753.6619 - accuracy: 0.3709 - val_loss: 3547.5476 - val_accuracy: 0.4049\n",
            "Epoch 12/150\n",
            "26/26 [==============================] - 26s 1s/step - loss: 3809.2600 - accuracy: 0.1934 - val_loss: 4298.3164 - val_accuracy: 0.0634\n",
            "Epoch 13/150\n",
            "26/26 [==============================] - 26s 1s/step - loss: 4048.2957 - accuracy: 0.1958 - val_loss: 3580.3569 - val_accuracy: 0.2829\n",
            "Epoch 14/150\n",
            "26/26 [==============================] - 26s 1s/step - loss: 4739.6016 - accuracy: 0.0747 - val_loss: 5656.6050 - val_accuracy: 0.2000\n",
            "Epoch 15/150\n",
            "26/26 [==============================] - 26s 1s/step - loss: 5302.8462 - accuracy: 0.0783 - val_loss: 3582.3374 - val_accuracy: 0.1317\n",
            "Epoch 16/150\n",
            "26/26 [==============================] - 26s 1s/step - loss: 5676.3403 - accuracy: 0.0159 - val_loss: 6141.7935 - val_accuracy: 0.2976\n",
            "Epoch 17/150\n",
            "26/26 [==============================] - 26s 1000ms/step - loss: 5674.5386 - accuracy: 0.0000e+00 - val_loss: 5676.7344 - val_accuracy: 0.4732\n",
            "Epoch 18/150\n",
            "26/26 [==============================] - 26s 1s/step - loss: 5569.6938 - accuracy: 0.0024 - val_loss: 5632.6333 - val_accuracy: 0.0732\n",
            "Epoch 19/150\n",
            "26/26 [==============================] - 26s 1s/step - loss: 5273.5557 - accuracy: 0.0073 - val_loss: 3794.1694 - val_accuracy: 0.0829\n",
            "Epoch 20/150\n",
            "26/26 [==============================] - 26s 1s/step - loss: 4730.4971 - accuracy: 0.0037 - val_loss: 5160.2295 - val_accuracy: 0.0098\n",
            "Epoch 21/150\n",
            "26/26 [==============================] - 26s 1s/step - loss: 5389.8042 - accuracy: 0.0661 - val_loss: 5539.7231 - val_accuracy: 0.5024\n",
            "Epoch 22/150\n",
            "26/26 [==============================] - 26s 1s/step - loss: 5271.8140 - accuracy: 0.3880 - val_loss: 5372.5322 - val_accuracy: 0.4829\n",
            "Epoch 23/150\n",
            "26/26 [==============================] - 26s 1s/step - loss: 5203.0674 - accuracy: 0.4272 - val_loss: 5420.7529 - val_accuracy: 0.4439\n",
            "Epoch 24/150\n",
            "26/26 [==============================] - 26s 999ms/step - loss: 5043.3242 - accuracy: 0.4345 - val_loss: 5443.8970 - val_accuracy: 0.4000\n",
            "Epoch 25/150\n",
            "26/26 [==============================] - 26s 1s/step - loss: 5373.2231 - accuracy: 0.4259 - val_loss: 5388.7451 - val_accuracy: 0.3902\n",
            "Epoch 26/150\n",
            "26/26 [==============================] - 26s 1s/step - loss: 5214.7715 - accuracy: 0.4284 - val_loss: 5258.1445 - val_accuracy: 0.3610\n",
            "Epoch 27/150\n",
            "26/26 [==============================] - 26s 995ms/step - loss: 4761.3428 - accuracy: 0.4382 - val_loss: 5158.5532 - val_accuracy: 0.3610\n",
            "Epoch 28/150\n",
            "26/26 [==============================] - 26s 1s/step - loss: 4769.4883 - accuracy: 0.4529 - val_loss: 5181.2114 - val_accuracy: 0.3610\n",
            "Epoch 29/150\n",
            "26/26 [==============================] - 26s 999ms/step - loss: 5062.2544 - accuracy: 0.4431 - val_loss: 5072.3374 - val_accuracy: 0.3512\n",
            "Epoch 30/150\n",
            "26/26 [==============================] - 26s 994ms/step - loss: 4564.9727 - accuracy: 0.4431 - val_loss: 4968.8818 - val_accuracy: 0.3512\n",
            "Epoch 31/150\n",
            "26/26 [==============================] - 26s 991ms/step - loss: 4430.1924 - accuracy: 0.3941 - val_loss: 4902.1582 - val_accuracy: 0.3512\n",
            "Epoch 32/150\n",
            "26/26 [==============================] - 26s 997ms/step - loss: 4527.9146 - accuracy: 0.4284 - val_loss: 4861.8960 - val_accuracy: 0.3512\n",
            "Epoch 33/150\n",
            "26/26 [==============================] - 25s 965ms/step - loss: 4383.5913 - accuracy: 0.4871 - val_loss: 4750.3667 - val_accuracy: 0.3512\n",
            "Epoch 34/150\n",
            "26/26 [==============================] - 26s 986ms/step - loss: 4397.7266 - accuracy: 0.5104 - val_loss: 4752.4468 - val_accuracy: 0.3610\n",
            "Epoch 35/150\n",
            "26/26 [==============================] - 25s 953ms/step - loss: 4307.6938 - accuracy: 0.5129 - val_loss: 4633.6191 - val_accuracy: 0.3707\n",
            "Epoch 36/150\n",
            "26/26 [==============================] - 25s 949ms/step - loss: 4290.8965 - accuracy: 0.4982 - val_loss: 4601.5098 - val_accuracy: 0.3756\n",
            "Epoch 37/150\n",
            "26/26 [==============================] - 25s 977ms/step - loss: 4393.6240 - accuracy: 0.5006 - val_loss: 4499.6997 - val_accuracy: 0.3854\n",
            "Epoch 38/150\n",
            "26/26 [==============================] - 25s 959ms/step - loss: 4319.9722 - accuracy: 0.4823 - val_loss: 4466.3633 - val_accuracy: 0.3854\n",
            "Epoch 39/150\n",
            "26/26 [==============================] - 25s 947ms/step - loss: 4282.2920 - accuracy: 0.4541 - val_loss: 4271.0762 - val_accuracy: 0.3805\n",
            "Epoch 40/150\n",
            "26/26 [==============================] - 25s 946ms/step - loss: 4283.1162 - accuracy: 0.4761 - val_loss: 4266.4653 - val_accuracy: 0.3756\n",
            "Epoch 41/150\n",
            "26/26 [==============================] - 25s 963ms/step - loss: 4317.5459 - accuracy: 0.4761 - val_loss: 4138.8120 - val_accuracy: 0.3756\n",
            "Epoch 42/150\n",
            "26/26 [==============================] - 24s 940ms/step - loss: 4281.0864 - accuracy: 0.4590 - val_loss: 4138.8906 - val_accuracy: 0.3756\n",
            "Epoch 43/150\n",
            "26/26 [==============================] - 24s 932ms/step - loss: 4277.4111 - accuracy: 0.4517 - val_loss: 4138.8120 - val_accuracy: 0.3756\n",
            "Epoch 44/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.4553 - val_loss: 4140.9351 - val_accuracy: 0.3610\n",
            "Epoch 45/150\n",
            "26/26 [==============================] - 24s 936ms/step - loss: 4277.4160 - accuracy: 0.4933 - val_loss: 4140.9351 - val_accuracy: 0.3707\n",
            "Epoch 46/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4102 - accuracy: 0.4859 - val_loss: 4140.9351 - val_accuracy: 0.3756\n",
            "Epoch 47/150\n",
            "26/26 [==============================] - 24s 935ms/step - loss: 4277.4736 - accuracy: 0.5006 - val_loss: 4140.9351 - val_accuracy: 0.3756\n",
            "Epoch 48/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4106 - accuracy: 0.5080 - val_loss: 4140.9351 - val_accuracy: 0.4049\n",
            "Epoch 49/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4102 - accuracy: 0.5116 - val_loss: 4140.9351 - val_accuracy: 0.4049\n",
            "Epoch 50/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.4982 - val_loss: 4140.9351 - val_accuracy: 0.4098\n",
            "Epoch 51/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4106 - accuracy: 0.5165 - val_loss: 4140.9351 - val_accuracy: 0.4146\n",
            "Epoch 52/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4102 - accuracy: 0.5080 - val_loss: 4140.9351 - val_accuracy: 0.4146\n",
            "Epoch 53/150\n",
            "26/26 [==============================] - 24s 934ms/step - loss: 4277.4111 - accuracy: 0.5116 - val_loss: 4140.9351 - val_accuracy: 0.4146\n",
            "Epoch 54/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4102 - accuracy: 0.5092 - val_loss: 4140.9351 - val_accuracy: 0.4195\n",
            "Epoch 55/150\n",
            "26/26 [==============================] - 24s 934ms/step - loss: 4277.4111 - accuracy: 0.4969 - val_loss: 4140.9351 - val_accuracy: 0.4195\n",
            "Epoch 56/150\n",
            "26/26 [==============================] - 24s 934ms/step - loss: 4277.4111 - accuracy: 0.5055 - val_loss: 4140.9351 - val_accuracy: 0.4195\n",
            "Epoch 57/150\n",
            "26/26 [==============================] - 24s 934ms/step - loss: 4277.4106 - accuracy: 0.5190 - val_loss: 4140.9351 - val_accuracy: 0.4195\n",
            "Epoch 58/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4106 - accuracy: 0.5006 - val_loss: 4140.9351 - val_accuracy: 0.4195\n",
            "Epoch 59/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5055 - val_loss: 4140.9351 - val_accuracy: 0.4195\n",
            "Epoch 60/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4106 - accuracy: 0.5116 - val_loss: 4140.9351 - val_accuracy: 0.4293\n",
            "Epoch 61/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5312 - val_loss: 4140.9351 - val_accuracy: 0.4293\n",
            "Epoch 62/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5165 - val_loss: 4140.9351 - val_accuracy: 0.4244\n",
            "Epoch 63/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4102 - accuracy: 0.5104 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 64/150\n",
            "26/26 [==============================] - 24s 932ms/step - loss: 4277.4102 - accuracy: 0.5129 - val_loss: 4140.9351 - val_accuracy: 0.4195\n",
            "Epoch 65/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5067 - val_loss: 4140.9351 - val_accuracy: 0.4293\n",
            "Epoch 66/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4106 - accuracy: 0.5104 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 67/150\n",
            "26/26 [==============================] - 24s 932ms/step - loss: 4277.4116 - accuracy: 0.5080 - val_loss: 4140.9351 - val_accuracy: 0.4293\n",
            "Epoch 68/150\n",
            "26/26 [==============================] - 24s 932ms/step - loss: 4277.4102 - accuracy: 0.5116 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 69/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4102 - accuracy: 0.5092 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 70/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.4994 - val_loss: 4140.9351 - val_accuracy: 0.4244\n",
            "Epoch 71/150\n",
            "26/26 [==============================] - 24s 934ms/step - loss: 4277.4111 - accuracy: 0.5067 - val_loss: 4140.9351 - val_accuracy: 0.4195\n",
            "Epoch 72/150\n",
            "26/26 [==============================] - 24s 934ms/step - loss: 4277.4111 - accuracy: 0.5202 - val_loss: 4140.9351 - val_accuracy: 0.4195\n",
            "Epoch 73/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4106 - accuracy: 0.5067 - val_loss: 4140.9351 - val_accuracy: 0.4195\n",
            "Epoch 74/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4106 - accuracy: 0.5177 - val_loss: 4140.9351 - val_accuracy: 0.4195\n",
            "Epoch 75/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5129 - val_loss: 4140.9351 - val_accuracy: 0.4195\n",
            "Epoch 76/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4106 - accuracy: 0.5092 - val_loss: 4140.9351 - val_accuracy: 0.4195\n",
            "Epoch 77/150\n",
            "26/26 [==============================] - 24s 932ms/step - loss: 4277.4111 - accuracy: 0.5190 - val_loss: 4140.9351 - val_accuracy: 0.4195\n",
            "Epoch 78/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5141 - val_loss: 4140.9351 - val_accuracy: 0.4195\n",
            "Epoch 79/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5092 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 80/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4106 - accuracy: 0.5226 - val_loss: 4140.9351 - val_accuracy: 0.4293\n",
            "Epoch 81/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5141 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 82/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5018 - val_loss: 4140.9351 - val_accuracy: 0.4244\n",
            "Epoch 83/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5055 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 84/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5129 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 85/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4106 - accuracy: 0.5141 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 86/150\n",
            "26/26 [==============================] - 24s 934ms/step - loss: 4277.4111 - accuracy: 0.5190 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 87/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5104 - val_loss: 4140.9351 - val_accuracy: 0.4293\n",
            "Epoch 88/150\n",
            "26/26 [==============================] - 24s 932ms/step - loss: 4277.4111 - accuracy: 0.5129 - val_loss: 4140.9351 - val_accuracy: 0.4244\n",
            "Epoch 89/150\n",
            "26/26 [==============================] - 24s 935ms/step - loss: 4277.4102 - accuracy: 0.5153 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 90/150\n",
            "26/26 [==============================] - 24s 932ms/step - loss: 4277.4106 - accuracy: 0.4982 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 91/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4106 - accuracy: 0.5104 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 92/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5104 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 93/150\n",
            "26/26 [==============================] - 24s 932ms/step - loss: 4277.4106 - accuracy: 0.5177 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 94/150\n",
            "26/26 [==============================] - 24s 932ms/step - loss: 4277.4111 - accuracy: 0.5129 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 95/150\n",
            "26/26 [==============================] - 24s 932ms/step - loss: 4277.4111 - accuracy: 0.5080 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 96/150\n",
            "26/26 [==============================] - 24s 932ms/step - loss: 4277.4102 - accuracy: 0.5190 - val_loss: 4140.9351 - val_accuracy: 0.4244\n",
            "Epoch 97/150\n",
            "26/26 [==============================] - 24s 931ms/step - loss: 4277.4111 - accuracy: 0.5080 - val_loss: 4140.9351 - val_accuracy: 0.4244\n",
            "Epoch 98/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5153 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 99/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4102 - accuracy: 0.5129 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 100/150\n",
            "26/26 [==============================] - 24s 931ms/step - loss: 4277.4111 - accuracy: 0.5165 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 101/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5043 - val_loss: 4140.9351 - val_accuracy: 0.4244\n",
            "Epoch 102/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4106 - accuracy: 0.5067 - val_loss: 4140.9351 - val_accuracy: 0.4195\n",
            "Epoch 103/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4106 - accuracy: 0.5153 - val_loss: 4140.9351 - val_accuracy: 0.4244\n",
            "Epoch 104/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4106 - accuracy: 0.5129 - val_loss: 4140.9351 - val_accuracy: 0.4293\n",
            "Epoch 105/150\n",
            "26/26 [==============================] - 24s 934ms/step - loss: 4277.4106 - accuracy: 0.5190 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 106/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4102 - accuracy: 0.5043 - val_loss: 4140.9351 - val_accuracy: 0.4195\n",
            "Epoch 107/150\n",
            "26/26 [==============================] - 24s 934ms/step - loss: 4277.4111 - accuracy: 0.5190 - val_loss: 4140.9351 - val_accuracy: 0.4244\n",
            "Epoch 108/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4106 - accuracy: 0.5055 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 109/150\n",
            "26/26 [==============================] - 24s 932ms/step - loss: 4277.4111 - accuracy: 0.5006 - val_loss: 4140.9351 - val_accuracy: 0.4195\n",
            "Epoch 110/150\n",
            "26/26 [==============================] - 24s 934ms/step - loss: 4277.4106 - accuracy: 0.4994 - val_loss: 4140.9351 - val_accuracy: 0.4195\n",
            "Epoch 111/150\n",
            "26/26 [==============================] - 24s 932ms/step - loss: 4277.4106 - accuracy: 0.5202 - val_loss: 4140.9351 - val_accuracy: 0.4195\n",
            "Epoch 112/150\n",
            "26/26 [==============================] - 24s 934ms/step - loss: 4277.4106 - accuracy: 0.5104 - val_loss: 4140.9351 - val_accuracy: 0.4195\n",
            "Epoch 113/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5055 - val_loss: 4140.9351 - val_accuracy: 0.4195\n",
            "Epoch 114/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5055 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 115/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5239 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 116/150\n",
            "26/26 [==============================] - 24s 934ms/step - loss: 4277.4106 - accuracy: 0.5202 - val_loss: 4140.9351 - val_accuracy: 0.4244\n",
            "Epoch 117/150\n",
            "26/26 [==============================] - 24s 935ms/step - loss: 4277.4111 - accuracy: 0.5239 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 118/150\n",
            "26/26 [==============================] - 24s 934ms/step - loss: 4277.4111 - accuracy: 0.5141 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 119/150\n",
            "26/26 [==============================] - 24s 932ms/step - loss: 4277.4121 - accuracy: 0.5239 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 120/150\n",
            "26/26 [==============================] - 24s 934ms/step - loss: 4277.4102 - accuracy: 0.5104 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 121/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5190 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 122/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5092 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 123/150\n",
            "26/26 [==============================] - 24s 932ms/step - loss: 4277.4111 - accuracy: 0.5141 - val_loss: 4140.9351 - val_accuracy: 0.4244\n",
            "Epoch 124/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5116 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 125/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4106 - accuracy: 0.5129 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 126/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5129 - val_loss: 4140.9351 - val_accuracy: 0.4244\n",
            "Epoch 127/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5177 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 128/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5153 - val_loss: 4140.9351 - val_accuracy: 0.4293\n",
            "Epoch 129/150\n",
            "26/26 [==============================] - 24s 932ms/step - loss: 4277.4111 - accuracy: 0.5177 - val_loss: 4140.9351 - val_accuracy: 0.4244\n",
            "Epoch 130/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5092 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 131/150\n",
            "26/26 [==============================] - 24s 932ms/step - loss: 4277.4106 - accuracy: 0.4982 - val_loss: 4140.9351 - val_accuracy: 0.4293\n",
            "Epoch 132/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5141 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 133/150\n",
            "26/26 [==============================] - 24s 934ms/step - loss: 4277.4111 - accuracy: 0.5129 - val_loss: 4140.9351 - val_accuracy: 0.4293\n",
            "Epoch 134/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4106 - accuracy: 0.5080 - val_loss: 4140.9351 - val_accuracy: 0.4293\n",
            "Epoch 135/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4116 - accuracy: 0.5165 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 136/150\n",
            "26/26 [==============================] - 24s 932ms/step - loss: 4277.4106 - accuracy: 0.5055 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 137/150\n",
            "26/26 [==============================] - 24s 931ms/step - loss: 4277.4111 - accuracy: 0.5092 - val_loss: 4140.9351 - val_accuracy: 0.4244\n",
            "Epoch 138/150\n",
            "26/26 [==============================] - 24s 931ms/step - loss: 4277.4106 - accuracy: 0.5080 - val_loss: 4140.9351 - val_accuracy: 0.4195\n",
            "Epoch 139/150\n",
            "26/26 [==============================] - 24s 931ms/step - loss: 4277.4111 - accuracy: 0.5165 - val_loss: 4140.9351 - val_accuracy: 0.4195\n",
            "Epoch 140/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4106 - accuracy: 0.5129 - val_loss: 4140.9351 - val_accuracy: 0.4244\n",
            "Epoch 141/150\n",
            "26/26 [==============================] - 24s 934ms/step - loss: 4277.4106 - accuracy: 0.5153 - val_loss: 4140.9351 - val_accuracy: 0.4195\n",
            "Epoch 142/150\n",
            "26/26 [==============================] - 24s 934ms/step - loss: 4277.4106 - accuracy: 0.5165 - val_loss: 4140.9351 - val_accuracy: 0.4293\n",
            "Epoch 143/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5165 - val_loss: 4140.9351 - val_accuracy: 0.4195\n",
            "Epoch 144/150\n",
            "26/26 [==============================] - 24s 934ms/step - loss: 4277.4106 - accuracy: 0.5104 - val_loss: 4140.9351 - val_accuracy: 0.4244\n",
            "Epoch 145/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5129 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 146/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4106 - accuracy: 0.5043 - val_loss: 4140.9351 - val_accuracy: 0.4244\n",
            "Epoch 147/150\n",
            "26/26 [==============================] - 24s 932ms/step - loss: 4277.4111 - accuracy: 0.5129 - val_loss: 4140.9351 - val_accuracy: 0.4244\n",
            "Epoch 148/150\n",
            "26/26 [==============================] - 24s 932ms/step - loss: 4277.4111 - accuracy: 0.5129 - val_loss: 4140.9351 - val_accuracy: 0.4341\n",
            "Epoch 149/150\n",
            "26/26 [==============================] - 24s 933ms/step - loss: 4277.4111 - accuracy: 0.5018 - val_loss: 4140.9351 - val_accuracy: 0.4195\n",
            "Epoch 150/150\n",
            "26/26 [==============================] - 24s 934ms/step - loss: 4277.4106 - accuracy: 0.5116 - val_loss: 4140.9351 - val_accuracy: 0.4195\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "opt = SGD(learning_rate=0.001)\n",
        "\n",
        "epochs = 150\n",
        "batch_size = 32\n",
        "display_step = 1\n",
        "data_len = len(train_data)\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping( patience=10),\n",
        "]\n",
        "\n",
        "#model.compile(loss = \"categorical_crossentropy\", optimizer = 'adam' ,metrics=[\"accuracy\"],)\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    train_label,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.2,\n",
        "    verbose=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model = keras.models.load_model(\"best_model.h5\")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_data, test_label)\n",
        "\n",
        "print(\"Test accuracy\", test_acc)\n",
        "print(\"Test loss\", test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4W1DiTwVQmC",
        "outputId": "de4b6dde-64c8-4799-d18e-be8cd986bea1"
      },
      "id": "p4W1DiTwVQmC",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11/11 [==============================] - 11s 172ms/step - loss: 4570.0625 - accuracy: 0.0241\n",
            "Test accuracy 0.024096384644508362\n",
            "Test loss 4570.0625\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "name": "Gait extraction2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
