{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53404,"status":"ok","timestamp":1645631822863,"user":{"displayName":"Kareem Saber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAnmhrPQmFkN4A27OT7FzoroxtI8hJGzUxIy3PELc=s64","userId":"13851535216512318722"},"user_tz":-120},"id":"LpuLm63_PzYf","outputId":"03b2eae6-7552-4dd3-ecf8-0e338df49874"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Mounted at /content/gdrive\n","(66542, 256, 6) (66542, 2) (7600, 256, 6) (7600, 2)\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive')\n","root_path = 'gdrive/My Drive/'  #change dir to your project folder\n","\n","import numpy as np\n","x_train = np.load('gdrive/My Drive/dataset/dataset5/trainX.npy')\n","y_train = np.load('gdrive/My Drive/dataset/dataset5/trainy.npy')\n","x_test = np.load('gdrive/My Drive/dataset/dataset5/testX.npy')\n","y_test = np.load('gdrive/My Drive/dataset/dataset5/testy.npy')\n","print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zd7mvvkWjg4n"},"outputs":[],"source":["from numpy import save, load\n","from pandas import read_csv\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ao--J_t2lG44"},"outputs":[],"source":["x_train = x_train.transpose(0,2,1)\n","x_test = x_test.transpose(0,2,1)\n","\n","idx = np.random.permutation(len(x_train))\n","x_train = x_train[idx]\n","y_train = y_train[idx]\n","from sklearn.model_selection import train_test_split\n","x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=0.20, random_state=42)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1645631826237,"user":{"displayName":"Kareem Saber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAnmhrPQmFkN4A27OT7FzoroxtI8hJGzUxIy3PELc=s64","userId":"13851535216512318722"},"user_tz":-120},"id":"Rn_4jS7GzEEH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3a075f70-4beb-44ad-8a06-976775ac265e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(53233, 6, 256)"]},"metadata":{},"execution_count":4}],"source":["x_train.shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F0T1QHphzGoZ"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3103,"status":"ok","timestamp":1645631829338,"user":{"displayName":"Kareem Saber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAnmhrPQmFkN4A27OT7FzoroxtI8hJGzUxIy3PELc=s64","userId":"13851535216512318722"},"user_tz":-120},"id":"fSaweocozMKT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6576d294-c9b6-49d6-d255-c6553d24abec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," transformer_block (Transfor  (None, 6, 256)           1184512   \n"," merBlock)                                                       \n","                                                                 \n"," global_average_pooling1d (G  (None, 256)              0         \n"," lobalAveragePooling1D)                                          \n","                                                                 \n"," dropout_2 (Dropout)         (None, 256)               0         \n","                                                                 \n"," dense_2 (Dense)             (None, 256)               65792     \n","                                                                 \n"," dropout_3 (Dropout)         (None, 256)               0         \n","                                                                 \n"," dense_3 (Dense)             (None, 20)                5140      \n","                                                                 \n","=================================================================\n","Total params: 1,255,444\n","Trainable params: 1,255,444\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["embed_dim = x_train.shape[-1]   # Embedding size for each token\n","num_heads = 4  # Number of attention heads\n","ff_dim = x_train.shape[-1]  # Hidden layer size in feed forward network inside transformer\n","\n","class TransformerBlock(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super(TransformerBlock, self).__init__()\n","        self.att = layers.MultiHeadAttention(num_heads=num_heads,\n","                                             key_dim=embed_dim)\n","        self.ffn = keras.Sequential([\n","            layers.Dense(ff_dim, activation=\"relu\"),\n","            layers.Dense(embed_dim),\n","        ])\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs, inputs)  # self-attention layer\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)  # layer norm\n","        ffn_output = self.ffn(out1)  #feed-forward layer\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)  # layer norm\n","model = keras.Sequential()\n","model.add(layers.Input(shape= x_train.shape[1:]))\n","model.add(TransformerBlock(embed_dim, num_heads, ff_dim))\n","model.add(layers.GlobalAveragePooling1D())\n","model.add(layers.Dropout(0.2))\n","model.add(layers.Dense(ff_dim, activation='relu'))\n","model.add(layers.Dropout(0.2))\n","model.add(layers.Dense(20, activation='softmax'))\n","model.compile(\n","    loss=\"categorical_crossentropy\",\n","    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n","    metrics=[\"accuracy\"],\n",")\n","model.summary()\n","callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":727},"executionInfo":{"elapsed":1533,"status":"error","timestamp":1645631830868,"user":{"displayName":"Kareem Saber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAnmhrPQmFkN4A27OT7FzoroxtI8hJGzUxIy3PELc=s64","userId":"13851535216512318722"},"user_tz":-120},"id":"q8mfp7apzYA2","outputId":"56744a73-54c5-4714-8dc9-2173d12c9bd6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-592af8fe033e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 919, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 1790, in categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 2) and (None, 20) are incompatible\n"]}],"source":["history = model.fit(x_train, y_train, epochs=200, verbose=True, validation_data=(x_validation, y_validation), batch_size=128,callbacks=callbacks)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5shX4JC5zbgo"},"outputs":[],"source":["loss, accuracy = model.evaluate(x_validation, y_validation, verbose=False)\n","print(\"Validation Accuracy:  {:.4f}\".format(accuracy))\n","loss, accuracy = model.evaluate(x_test, y_test, verbose=False)\n","print(\"Testing Accuracy: {:.4f}\".format(accuracy))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OxZv5hvkzfNV"},"outputs":[],"source":["loss, accuracy = model.evaluate(x_validation, y_validation, verbose=False)\n","print(\"Validation Accuracy:  {:.4f}\".format(accuracy))\n","loss, accuracy = model.evaluate(x_test, y_test, verbose=False)\n","print(\"Testing Accuracy: {:.4f}\".format(accuracy))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ewWxsAqizkbW"},"outputs":[],"source":["\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mGLV-qkF5OQN"},"outputs":[],"source":["\n","# compare machine learning models for regression\n","from numpy import mean\n","from numpy import std\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import RepeatedKFold\n","from sklearn.linear_model import LinearRegression\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.svm import SVR\n","from matplotlib import pyplot\n"," \n","# get the dataset\n","def get_dataset():\n","\tX, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=1)\n","\treturn X, y\n"," \n","# get a list of models to evaluate\n","def get_models():\n","\tmodels = dict()\n","\tmodels['knn'] = KNeighborsRegressor()\n","\tmodels['cart'] = DecisionTreeRegressor()\n","\tmodels['svm'] = SVR()\n","\treturn models\n"," \n","# evaluate a given model using cross-validation\n","def evaluate_model(model, X, y):\n","\tcv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n","\tscores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n","\treturn scores\n"," \n","# define dataset\n","X, y = get_dataset()\n","# get the models to evaluate\n","models = get_models()\n","# evaluate the models and store results\n","results, names = list(), list()\n","for name, model in models.items():\n","\tscores = evaluate_model(model, X, y)\n","\tresults.append(scores)\n","\tnames.append(name)\n","\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n","# plot model performance for comparison\n","pyplot.boxplot(results, labels=names, showmeans=True)\n","pyplot.show()"]},{"cell_type":"code","source":["# evaluate a weighted average ensemble for classification\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.ensemble import VotingClassifier\n"," \n","# get a list of base models\n","def get_models():\n","\tmodels = list()\n","\tmodels.append(('lr', LogisticRegression()))\n","\tmodels.append(('cart', DecisionTreeClassifier()))\n","\tmodels.append(('bayes', GaussianNB()))\n","\treturn models\n"," \n","# evaluate each base model\n","def evaluate_models(models, X_train, X_val, y_train, y_val):\n","\t# fit and evaluate the models\n","\tscores = list()\n","\tfor name, model in models:\n","\t\t# fit the model\n","\t\tmodel.fit(X_train, y_train)\n","\t\t# evaluate the model\n","\t\tyhat = model.predict(X_val)\n","\t\tacc = accuracy_score(y_val, yhat)\n","\t\t# store the performance\n","\t\tscores.append(acc)\n","\t\t# report model performance\n","\treturn scores\n"," \n","# define dataset\n","X, y = make_classification(n_samples=10000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n","# split dataset into train and test sets\n","X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.50, random_state=1)\n","# split the full train set into train and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.33, random_state=1)\n","# create the base models\n","models = get_models()\n","# fit and evaluate each model\n","scores = evaluate_models(models, X_train, X_val, y_train, y_val)\n","print(scores)\n","# create the ensemble\n","ensemble = VotingClassifier(estimators=models, voting='soft', weights=scores)\n","# fit the ensemble on the training dataset\n","ensemble.fit(X_train_full, y_train_full)\n","# make predictions on test set\n","yhat = ensemble.predict(X_test)\n","# evaluate predictions\n","score = accuracy_score(y_test, yhat)\n","print('Weighted Avg Accuracy: %.3f' % (score*100))"],"metadata":{"id":"7likvC__Kc1J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"UiVzoYGGKeL2"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"Copy of Untitled0.ipynb","provenance":[{"file_id":"1A4qxARgvNpAsjMFCckIVq-pUnpUoO4ie","timestamp":1645634184943}],"collapsed_sections":[],"authorship_tag":"ABX9TyMK/UxhqRk93nSLw1CKq5H7"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}