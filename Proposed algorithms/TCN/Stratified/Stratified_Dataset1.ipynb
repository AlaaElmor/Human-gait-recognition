{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HzTuqLMwGxiF"
      },
      "outputs": [],
      "source": [
        "from numpy import save, load\n",
        "from pandas import read_csv\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras import Model\n",
        "from keras.layers import Conv1D, SpatialDropout1D\n",
        "from keras.layers import Convolution1D, Dense,Activation\n",
        "from keras.models import Input, Model\n",
        "from keras.layers import LSTM, Conv1D, concatenate,GlobalMaxPooling1D,GlobalAveragePooling1D,TimeDistributed, MaxPooling1D\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "import keras.layers\n",
        "from keras import optimizers\n",
        "from keras.layers import Activation, Lambda\n",
        "from keras.layers import Convolution1D, Dense\n",
        "from keras.models import Input, Model\n",
        "from typing import List, Tuple\n",
        "from statistics import mean, stdev\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import linear_model\n",
        "from sklearn import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RS-oWhjJcBj",
        "outputId": "c00602ee-eb0b-459a-ce72-bd22fe791b46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Mounted at /content/gdrive\n",
            "(33104, 128, 3) (33104, 118) (3740, 128, 3) (3740, 118)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/'  #change dir to your project folder\n",
        "\n",
        "import numpy as np\n",
        "x_train = np.load('gdrive/MyDrive/Colab Notebooks/Dataset1/trainX.npy')\n",
        "y_train = np.load('gdrive/MyDrive/Colab Notebooks/Dataset1/trainy.npy')\n",
        "x_test = np.load('gdrive/MyDrive/Colab Notebooks/Dataset1/testX.npy')\n",
        "y_test = np.load('gdrive/MyDrive/Colab Notebooks/Dataset1/testy.npy')\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NJd0grO3JbFR"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=0.20, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iuybbVmRJa6l"
      },
      "outputs": [],
      "source": [
        "def channel_normalization(x):\n",
        "    max_values = K.max(K.abs(x), 2, keepdims=True) + 1e-5\n",
        "    out = x / max_values\n",
        "    return out\n",
        "\n",
        "def residual_block(x, s, i, activation, nb_filters, kernel_size, padding, dropout_rate=0, name=''):\n",
        "    original_x = x\n",
        "    conv = Conv1D(filters=nb_filters, kernel_size=kernel_size,\n",
        "                  dilation_rate=i, padding=padding,\n",
        "                  name=name + '_dilated_conv_%d_tanh_s%d' % (i, s))(x)\n",
        "    if activation == 'norm_relu':\n",
        "        x = Activation('relu')(conv)\n",
        "        x = Lambda(channel_normalization)(x)\n",
        "    else:\n",
        "        x = Activation(activation)(conv)\n",
        "\n",
        "    x = SpatialDropout1D(dropout_rate, name=name + '_spatial_dropout1d_%d_s%d_%f' % (i, s, dropout_rate))(x)\n",
        "\n",
        "    # 1x1 conv.\n",
        "    x = Convolution1D(nb_filters, 1, padding='same')(x)\n",
        "    res_x = keras.layers.add([original_x, x])\n",
        "    return res_x, x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "I-aKoSJzJayg"
      },
      "outputs": [],
      "source": [
        "class TCN:\n",
        "\n",
        "    def __init__(self,\n",
        "                 nb_filters=64,\n",
        "                 kernel_size=2,\n",
        "                 nb_stacks=1,\n",
        "                 dilations=None,\n",
        "                 activation='norm_relu',\n",
        "                 padding='causal',\n",
        "                 use_skip_connections=True,\n",
        "                 dropout_rate=0.0,\n",
        "                 return_sequences=True,\n",
        "                 name='tcn'):\n",
        "        self.name = name\n",
        "        self.return_sequences = return_sequences\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.use_skip_connections = use_skip_connections\n",
        "        self.activation = activation\n",
        "        self.dilations = dilations\n",
        "        self.nb_stacks = nb_stacks\n",
        "        self.kernel_size = kernel_size\n",
        "        self.nb_filters = nb_filters\n",
        "        self.padding = padding\n",
        "        \n",
        "        if padding != 'causal' and padding != 'same':\n",
        "            raise ValueError(\"Only 'causal' or 'same' paddings are compatible for this layer.\")\n",
        "\n",
        "        if not isinstance(nb_filters, int):\n",
        "            print('An interface change occurred after the version 2.1.2.')\n",
        "            print('Before: tcn.TCN(i, return_sequences=False, ...)')\n",
        "            print('Now should be: tcn.TCN(return_sequences=False, ...)(i)')\n",
        "            print('Second solution is to pip install keras-tcn==2.1.2 to downgrade.')\n",
        "            raise Exception()\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        if self.dilations is None:\n",
        "            self.dilations = [1, 2, 4, 8, 16, 32]\n",
        "        x = inputs\n",
        "        x = Convolution1D(self.nb_filters, 1, padding=self.padding, name=self.name + '_initial_conv')(x)\n",
        "        skip_connections = []\n",
        "        for s in range(self.nb_stacks):\n",
        "            for i in self.dilations:\n",
        "                x, skip_out = residual_block(x, s, i, self.activation, self.nb_filters,\n",
        "                                             self.kernel_size, self.padding, self.dropout_rate, name=self.name)\n",
        "                skip_connections.append(skip_out)\n",
        "        if self.use_skip_connections:\n",
        "            x = keras.layers.add(skip_connections)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "        if not self.return_sequences:\n",
        "            output_slice_index = -1\n",
        "            x = Lambda(lambda tt: tt[:, output_slice_index, :])(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]"
      ],
      "metadata": {
        "id": "a_BCk4qP089c"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = x_train.shape[1:]\n",
        "input = Input(shape=x_train.shape[1:])"
      ],
      "metadata": {
        "id": "ciO2dStM1JX8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYzcyeLC4iZn",
        "outputId": "86557cef-f787-4a65-c7c9-b507c1863cbb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26483, 118)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = (np.argmax(y_train, axis=1)).reshape(-1, 1)\n",
        "y_test =  (np.argmax(y_test, axis=1)).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "CpyXZutNXUvi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyskGFWS9G8C",
        "outputId": "fe6f9c10-3638-4b36-cfa6-e8278ba06bb8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26483, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "num_folds = 2\n",
        "inputs = np.concatenate((x_train, x_test), axis=0)\n",
        "targets = np.concatenate((y_train, y_test), axis=0)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "fold_no = 1\n",
        "input_shape = x_train.shape[1:]\n",
        "input = Input(shape=x_train.shape[1:])\n",
        "for train, test in skf.split(inputs, targets):\n",
        "\n",
        "  x = SpatialDropout1D(0.2)(input)\n",
        "  x = TCN(128,dilations = [1, 2, 4, 8, 16],kernel_size = 3, return_sequences=True, name = 'tnc1')(x)\n",
        "  x = TCN(128,dilations = [1, 2, 4, 8, 16],kernel_size = 3, return_sequences=True, name = 'tnc2')(x)\n",
        "  max_pool = GlobalMaxPooling1D()(x)\n",
        "  x = Dense(128, activation=\"relu\")(max_pool)\n",
        "  x = Dropout(0.2)(x)\n",
        "  output = Dense(118, activation=\"softmax\")(x)    \n",
        "  model = Model(inputs=input, outputs=output)\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  history = model.fit(inputs[train], targets[train], epochs=200, verbose=True, batch_size=128,callbacks=[keras.callbacks.EarlyStopping(monitor='loss', patience=5,restore_best_weights=True)])\n",
        "    # Generate generalization metrics\n",
        "  scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1NlnQvaZxR9",
        "outputId": "b3cfff9f-e440-4315-9af1-64eb289de96e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "189/189 [==============================] - 34s 163ms/step - loss: 3.1501 - accuracy: 0.2784\n",
            "Epoch 2/200\n",
            "189/189 [==============================] - 29s 151ms/step - loss: 1.4035 - accuracy: 0.6513\n",
            "Epoch 3/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.9264 - accuracy: 0.7648\n",
            "Epoch 4/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.7409 - accuracy: 0.8118\n",
            "Epoch 5/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.6248 - accuracy: 0.8408\n",
            "Epoch 6/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.5703 - accuracy: 0.8541\n",
            "Epoch 7/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.5024 - accuracy: 0.8724\n",
            "Epoch 8/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.4592 - accuracy: 0.8800\n",
            "Epoch 9/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.4248 - accuracy: 0.8899\n",
            "Epoch 10/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.4008 - accuracy: 0.8942\n",
            "Epoch 11/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.3802 - accuracy: 0.9016\n",
            "Epoch 12/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.3636 - accuracy: 0.9057\n",
            "Epoch 13/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.3475 - accuracy: 0.9095\n",
            "Epoch 14/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.3325 - accuracy: 0.9129\n",
            "Epoch 15/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.3142 - accuracy: 0.9173\n",
            "Epoch 16/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.3088 - accuracy: 0.9197\n",
            "Epoch 17/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2912 - accuracy: 0.9211\n",
            "Epoch 18/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2853 - accuracy: 0.9246\n",
            "Epoch 19/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.2799 - accuracy: 0.9272\n",
            "Epoch 20/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.2743 - accuracy: 0.9279\n",
            "Epoch 21/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.2574 - accuracy: 0.9311\n",
            "Epoch 22/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.2642 - accuracy: 0.9302\n",
            "Epoch 23/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2543 - accuracy: 0.9320\n",
            "Epoch 24/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2496 - accuracy: 0.9337\n",
            "Epoch 25/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2518 - accuracy: 0.9323\n",
            "Epoch 26/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2452 - accuracy: 0.9337\n",
            "Epoch 27/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2354 - accuracy: 0.9351\n",
            "Epoch 28/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2250 - accuracy: 0.9399\n",
            "Epoch 29/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.2255 - accuracy: 0.9384\n",
            "Epoch 30/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2162 - accuracy: 0.9418\n",
            "Epoch 31/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2088 - accuracy: 0.9448\n",
            "Epoch 32/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2146 - accuracy: 0.9422\n",
            "Epoch 33/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2034 - accuracy: 0.9454\n",
            "Epoch 34/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1969 - accuracy: 0.9463\n",
            "Epoch 35/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2106 - accuracy: 0.9436\n",
            "Epoch 36/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2069 - accuracy: 0.9449\n",
            "Epoch 37/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1936 - accuracy: 0.9489\n",
            "Epoch 38/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1889 - accuracy: 0.9509\n",
            "Epoch 39/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1882 - accuracy: 0.9496\n",
            "Epoch 40/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1894 - accuracy: 0.9488\n",
            "Epoch 41/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1940 - accuracy: 0.9493\n",
            "Epoch 42/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2002 - accuracy: 0.9474\n",
            "Epoch 43/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.1800 - accuracy: 0.9531\n",
            "Epoch 44/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1832 - accuracy: 0.9520\n",
            "Epoch 45/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1825 - accuracy: 0.9492\n",
            "Epoch 46/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1798 - accuracy: 0.9516\n",
            "Epoch 47/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1783 - accuracy: 0.9509\n",
            "Epoch 48/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1896 - accuracy: 0.9493\n",
            "Epoch 49/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1766 - accuracy: 0.9520\n",
            "Epoch 50/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1726 - accuracy: 0.9541\n",
            "Epoch 51/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1670 - accuracy: 0.9536\n",
            "Epoch 52/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1739 - accuracy: 0.9536\n",
            "Epoch 53/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1750 - accuracy: 0.9526\n",
            "Epoch 54/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1649 - accuracy: 0.9553\n",
            "Epoch 55/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1678 - accuracy: 0.9552\n",
            "Epoch 56/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1708 - accuracy: 0.9552\n",
            "Epoch 57/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1567 - accuracy: 0.9577\n",
            "Epoch 58/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1546 - accuracy: 0.9577\n",
            "Epoch 59/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1637 - accuracy: 0.9560\n",
            "Epoch 60/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1588 - accuracy: 0.9565\n",
            "Epoch 61/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1562 - accuracy: 0.9588\n",
            "Epoch 62/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1481 - accuracy: 0.9589\n",
            "Epoch 63/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1536 - accuracy: 0.9587\n",
            "Epoch 64/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1535 - accuracy: 0.9585\n",
            "Epoch 65/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1481 - accuracy: 0.9617\n",
            "Epoch 66/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1489 - accuracy: 0.9597\n",
            "Epoch 67/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1551 - accuracy: 0.9589\n",
            "Score for fold 1: loss of 0.15169847011566162; accuracy of 96.24482989311218%\n",
            "Epoch 1/200\n",
            "189/189 [==============================] - 32s 149ms/step - loss: 3.2653 - accuracy: 0.2398\n",
            "Epoch 2/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 1.5482 - accuracy: 0.6126\n",
            "Epoch 3/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.9922 - accuracy: 0.7498\n",
            "Epoch 4/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.7863 - accuracy: 0.7993\n",
            "Epoch 5/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.6510 - accuracy: 0.8293\n",
            "Epoch 6/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.5770 - accuracy: 0.8469\n",
            "Epoch 7/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.5170 - accuracy: 0.8661\n",
            "Epoch 8/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.4820 - accuracy: 0.8756\n",
            "Epoch 9/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.4441 - accuracy: 0.8850\n",
            "Epoch 10/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.4129 - accuracy: 0.8918\n",
            "Epoch 11/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.3987 - accuracy: 0.8959\n",
            "Epoch 12/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.3817 - accuracy: 0.8985\n",
            "Epoch 13/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.3640 - accuracy: 0.9040\n",
            "Epoch 14/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.3442 - accuracy: 0.9093\n",
            "Epoch 15/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.3237 - accuracy: 0.9146\n",
            "Epoch 16/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.3221 - accuracy: 0.9152\n",
            "Epoch 17/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.3147 - accuracy: 0.9163\n",
            "Epoch 18/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.2986 - accuracy: 0.9194\n",
            "Epoch 19/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.2930 - accuracy: 0.9207\n",
            "Epoch 20/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2819 - accuracy: 0.9236\n",
            "Epoch 21/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.2755 - accuracy: 0.9265\n",
            "Epoch 22/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2729 - accuracy: 0.9275\n",
            "Epoch 23/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2656 - accuracy: 0.9284\n",
            "Epoch 24/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2602 - accuracy: 0.9296\n",
            "Epoch 25/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.2414 - accuracy: 0.9337\n",
            "Epoch 26/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.2428 - accuracy: 0.9343\n",
            "Epoch 27/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2307 - accuracy: 0.9376\n",
            "Epoch 28/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2362 - accuracy: 0.9359\n",
            "Epoch 29/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.2400 - accuracy: 0.9361\n",
            "Epoch 30/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2383 - accuracy: 0.9353\n",
            "Epoch 31/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2325 - accuracy: 0.9376\n",
            "Epoch 32/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2105 - accuracy: 0.9421\n",
            "Epoch 33/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2119 - accuracy: 0.9440\n",
            "Epoch 34/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.2117 - accuracy: 0.9448\n",
            "Epoch 35/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2051 - accuracy: 0.9455\n",
            "Epoch 36/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2062 - accuracy: 0.9434\n",
            "Epoch 37/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1986 - accuracy: 0.9470\n",
            "Epoch 38/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2065 - accuracy: 0.9456\n",
            "Epoch 39/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1959 - accuracy: 0.9490\n",
            "Epoch 40/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1986 - accuracy: 0.9468\n",
            "Epoch 41/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1952 - accuracy: 0.9473\n",
            "Epoch 42/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1943 - accuracy: 0.9466\n",
            "Epoch 43/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1970 - accuracy: 0.9462\n",
            "Epoch 44/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1890 - accuracy: 0.9502\n",
            "Epoch 45/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1815 - accuracy: 0.9512\n",
            "Epoch 46/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1839 - accuracy: 0.9495\n",
            "Epoch 47/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1763 - accuracy: 0.9532\n",
            "Epoch 48/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1835 - accuracy: 0.9510\n",
            "Epoch 49/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1836 - accuracy: 0.9512\n",
            "Epoch 50/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1803 - accuracy: 0.9525\n",
            "Epoch 51/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1755 - accuracy: 0.9536\n",
            "Epoch 52/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1641 - accuracy: 0.9571\n",
            "Epoch 53/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1690 - accuracy: 0.9550\n",
            "Epoch 54/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1629 - accuracy: 0.9565\n",
            "Epoch 55/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1701 - accuracy: 0.9546\n",
            "Epoch 56/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1633 - accuracy: 0.9566\n",
            "Epoch 57/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1609 - accuracy: 0.9573\n",
            "Epoch 58/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1572 - accuracy: 0.9576\n",
            "Epoch 59/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1781 - accuracy: 0.9526\n",
            "Epoch 60/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1590 - accuracy: 0.9581\n",
            "Epoch 61/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1548 - accuracy: 0.9586\n",
            "Epoch 62/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1589 - accuracy: 0.9576\n",
            "Epoch 63/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1626 - accuracy: 0.9552\n",
            "Epoch 64/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1521 - accuracy: 0.9593\n",
            "Epoch 65/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1451 - accuracy: 0.9615\n",
            "Epoch 66/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1555 - accuracy: 0.9581\n",
            "Epoch 67/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1591 - accuracy: 0.9578\n",
            "Epoch 68/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1460 - accuracy: 0.9607\n",
            "Epoch 69/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1541 - accuracy: 0.9583\n",
            "Epoch 70/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1455 - accuracy: 0.9605\n",
            "Score for fold 2: loss of 0.11963732540607452; accuracy of 97.20430374145508%\n",
            "Epoch 1/200\n",
            "189/189 [==============================] - 32s 150ms/step - loss: 3.1442 - accuracy: 0.2803\n",
            "Epoch 2/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 1.4341 - accuracy: 0.6380\n",
            "Epoch 3/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.9267 - accuracy: 0.7648\n",
            "Epoch 4/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.7462 - accuracy: 0.8107\n",
            "Epoch 5/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.6219 - accuracy: 0.8407\n",
            "Epoch 6/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.5617 - accuracy: 0.8554\n",
            "Epoch 7/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.5181 - accuracy: 0.8660\n",
            "Epoch 8/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.4705 - accuracy: 0.8795\n",
            "Epoch 9/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.4335 - accuracy: 0.8893\n",
            "Epoch 10/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.4004 - accuracy: 0.8964\n",
            "Epoch 11/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.3849 - accuracy: 0.8995\n",
            "Epoch 12/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.3812 - accuracy: 0.9001\n",
            "Epoch 13/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.3527 - accuracy: 0.9075\n",
            "Epoch 14/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.3429 - accuracy: 0.9095\n",
            "Epoch 15/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.3268 - accuracy: 0.9135\n",
            "Epoch 16/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.3090 - accuracy: 0.9179\n",
            "Epoch 17/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.3142 - accuracy: 0.9196\n",
            "Epoch 18/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.3068 - accuracy: 0.9203\n",
            "Epoch 19/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2820 - accuracy: 0.9258\n",
            "Epoch 20/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.2624 - accuracy: 0.9298\n",
            "Epoch 21/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.2769 - accuracy: 0.9256\n",
            "Epoch 22/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2646 - accuracy: 0.9302\n",
            "Epoch 23/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2646 - accuracy: 0.9303\n",
            "Epoch 24/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.2549 - accuracy: 0.9323\n",
            "Epoch 25/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2503 - accuracy: 0.9325\n",
            "Epoch 26/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2437 - accuracy: 0.9352\n",
            "Epoch 27/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2349 - accuracy: 0.9387\n",
            "Epoch 28/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2392 - accuracy: 0.9364\n",
            "Epoch 29/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2331 - accuracy: 0.9375\n",
            "Epoch 30/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2277 - accuracy: 0.9391\n",
            "Epoch 31/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.2252 - accuracy: 0.9390\n",
            "Epoch 32/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.2187 - accuracy: 0.9407\n",
            "Epoch 33/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.2180 - accuracy: 0.9409\n",
            "Epoch 34/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.2065 - accuracy: 0.9459\n",
            "Epoch 35/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.2093 - accuracy: 0.9440\n",
            "Epoch 36/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.2025 - accuracy: 0.9477\n",
            "Epoch 37/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.2005 - accuracy: 0.9462\n",
            "Epoch 38/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1945 - accuracy: 0.9477\n",
            "Epoch 39/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1979 - accuracy: 0.9464\n",
            "Epoch 40/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1845 - accuracy: 0.9507\n",
            "Epoch 41/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1966 - accuracy: 0.9471\n",
            "Epoch 42/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1814 - accuracy: 0.9510\n",
            "Epoch 43/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1854 - accuracy: 0.9508\n",
            "Epoch 44/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1782 - accuracy: 0.9521\n",
            "Epoch 45/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1914 - accuracy: 0.9490\n",
            "Epoch 46/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1870 - accuracy: 0.9490\n",
            "Epoch 47/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1863 - accuracy: 0.9499\n",
            "Epoch 48/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1831 - accuracy: 0.9509\n",
            "Epoch 49/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1692 - accuracy: 0.9541\n",
            "Epoch 50/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1760 - accuracy: 0.9543\n",
            "Epoch 51/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1762 - accuracy: 0.9535\n",
            "Epoch 52/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1705 - accuracy: 0.9537\n",
            "Epoch 53/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1739 - accuracy: 0.9528\n",
            "Epoch 54/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1652 - accuracy: 0.9554\n",
            "Epoch 55/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.1571 - accuracy: 0.9579\n",
            "Epoch 56/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1542 - accuracy: 0.9601\n",
            "Epoch 57/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1560 - accuracy: 0.9574\n",
            "Epoch 58/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1599 - accuracy: 0.9578\n",
            "Epoch 59/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1737 - accuracy: 0.9538\n",
            "Epoch 60/200\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 0.1641 - accuracy: 0.9539\n",
            "Epoch 61/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.1569 - accuracy: 0.9567\n",
            "Score for fold 3: loss of 0.19478358328342438; accuracy of 95.07030844688416%\n",
            "Epoch 1/200\n",
            "189/189 [==============================] - 34s 159ms/step - loss: 3.0749 - accuracy: 0.2907\n",
            "Epoch 2/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 1.4266 - accuracy: 0.6428\n",
            "Epoch 3/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.9371 - accuracy: 0.7606\n",
            "Epoch 4/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.7420 - accuracy: 0.8114\n",
            "Epoch 5/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.6297 - accuracy: 0.8405\n",
            "Epoch 6/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.5537 - accuracy: 0.8564\n",
            "Epoch 7/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.5111 - accuracy: 0.8680\n",
            "Epoch 8/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.4670 - accuracy: 0.8799\n",
            "Epoch 9/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.4346 - accuracy: 0.8884\n",
            "Epoch 10/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.4032 - accuracy: 0.8945\n",
            "Epoch 11/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.3959 - accuracy: 0.8958\n",
            "Epoch 12/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.3674 - accuracy: 0.9014\n",
            "Epoch 13/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.3545 - accuracy: 0.9056\n",
            "Epoch 14/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.3419 - accuracy: 0.9102\n",
            "Epoch 15/200\n",
            "189/189 [==============================] - 28s 149ms/step - loss: 0.3239 - accuracy: 0.9148\n",
            "Epoch 16/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.3089 - accuracy: 0.9157\n",
            "Epoch 17/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.3098 - accuracy: 0.9189\n",
            "Epoch 18/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2942 - accuracy: 0.9208\n",
            "Epoch 19/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2888 - accuracy: 0.9235\n",
            "Epoch 20/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2773 - accuracy: 0.9263\n",
            "Epoch 21/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2718 - accuracy: 0.9271\n",
            "Epoch 22/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2619 - accuracy: 0.9293\n",
            "Epoch 23/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2504 - accuracy: 0.9346\n",
            "Epoch 24/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.2506 - accuracy: 0.9322\n",
            "Epoch 25/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2557 - accuracy: 0.9307\n",
            "Epoch 26/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2402 - accuracy: 0.9353\n",
            "Epoch 27/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.2422 - accuracy: 0.9349\n",
            "Epoch 28/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2417 - accuracy: 0.9359\n",
            "Epoch 29/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2261 - accuracy: 0.9397\n",
            "Epoch 30/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2261 - accuracy: 0.9398\n",
            "Epoch 31/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.2167 - accuracy: 0.9441\n",
            "Epoch 32/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.2208 - accuracy: 0.9411\n",
            "Epoch 33/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2077 - accuracy: 0.9439\n",
            "Epoch 34/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.2079 - accuracy: 0.9452\n",
            "Epoch 35/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1992 - accuracy: 0.9463\n",
            "Epoch 36/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.1997 - accuracy: 0.9466\n",
            "Epoch 37/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1934 - accuracy: 0.9486\n",
            "Epoch 38/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.1956 - accuracy: 0.9475\n",
            "Epoch 39/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2028 - accuracy: 0.9458\n",
            "Epoch 40/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1889 - accuracy: 0.9484\n",
            "Epoch 41/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.1905 - accuracy: 0.9489\n",
            "Epoch 42/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1902 - accuracy: 0.9476\n",
            "Epoch 43/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1845 - accuracy: 0.9505\n",
            "Epoch 44/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1900 - accuracy: 0.9480\n",
            "Epoch 45/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1802 - accuracy: 0.9517\n",
            "Epoch 46/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1825 - accuracy: 0.9510\n",
            "Epoch 47/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1739 - accuracy: 0.9532\n",
            "Epoch 48/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1703 - accuracy: 0.9541\n",
            "Epoch 49/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1653 - accuracy: 0.9553\n",
            "Epoch 50/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1706 - accuracy: 0.9550\n",
            "Epoch 51/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1723 - accuracy: 0.9541\n",
            "Epoch 52/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1571 - accuracy: 0.9572\n",
            "Epoch 53/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.1759 - accuracy: 0.9537\n",
            "Epoch 54/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.1701 - accuracy: 0.9547\n",
            "Epoch 55/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1686 - accuracy: 0.9534\n",
            "Epoch 56/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.1666 - accuracy: 0.9547\n",
            "Epoch 57/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1577 - accuracy: 0.9572\n",
            "Score for fold 4: loss of 0.1625102460384369; accuracy of 96.04566693305969%\n",
            "Epoch 1/200\n",
            "189/189 [==============================] - 32s 151ms/step - loss: 3.2806 - accuracy: 0.2477\n",
            "Epoch 2/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 1.4425 - accuracy: 0.6389\n",
            "Epoch 3/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.9466 - accuracy: 0.7566\n",
            "Epoch 4/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.7419 - accuracy: 0.8088\n",
            "Epoch 5/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.6222 - accuracy: 0.8406\n",
            "Epoch 6/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.5414 - accuracy: 0.8598\n",
            "Epoch 7/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.5025 - accuracy: 0.8677\n",
            "Epoch 8/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.4614 - accuracy: 0.8802\n",
            "Epoch 9/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.4235 - accuracy: 0.8889\n",
            "Epoch 10/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.4115 - accuracy: 0.8889\n",
            "Epoch 11/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.3852 - accuracy: 0.8977\n",
            "Epoch 12/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.3528 - accuracy: 0.9094\n",
            "Epoch 13/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.3370 - accuracy: 0.9100\n",
            "Epoch 14/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.3273 - accuracy: 0.9134\n",
            "Epoch 15/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.3155 - accuracy: 0.9171\n",
            "Epoch 16/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.3106 - accuracy: 0.9194\n",
            "Epoch 17/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2956 - accuracy: 0.9217\n",
            "Epoch 18/200\n",
            "189/189 [==============================] - 29s 151ms/step - loss: 0.2874 - accuracy: 0.9233\n",
            "Epoch 19/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2821 - accuracy: 0.9240\n",
            "Epoch 20/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2638 - accuracy: 0.9286\n",
            "Epoch 21/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2619 - accuracy: 0.9292\n",
            "Epoch 22/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2568 - accuracy: 0.9312\n",
            "Epoch 23/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2482 - accuracy: 0.9337\n",
            "Epoch 24/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.2491 - accuracy: 0.9344\n",
            "Epoch 25/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2362 - accuracy: 0.9367\n",
            "Epoch 26/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2346 - accuracy: 0.9369\n",
            "Epoch 27/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2441 - accuracy: 0.9355\n",
            "Epoch 28/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2273 - accuracy: 0.9399\n",
            "Epoch 29/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2168 - accuracy: 0.9417\n",
            "Epoch 30/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2102 - accuracy: 0.9421\n",
            "Epoch 31/200\n",
            "189/189 [==============================] - 29s 152ms/step - loss: 0.2188 - accuracy: 0.9392\n",
            "Epoch 32/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2104 - accuracy: 0.9435\n",
            "Epoch 33/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2068 - accuracy: 0.9445\n",
            "Epoch 34/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2053 - accuracy: 0.9450\n",
            "Epoch 35/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2011 - accuracy: 0.9454\n",
            "Epoch 36/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.2006 - accuracy: 0.9458\n",
            "Epoch 37/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.2016 - accuracy: 0.9464\n",
            "Epoch 38/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1877 - accuracy: 0.9489\n",
            "Epoch 39/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.1922 - accuracy: 0.9471\n",
            "Epoch 40/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1834 - accuracy: 0.9508\n",
            "Epoch 41/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1859 - accuracy: 0.9510\n",
            "Epoch 42/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1841 - accuracy: 0.9514\n",
            "Epoch 43/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1873 - accuracy: 0.9500\n",
            "Epoch 44/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1823 - accuracy: 0.9513\n",
            "Epoch 45/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1766 - accuracy: 0.9522\n",
            "Epoch 46/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1750 - accuracy: 0.9522\n",
            "Epoch 47/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1742 - accuracy: 0.9532\n",
            "Epoch 48/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1749 - accuracy: 0.9526\n",
            "Epoch 49/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1754 - accuracy: 0.9519\n",
            "Epoch 50/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1807 - accuracy: 0.9514\n",
            "Epoch 51/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1716 - accuracy: 0.9545\n",
            "Epoch 52/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1552 - accuracy: 0.9593\n",
            "Epoch 53/200\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 0.1712 - accuracy: 0.9540\n",
            "Epoch 54/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1645 - accuracy: 0.9562\n",
            "Epoch 55/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1639 - accuracy: 0.9558\n",
            "Epoch 56/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1559 - accuracy: 0.9579\n",
            "Epoch 57/200\n",
            "189/189 [==============================] - 28s 151ms/step - loss: 0.1663 - accuracy: 0.9556\n",
            "Score for fold 5: loss of 0.14939068257808685; accuracy of 95.94639539718628%\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 128, 3)]     0           []                               \n",
            "                                                                                                  \n",
            " spatial_dropout1d_5 (SpatialDr  (None, 128, 3)      0           ['input_3[0][0]']                \n",
            " opout1D)                                                                                         \n",
            "                                                                                                  \n",
            " tnc1_initial_conv (Conv1D)     (None, 128, 128)     512         ['spatial_dropout1d_5[0][0]']    \n",
            "                                                                                                  \n",
            " tnc1_dilated_conv_1_tanh_s0 (C  (None, 128, 128)    49280       ['tnc1_initial_conv[0][0]']      \n",
            " onv1D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_60 (Activation)     (None, 128, 128)     0           ['tnc1_dilated_conv_1_tanh_s0[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " lambda_50 (Lambda)             (None, 128, 128)     0           ['activation_60[0][0]']          \n",
            "                                                                                                  \n",
            " tnc1_spatial_dropout1d_1_s0_0.  (None, 128, 128)    0           ['lambda_50[0][0]']              \n",
            " 000000 (SpatialDropout1D)                                                                        \n",
            "                                                                                                  \n",
            " conv1d_50 (Conv1D)             (None, 128, 128)     16512       ['tnc1_spatial_dropout1d_1_s0_0.0\n",
            "                                                                 00000[0][0]']                    \n",
            "                                                                                                  \n",
            " add_60 (Add)                   (None, 128, 128)     0           ['tnc1_initial_conv[0][0]',      \n",
            "                                                                  'conv1d_50[0][0]']              \n",
            "                                                                                                  \n",
            " tnc1_dilated_conv_2_tanh_s0 (C  (None, 128, 128)    49280       ['add_60[0][0]']                 \n",
            " onv1D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_61 (Activation)     (None, 128, 128)     0           ['tnc1_dilated_conv_2_tanh_s0[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " lambda_51 (Lambda)             (None, 128, 128)     0           ['activation_61[0][0]']          \n",
            "                                                                                                  \n",
            " tnc1_spatial_dropout1d_2_s0_0.  (None, 128, 128)    0           ['lambda_51[0][0]']              \n",
            " 000000 (SpatialDropout1D)                                                                        \n",
            "                                                                                                  \n",
            " conv1d_51 (Conv1D)             (None, 128, 128)     16512       ['tnc1_spatial_dropout1d_2_s0_0.0\n",
            "                                                                 00000[0][0]']                    \n",
            "                                                                                                  \n",
            " add_61 (Add)                   (None, 128, 128)     0           ['add_60[0][0]',                 \n",
            "                                                                  'conv1d_51[0][0]']              \n",
            "                                                                                                  \n",
            " tnc1_dilated_conv_4_tanh_s0 (C  (None, 128, 128)    49280       ['add_61[0][0]']                 \n",
            " onv1D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_62 (Activation)     (None, 128, 128)     0           ['tnc1_dilated_conv_4_tanh_s0[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " lambda_52 (Lambda)             (None, 128, 128)     0           ['activation_62[0][0]']          \n",
            "                                                                                                  \n",
            " tnc1_spatial_dropout1d_4_s0_0.  (None, 128, 128)    0           ['lambda_52[0][0]']              \n",
            " 000000 (SpatialDropout1D)                                                                        \n",
            "                                                                                                  \n",
            " conv1d_52 (Conv1D)             (None, 128, 128)     16512       ['tnc1_spatial_dropout1d_4_s0_0.0\n",
            "                                                                 00000[0][0]']                    \n",
            "                                                                                                  \n",
            " add_62 (Add)                   (None, 128, 128)     0           ['add_61[0][0]',                 \n",
            "                                                                  'conv1d_52[0][0]']              \n",
            "                                                                                                  \n",
            " tnc1_dilated_conv_8_tanh_s0 (C  (None, 128, 128)    49280       ['add_62[0][0]']                 \n",
            " onv1D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_63 (Activation)     (None, 128, 128)     0           ['tnc1_dilated_conv_8_tanh_s0[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " lambda_53 (Lambda)             (None, 128, 128)     0           ['activation_63[0][0]']          \n",
            "                                                                                                  \n",
            " tnc1_spatial_dropout1d_8_s0_0.  (None, 128, 128)    0           ['lambda_53[0][0]']              \n",
            " 000000 (SpatialDropout1D)                                                                        \n",
            "                                                                                                  \n",
            " conv1d_53 (Conv1D)             (None, 128, 128)     16512       ['tnc1_spatial_dropout1d_8_s0_0.0\n",
            "                                                                 00000[0][0]']                    \n",
            "                                                                                                  \n",
            " add_63 (Add)                   (None, 128, 128)     0           ['add_62[0][0]',                 \n",
            "                                                                  'conv1d_53[0][0]']              \n",
            "                                                                                                  \n",
            " tnc1_dilated_conv_16_tanh_s0 (  (None, 128, 128)    49280       ['add_63[0][0]']                 \n",
            " Conv1D)                                                                                          \n",
            "                                                                                                  \n",
            " activation_64 (Activation)     (None, 128, 128)     0           ['tnc1_dilated_conv_16_tanh_s0[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " lambda_54 (Lambda)             (None, 128, 128)     0           ['activation_64[0][0]']          \n",
            "                                                                                                  \n",
            " tnc1_spatial_dropout1d_16_s0_0  (None, 128, 128)    0           ['lambda_54[0][0]']              \n",
            " .000000 (SpatialDropout1D)                                                                       \n",
            "                                                                                                  \n",
            " conv1d_54 (Conv1D)             (None, 128, 128)     16512       ['tnc1_spatial_dropout1d_16_s0_0.\n",
            "                                                                 000000[0][0]']                   \n",
            "                                                                                                  \n",
            " add_65 (Add)                   (None, 128, 128)     0           ['conv1d_50[0][0]',              \n",
            "                                                                  'conv1d_51[0][0]',              \n",
            "                                                                  'conv1d_52[0][0]',              \n",
            "                                                                  'conv1d_53[0][0]',              \n",
            "                                                                  'conv1d_54[0][0]']              \n",
            "                                                                                                  \n",
            " activation_65 (Activation)     (None, 128, 128)     0           ['add_65[0][0]']                 \n",
            "                                                                                                  \n",
            " tnc2_initial_conv (Conv1D)     (None, 128, 128)     16512       ['activation_65[0][0]']          \n",
            "                                                                                                  \n",
            " tnc2_dilated_conv_1_tanh_s0 (C  (None, 128, 128)    49280       ['tnc2_initial_conv[0][0]']      \n",
            " onv1D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_66 (Activation)     (None, 128, 128)     0           ['tnc2_dilated_conv_1_tanh_s0[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " lambda_55 (Lambda)             (None, 128, 128)     0           ['activation_66[0][0]']          \n",
            "                                                                                                  \n",
            " tnc2_spatial_dropout1d_1_s0_0.  (None, 128, 128)    0           ['lambda_55[0][0]']              \n",
            " 000000 (SpatialDropout1D)                                                                        \n",
            "                                                                                                  \n",
            " conv1d_55 (Conv1D)             (None, 128, 128)     16512       ['tnc2_spatial_dropout1d_1_s0_0.0\n",
            "                                                                 00000[0][0]']                    \n",
            "                                                                                                  \n",
            " add_66 (Add)                   (None, 128, 128)     0           ['tnc2_initial_conv[0][0]',      \n",
            "                                                                  'conv1d_55[0][0]']              \n",
            "                                                                                                  \n",
            " tnc2_dilated_conv_2_tanh_s0 (C  (None, 128, 128)    49280       ['add_66[0][0]']                 \n",
            " onv1D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_67 (Activation)     (None, 128, 128)     0           ['tnc2_dilated_conv_2_tanh_s0[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " lambda_56 (Lambda)             (None, 128, 128)     0           ['activation_67[0][0]']          \n",
            "                                                                                                  \n",
            " tnc2_spatial_dropout1d_2_s0_0.  (None, 128, 128)    0           ['lambda_56[0][0]']              \n",
            " 000000 (SpatialDropout1D)                                                                        \n",
            "                                                                                                  \n",
            " conv1d_56 (Conv1D)             (None, 128, 128)     16512       ['tnc2_spatial_dropout1d_2_s0_0.0\n",
            "                                                                 00000[0][0]']                    \n",
            "                                                                                                  \n",
            " add_67 (Add)                   (None, 128, 128)     0           ['add_66[0][0]',                 \n",
            "                                                                  'conv1d_56[0][0]']              \n",
            "                                                                                                  \n",
            " tnc2_dilated_conv_4_tanh_s0 (C  (None, 128, 128)    49280       ['add_67[0][0]']                 \n",
            " onv1D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_68 (Activation)     (None, 128, 128)     0           ['tnc2_dilated_conv_4_tanh_s0[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " lambda_57 (Lambda)             (None, 128, 128)     0           ['activation_68[0][0]']          \n",
            "                                                                                                  \n",
            " tnc2_spatial_dropout1d_4_s0_0.  (None, 128, 128)    0           ['lambda_57[0][0]']              \n",
            " 000000 (SpatialDropout1D)                                                                        \n",
            "                                                                                                  \n",
            " conv1d_57 (Conv1D)             (None, 128, 128)     16512       ['tnc2_spatial_dropout1d_4_s0_0.0\n",
            "                                                                 00000[0][0]']                    \n",
            "                                                                                                  \n",
            " add_68 (Add)                   (None, 128, 128)     0           ['add_67[0][0]',                 \n",
            "                                                                  'conv1d_57[0][0]']              \n",
            "                                                                                                  \n",
            " tnc2_dilated_conv_8_tanh_s0 (C  (None, 128, 128)    49280       ['add_68[0][0]']                 \n",
            " onv1D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_69 (Activation)     (None, 128, 128)     0           ['tnc2_dilated_conv_8_tanh_s0[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " lambda_58 (Lambda)             (None, 128, 128)     0           ['activation_69[0][0]']          \n",
            "                                                                                                  \n",
            " tnc2_spatial_dropout1d_8_s0_0.  (None, 128, 128)    0           ['lambda_58[0][0]']              \n",
            " 000000 (SpatialDropout1D)                                                                        \n",
            "                                                                                                  \n",
            " conv1d_58 (Conv1D)             (None, 128, 128)     16512       ['tnc2_spatial_dropout1d_8_s0_0.0\n",
            "                                                                 00000[0][0]']                    \n",
            "                                                                                                  \n",
            " add_69 (Add)                   (None, 128, 128)     0           ['add_68[0][0]',                 \n",
            "                                                                  'conv1d_58[0][0]']              \n",
            "                                                                                                  \n",
            " tnc2_dilated_conv_16_tanh_s0 (  (None, 128, 128)    49280       ['add_69[0][0]']                 \n",
            " Conv1D)                                                                                          \n",
            "                                                                                                  \n",
            " activation_70 (Activation)     (None, 128, 128)     0           ['tnc2_dilated_conv_16_tanh_s0[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " lambda_59 (Lambda)             (None, 128, 128)     0           ['activation_70[0][0]']          \n",
            "                                                                                                  \n",
            " tnc2_spatial_dropout1d_16_s0_0  (None, 128, 128)    0           ['lambda_59[0][0]']              \n",
            " .000000 (SpatialDropout1D)                                                                       \n",
            "                                                                                                  \n",
            " conv1d_59 (Conv1D)             (None, 128, 128)     16512       ['tnc2_spatial_dropout1d_16_s0_0.\n",
            "                                                                 000000[0][0]']                   \n",
            "                                                                                                  \n",
            " add_71 (Add)                   (None, 128, 128)     0           ['conv1d_55[0][0]',              \n",
            "                                                                  'conv1d_56[0][0]',              \n",
            "                                                                  'conv1d_57[0][0]',              \n",
            "                                                                  'conv1d_58[0][0]',              \n",
            "                                                                  'conv1d_59[0][0]']              \n",
            "                                                                                                  \n",
            " activation_71 (Activation)     (None, 128, 128)     0           ['add_71[0][0]']                 \n",
            "                                                                                                  \n",
            " global_max_pooling1d_5 (Global  (None, 128)         0           ['activation_71[0][0]']          \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 128)          16512       ['global_max_pooling1d_5[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 128)          0           ['dense_10[0][0]']               \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 118)          15222       ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 706,678\n",
            "Trainable params: 706,678\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDA-lbMQ6tWg",
        "outputId": "cd88bf70-4546-49a8-a217-c75b9d135e70"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.15169847011566162 - Accuracy: 96.24482989311218%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.11963732540607452 - Accuracy: 97.20430374145508%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.19478358328342438 - Accuracy: 95.07030844688416%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.1625102460384369 - Accuracy: 96.04566693305969%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.14939068257808685 - Accuracy: 95.94639539718628%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 96.10230088233948 (+- 0.6822385406527091)\n",
            "> Loss: 0.15560406148433686\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Final_Stratified_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
