{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YQFREF7L5GqL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6913026c-1f97-449d-980f-bd245d7ed9ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/'  #change dir to your project folder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import save, load\n",
        "from pandas import read_csv\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras import Model\n",
        "from keras.layers import Conv1D, SpatialDropout1D\n",
        "from keras.layers import Convolution1D, Dense,Activation\n",
        "from keras.models import Input, Model\n",
        "from keras.layers import LSTM, Conv1D, concatenate,GlobalMaxPooling1D,GlobalAveragePooling1D,TimeDistributed, MaxPooling1D\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "import keras.layers\n",
        "from keras import optimizers\n",
        "from keras.layers import Activation, Lambda\n",
        "from keras.layers import Convolution1D, Dense\n",
        "from keras.models import Input, Model\n",
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "metadata": {
        "id": "fc27_BqAfIsY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxaKZAFk5TvK",
        "outputId": "b7a2ec62-9593-4e7a-8c8a-2e9854dbe274"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(33104, 128, 6) (33104, 118) (3740, 128, 6) (3740, 118)\n"
          ]
        }
      ],
      "source": [
        "x_train = np.load('gdrive/My Drive/dataset1/acc+gyr/trainX.npy')\n",
        "y_train = np.load('gdrive/My Drive/dataset1/acc+gyr/trainy.npy')\n",
        "x_test = np.load('gdrive/My Drive/dataset1/acc+gyr/testX.npy')\n",
        "y_test = np.load('gdrive/My Drive/dataset1/acc+gyr/testy.npy')\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge inputs and targets\n",
        "inputs = np.concatenate((x_train, x_test), axis=0)\n",
        "targets = np.concatenate((y_train, y_test), axis=0)\n"
      ],
      "metadata": {
        "id": "qUlefJ0211kN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the cross-validation procedure\n",
        "#cv = KFold(n_splits=10, random_state=1, shuffle=True)"
      ],
      "metadata": {
        "id": "NqIuESQUkJAB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "OqP6bY3k5UZH"
      },
      "outputs": [],
      "source": [
        "#from sklearn.model_selection import train_test_split\n",
        "#x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=0.20, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def channel_normalization(x):\n",
        "    max_values = K.max(K.abs(x), 2, keepdims=True) + 1e-5\n",
        "    out = x / max_values\n",
        "    return out\n",
        "\n",
        "def residual_block(x, s, i, activation, nb_filters, kernel_size, padding, dropout_rate=0, name=''):\n",
        "    original_x = x\n",
        "    conv = Conv1D(filters=nb_filters, kernel_size=kernel_size,\n",
        "                  dilation_rate=i, padding=padding,\n",
        "                  name=name + '_dilated_conv_%d_tanh_s%d' % (i, s))(x)\n",
        "    if activation == 'norm_relu':\n",
        "        x = Activation('relu')(conv)\n",
        "        x = Lambda(channel_normalization)(x)\n",
        "    else:\n",
        "        x = Activation(activation)(conv)\n",
        "\n",
        "    x = SpatialDropout1D(dropout_rate, name=name + '_spatial_dropout1d_%d_s%d_%f' % (i, s, dropout_rate))(x)\n",
        "\n",
        "    # 1x1 conv.\n",
        "    x = Convolution1D(nb_filters, 1, padding='same')(x)\n",
        "    res_x = keras.layers.add([original_x, x])\n",
        "    return res_x, x"
      ],
      "metadata": {
        "id": "tRsgs0Xp22cS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TCN:\n",
        "\n",
        "    def __init__(self,\n",
        "                 nb_filters=64,\n",
        "                 kernel_size=2,\n",
        "                 nb_stacks=1,\n",
        "                 dilations=None,\n",
        "                 activation='norm_relu',\n",
        "                 padding='causal',\n",
        "                 use_skip_connections=True,\n",
        "                 dropout_rate=0.0,\n",
        "                 return_sequences=True,\n",
        "                 name='tcn'):\n",
        "        self.name = name\n",
        "        self.return_sequences = return_sequences\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.use_skip_connections = use_skip_connections\n",
        "        self.activation = activation\n",
        "        self.dilations = dilations\n",
        "        self.nb_stacks = nb_stacks\n",
        "        self.kernel_size = kernel_size\n",
        "        self.nb_filters = nb_filters\n",
        "        self.padding = padding\n",
        "        \n",
        "        if padding != 'causal' and padding != 'same':\n",
        "            raise ValueError(\"Only 'causal' or 'same' paddings are compatible for this layer.\")\n",
        "\n",
        "        if not isinstance(nb_filters, int):\n",
        "            print('An interface change occurred after the version 2.1.2.')\n",
        "            print('Before: tcn.TCN(i, return_sequences=False, ...)')\n",
        "            print('Now should be: tcn.TCN(return_sequences=False, ...)(i)')\n",
        "            print('Second solution is to pip install keras-tcn==2.1.2 to downgrade.')\n",
        "            raise Exception()\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        if self.dilations is None:\n",
        "            self.dilations = [1, 2, 4, 8, 16, 32]\n",
        "        x = inputs\n",
        "        x = Convolution1D(self.nb_filters, 1, padding=self.padding, name=self.name + '_initial_conv')(x)\n",
        "        skip_connections = []\n",
        "        for s in range(self.nb_stacks):\n",
        "            for i in self.dilations:\n",
        "                x, skip_out = residual_block(x, s, i, self.activation, self.nb_filters,\n",
        "                                             self.kernel_size, self.padding, self.dropout_rate, name=self.name)\n",
        "                skip_connections.append(skip_out)\n",
        "        if self.use_skip_connections:\n",
        "            x = keras.layers.add(skip_connections)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "        if not self.return_sequences:\n",
        "            output_slice_index = -1\n",
        "            x = Lambda(lambda tt: tt[:, output_slice_index, :])(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "alKxQs751llb"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fampnwRy5XUm",
        "outputId": "bb399d83-bda5-406c-8439-27d862ddcefb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "231/231 [==============================] - 20s 73ms/step - loss: 2.7636 - accuracy: 0.3370\n",
            "Epoch 2/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.9140 - accuracy: 0.7624\n",
            "Epoch 3/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.5463 - accuracy: 0.8611\n",
            "Epoch 4/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.4032 - accuracy: 0.8941\n",
            "Epoch 5/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.3306 - accuracy: 0.9141\n",
            "Epoch 6/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.2967 - accuracy: 0.9195\n",
            "Epoch 7/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.2478 - accuracy: 0.9318\n",
            "Epoch 8/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.2297 - accuracy: 0.9381\n",
            "Epoch 9/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.2083 - accuracy: 0.9441\n",
            "Epoch 10/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1912 - accuracy: 0.9470\n",
            "Epoch 11/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1833 - accuracy: 0.9512\n",
            "Epoch 12/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1673 - accuracy: 0.9549\n",
            "Epoch 13/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1566 - accuracy: 0.9568\n",
            "Epoch 14/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1531 - accuracy: 0.9562\n",
            "Epoch 15/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1396 - accuracy: 0.9612\n",
            "Epoch 16/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1400 - accuracy: 0.9604\n",
            "Epoch 17/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1411 - accuracy: 0.9614\n",
            "Epoch 18/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1327 - accuracy: 0.9628\n",
            "Epoch 19/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1304 - accuracy: 0.9628\n",
            "Epoch 20/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1217 - accuracy: 0.9654\n",
            "Epoch 21/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1095 - accuracy: 0.9681\n",
            "Epoch 22/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1086 - accuracy: 0.9676\n",
            "Epoch 23/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1068 - accuracy: 0.9702\n",
            "Epoch 24/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1025 - accuracy: 0.9715\n",
            "Epoch 25/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1080 - accuracy: 0.9689\n",
            "Epoch 26/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1007 - accuracy: 0.9718\n",
            "Epoch 27/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1007 - accuracy: 0.9716\n",
            "Epoch 28/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0975 - accuracy: 0.9725\n",
            "Epoch 29/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0910 - accuracy: 0.9731\n",
            "Epoch 30/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0972 - accuracy: 0.9720\n",
            "Epoch 31/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0914 - accuracy: 0.9727\n",
            "Epoch 32/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0883 - accuracy: 0.9742\n",
            "Epoch 33/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0856 - accuracy: 0.9759\n",
            "Epoch 34/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0840 - accuracy: 0.9752\n",
            "Epoch 35/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0870 - accuracy: 0.9748\n",
            "Epoch 36/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0859 - accuracy: 0.9753\n",
            "Epoch 37/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0801 - accuracy: 0.9768\n",
            "Epoch 38/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0774 - accuracy: 0.9772\n",
            "Epoch 39/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0783 - accuracy: 0.9771\n",
            "Epoch 40/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0785 - accuracy: 0.9773\n",
            "Epoch 41/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0755 - accuracy: 0.9774\n",
            "Epoch 42/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0760 - accuracy: 0.9783\n",
            "Epoch 43/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0718 - accuracy: 0.9787\n",
            "Epoch 44/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0680 - accuracy: 0.9807\n",
            "Epoch 45/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0713 - accuracy: 0.9786\n",
            "Epoch 46/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0665 - accuracy: 0.9801\n",
            "Epoch 47/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0669 - accuracy: 0.9799\n",
            "Epoch 48/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0687 - accuracy: 0.9797\n",
            "Epoch 49/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0624 - accuracy: 0.9823\n",
            "Epoch 50/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0683 - accuracy: 0.9803\n",
            "Epoch 51/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0723 - accuracy: 0.9785\n",
            "Epoch 52/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0592 - accuracy: 0.9824\n",
            "Epoch 53/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0638 - accuracy: 0.9809\n",
            "Epoch 54/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0604 - accuracy: 0.9822\n",
            "Epoch 55/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0530 - accuracy: 0.9844\n",
            "Epoch 56/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0620 - accuracy: 0.9822\n",
            "Epoch 57/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0639 - accuracy: 0.9808\n",
            "Epoch 58/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0643 - accuracy: 0.9813\n",
            "Epoch 59/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0530 - accuracy: 0.9847\n",
            "Epoch 60/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0582 - accuracy: 0.9826\n",
            "Score for fold 1: loss of 0.10499321669340134; accuracy of 97.5709080696106%\n",
            "Epoch 1/200\n",
            "231/231 [==============================] - 18s 68ms/step - loss: 2.9189 - accuracy: 0.3092\n",
            "Epoch 2/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.9983 - accuracy: 0.7414\n",
            "Epoch 3/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.5433 - accuracy: 0.8592\n",
            "Epoch 4/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.3930 - accuracy: 0.8984\n",
            "Epoch 5/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.3195 - accuracy: 0.9168\n",
            "Epoch 6/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.2759 - accuracy: 0.9280\n",
            "Epoch 7/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.2415 - accuracy: 0.9353\n",
            "Epoch 8/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.2167 - accuracy: 0.9425\n",
            "Epoch 9/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.2057 - accuracy: 0.9437\n",
            "Epoch 10/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1908 - accuracy: 0.9485\n",
            "Epoch 11/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1813 - accuracy: 0.9507\n",
            "Epoch 12/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1622 - accuracy: 0.9554\n",
            "Epoch 13/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1594 - accuracy: 0.9567\n",
            "Epoch 14/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1476 - accuracy: 0.9585\n",
            "Epoch 15/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1451 - accuracy: 0.9612\n",
            "Epoch 16/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1454 - accuracy: 0.9585\n",
            "Epoch 17/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1275 - accuracy: 0.9643\n",
            "Epoch 18/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1276 - accuracy: 0.9643\n",
            "Epoch 19/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1232 - accuracy: 0.9634\n",
            "Epoch 20/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1185 - accuracy: 0.9662\n",
            "Epoch 21/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1157 - accuracy: 0.9670\n",
            "Epoch 22/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1138 - accuracy: 0.9674\n",
            "Epoch 23/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1022 - accuracy: 0.9704\n",
            "Epoch 24/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1099 - accuracy: 0.9685\n",
            "Epoch 25/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0995 - accuracy: 0.9719\n",
            "Epoch 26/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0950 - accuracy: 0.9725\n",
            "Epoch 27/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0952 - accuracy: 0.9717\n",
            "Epoch 28/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0983 - accuracy: 0.9710\n",
            "Epoch 29/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0900 - accuracy: 0.9734\n",
            "Epoch 30/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0893 - accuracy: 0.9738\n",
            "Epoch 31/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0866 - accuracy: 0.9749\n",
            "Epoch 32/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0794 - accuracy: 0.9758\n",
            "Epoch 33/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0893 - accuracy: 0.9740\n",
            "Epoch 34/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0761 - accuracy: 0.9785\n",
            "Epoch 35/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0841 - accuracy: 0.9751\n",
            "Epoch 36/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0813 - accuracy: 0.9765\n",
            "Epoch 37/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0713 - accuracy: 0.9788\n",
            "Epoch 38/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0751 - accuracy: 0.9786\n",
            "Epoch 39/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0769 - accuracy: 0.9769\n",
            "Epoch 40/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0662 - accuracy: 0.9804\n",
            "Epoch 41/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0737 - accuracy: 0.9780\n",
            "Epoch 42/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0697 - accuracy: 0.9789\n",
            "Epoch 43/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0737 - accuracy: 0.9783\n",
            "Epoch 44/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0699 - accuracy: 0.9789\n",
            "Epoch 45/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0714 - accuracy: 0.9785\n",
            "Score for fold 2: loss of 0.09094121307134628; accuracy of 97.81517386436462%\n",
            "Epoch 1/200\n",
            "231/231 [==============================] - 18s 68ms/step - loss: 2.8724 - accuracy: 0.3203\n",
            "Epoch 2/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.9148 - accuracy: 0.7627\n",
            "Epoch 3/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.5214 - accuracy: 0.8656\n",
            "Epoch 4/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.3908 - accuracy: 0.8982\n",
            "Epoch 5/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.3197 - accuracy: 0.9168\n",
            "Epoch 6/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.2676 - accuracy: 0.9281\n",
            "Epoch 7/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.2426 - accuracy: 0.9350\n",
            "Epoch 8/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.2259 - accuracy: 0.9400\n",
            "Epoch 9/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1971 - accuracy: 0.9480\n",
            "Epoch 10/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1821 - accuracy: 0.9502\n",
            "Epoch 11/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1776 - accuracy: 0.9510\n",
            "Epoch 12/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1584 - accuracy: 0.9570\n",
            "Epoch 13/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1615 - accuracy: 0.9552\n",
            "Epoch 14/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1459 - accuracy: 0.9599\n",
            "Epoch 15/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1457 - accuracy: 0.9602\n",
            "Epoch 16/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1340 - accuracy: 0.9634\n",
            "Epoch 17/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1327 - accuracy: 0.9617\n",
            "Epoch 18/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1269 - accuracy: 0.9649\n",
            "Epoch 19/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1202 - accuracy: 0.9656\n",
            "Epoch 20/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1189 - accuracy: 0.9658\n",
            "Epoch 21/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1162 - accuracy: 0.9668\n",
            "Epoch 22/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1078 - accuracy: 0.9691\n",
            "Epoch 23/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1031 - accuracy: 0.9704\n",
            "Epoch 24/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0958 - accuracy: 0.9723\n",
            "Epoch 25/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1026 - accuracy: 0.9703\n",
            "Epoch 26/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0969 - accuracy: 0.9733\n",
            "Epoch 27/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0950 - accuracy: 0.9726\n",
            "Epoch 28/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0861 - accuracy: 0.9757\n",
            "Epoch 29/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0895 - accuracy: 0.9752\n",
            "Epoch 30/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0878 - accuracy: 0.9738\n",
            "Epoch 31/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0829 - accuracy: 0.9759\n",
            "Epoch 32/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0865 - accuracy: 0.9744\n",
            "Epoch 33/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0835 - accuracy: 0.9760\n",
            "Epoch 34/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0756 - accuracy: 0.9774\n",
            "Epoch 35/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0769 - accuracy: 0.9778\n",
            "Epoch 36/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0846 - accuracy: 0.9752\n",
            "Epoch 37/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0731 - accuracy: 0.9787\n",
            "Epoch 38/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0771 - accuracy: 0.9773\n",
            "Epoch 39/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0753 - accuracy: 0.9776\n",
            "Epoch 40/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0724 - accuracy: 0.9786\n",
            "Epoch 41/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0757 - accuracy: 0.9784\n",
            "Epoch 42/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0766 - accuracy: 0.9786\n",
            "Epoch 43/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0720 - accuracy: 0.9793\n",
            "Epoch 44/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0676 - accuracy: 0.9795\n",
            "Epoch 45/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0661 - accuracy: 0.9801\n",
            "Epoch 46/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0657 - accuracy: 0.9811\n",
            "Epoch 47/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0647 - accuracy: 0.9814\n",
            "Epoch 48/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0671 - accuracy: 0.9802\n",
            "Epoch 49/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0634 - accuracy: 0.9815\n",
            "Epoch 50/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0629 - accuracy: 0.9818\n",
            "Epoch 51/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0636 - accuracy: 0.9815\n",
            "Epoch 52/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0582 - accuracy: 0.9820\n",
            "Epoch 53/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0640 - accuracy: 0.9817\n",
            "Epoch 54/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0623 - accuracy: 0.9827\n",
            "Epoch 55/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0615 - accuracy: 0.9825\n",
            "Epoch 56/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0551 - accuracy: 0.9842\n",
            "Epoch 57/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0557 - accuracy: 0.9835\n",
            "Epoch 58/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0569 - accuracy: 0.9829\n",
            "Epoch 59/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0569 - accuracy: 0.9829\n",
            "Epoch 60/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0556 - accuracy: 0.9840\n",
            "Epoch 61/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0571 - accuracy: 0.9840\n",
            "Score for fold 3: loss of 0.09887058287858963; accuracy of 97.67946600914001%\n",
            "Epoch 1/200\n",
            "231/231 [==============================] - 18s 68ms/step - loss: 2.9577 - accuracy: 0.2933\n",
            "Epoch 2/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.9770 - accuracy: 0.7458\n",
            "Epoch 3/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.5471 - accuracy: 0.8560\n",
            "Epoch 4/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.4063 - accuracy: 0.8932\n",
            "Epoch 5/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.3280 - accuracy: 0.9150\n",
            "Epoch 6/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.2706 - accuracy: 0.9277\n",
            "Epoch 7/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.2392 - accuracy: 0.9349\n",
            "Epoch 8/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.2228 - accuracy: 0.9399\n",
            "Epoch 9/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.2039 - accuracy: 0.9439\n",
            "Epoch 10/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.1903 - accuracy: 0.9491\n",
            "Epoch 11/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.1733 - accuracy: 0.9528\n",
            "Epoch 12/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.1688 - accuracy: 0.9528\n",
            "Epoch 13/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.1580 - accuracy: 0.9558\n",
            "Epoch 14/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.1513 - accuracy: 0.9578\n",
            "Epoch 15/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1415 - accuracy: 0.9606\n",
            "Epoch 16/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1380 - accuracy: 0.9604\n",
            "Epoch 17/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1293 - accuracy: 0.9635\n",
            "Epoch 18/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1252 - accuracy: 0.9653\n",
            "Epoch 19/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1167 - accuracy: 0.9671\n",
            "Epoch 20/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1197 - accuracy: 0.9656\n",
            "Epoch 21/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1130 - accuracy: 0.9674\n",
            "Epoch 22/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1065 - accuracy: 0.9710\n",
            "Epoch 23/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.1049 - accuracy: 0.9699\n",
            "Epoch 24/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0996 - accuracy: 0.9705\n",
            "Epoch 25/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.1008 - accuracy: 0.9707\n",
            "Epoch 26/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0999 - accuracy: 0.9704\n",
            "Epoch 27/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0967 - accuracy: 0.9720\n",
            "Epoch 28/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0935 - accuracy: 0.9729\n",
            "Epoch 29/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0939 - accuracy: 0.9736\n",
            "Epoch 30/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0904 - accuracy: 0.9742\n",
            "Epoch 31/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0920 - accuracy: 0.9728\n",
            "Epoch 32/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0873 - accuracy: 0.9745\n",
            "Epoch 33/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0833 - accuracy: 0.9755\n",
            "Epoch 34/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0837 - accuracy: 0.9746\n",
            "Epoch 35/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0761 - accuracy: 0.9770\n",
            "Epoch 36/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0774 - accuracy: 0.9770\n",
            "Epoch 37/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0785 - accuracy: 0.9779\n",
            "Epoch 38/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0778 - accuracy: 0.9774\n",
            "Epoch 39/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0777 - accuracy: 0.9775\n",
            "Epoch 40/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0742 - accuracy: 0.9789\n",
            "Epoch 41/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0755 - accuracy: 0.9785\n",
            "Epoch 42/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0804 - accuracy: 0.9769\n",
            "Epoch 43/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0683 - accuracy: 0.9795\n",
            "Epoch 44/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0663 - accuracy: 0.9813\n",
            "Epoch 45/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0681 - accuracy: 0.9799\n",
            "Epoch 46/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0675 - accuracy: 0.9806\n",
            "Epoch 47/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0618 - accuracy: 0.9819\n",
            "Epoch 48/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0674 - accuracy: 0.9807\n",
            "Epoch 49/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0665 - accuracy: 0.9813\n",
            "Epoch 50/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0615 - accuracy: 0.9812\n",
            "Epoch 51/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0620 - accuracy: 0.9824\n",
            "Epoch 52/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0586 - accuracy: 0.9825\n",
            "Epoch 53/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0672 - accuracy: 0.9805\n",
            "Epoch 54/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0644 - accuracy: 0.9808\n",
            "Epoch 55/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0571 - accuracy: 0.9834\n",
            "Epoch 56/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0609 - accuracy: 0.9824\n",
            "Epoch 57/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0567 - accuracy: 0.9834\n",
            "Epoch 58/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0524 - accuracy: 0.9844\n",
            "Epoch 59/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0562 - accuracy: 0.9840\n",
            "Epoch 60/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0528 - accuracy: 0.9848\n",
            "Epoch 61/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0540 - accuracy: 0.9846\n",
            "Epoch 62/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0516 - accuracy: 0.9852\n",
            "Epoch 63/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0583 - accuracy: 0.9832\n",
            "Epoch 64/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0543 - accuracy: 0.9838\n",
            "Epoch 65/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0532 - accuracy: 0.9844\n",
            "Epoch 66/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0539 - accuracy: 0.9842\n",
            "Epoch 67/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0500 - accuracy: 0.9852\n",
            "Epoch 68/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0486 - accuracy: 0.9859\n",
            "Epoch 69/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0519 - accuracy: 0.9851\n",
            "Epoch 70/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0502 - accuracy: 0.9849\n",
            "Epoch 71/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0554 - accuracy: 0.9843\n",
            "Epoch 72/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0507 - accuracy: 0.9840\n",
            "Epoch 73/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0490 - accuracy: 0.9851\n",
            "Score for fold 4: loss of 0.08894988149404526; accuracy of 97.97801375389099%\n",
            "Epoch 1/200\n",
            "231/231 [==============================] - 20s 74ms/step - loss: 2.9337 - accuracy: 0.2994\n",
            "Epoch 2/200\n",
            "231/231 [==============================] - 16s 70ms/step - loss: 0.9841 - accuracy: 0.7434\n",
            "Epoch 3/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.5353 - accuracy: 0.8595\n",
            "Epoch 4/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.3928 - accuracy: 0.8974\n",
            "Epoch 5/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.3232 - accuracy: 0.9171\n",
            "Epoch 6/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.2793 - accuracy: 0.9266\n",
            "Epoch 7/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.2520 - accuracy: 0.9320\n",
            "Epoch 8/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.2252 - accuracy: 0.9389\n",
            "Epoch 9/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.2103 - accuracy: 0.9434\n",
            "Epoch 10/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.1892 - accuracy: 0.9484\n",
            "Epoch 11/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.1746 - accuracy: 0.9523\n",
            "Epoch 12/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.1697 - accuracy: 0.9544\n",
            "Epoch 13/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.1578 - accuracy: 0.9562\n",
            "Epoch 14/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.1580 - accuracy: 0.9566\n",
            "Epoch 15/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.1437 - accuracy: 0.9602\n",
            "Epoch 16/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.1437 - accuracy: 0.9594\n",
            "Epoch 17/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.1355 - accuracy: 0.9630\n",
            "Epoch 18/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.1325 - accuracy: 0.9627\n",
            "Epoch 19/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.1225 - accuracy: 0.9659\n",
            "Epoch 20/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.1196 - accuracy: 0.9653\n",
            "Epoch 21/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.1120 - accuracy: 0.9678\n",
            "Epoch 22/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.1121 - accuracy: 0.9686\n",
            "Epoch 23/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.1084 - accuracy: 0.9682\n",
            "Epoch 24/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.1017 - accuracy: 0.9708\n",
            "Epoch 25/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.1015 - accuracy: 0.9707\n",
            "Epoch 26/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.1029 - accuracy: 0.9701\n",
            "Epoch 27/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0971 - accuracy: 0.9717\n",
            "Epoch 28/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0945 - accuracy: 0.9724\n",
            "Epoch 29/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.1008 - accuracy: 0.9716\n",
            "Epoch 30/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0892 - accuracy: 0.9746\n",
            "Epoch 31/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0854 - accuracy: 0.9750\n",
            "Epoch 32/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0942 - accuracy: 0.9732\n",
            "Epoch 33/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0839 - accuracy: 0.9760\n",
            "Epoch 34/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0881 - accuracy: 0.9758\n",
            "Epoch 35/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0840 - accuracy: 0.9749\n",
            "Epoch 36/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0886 - accuracy: 0.9746\n",
            "Epoch 37/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0766 - accuracy: 0.9780\n",
            "Epoch 38/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0789 - accuracy: 0.9782\n",
            "Epoch 39/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0811 - accuracy: 0.9766\n",
            "Epoch 40/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0761 - accuracy: 0.9771\n",
            "Epoch 41/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0758 - accuracy: 0.9782\n",
            "Epoch 42/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0692 - accuracy: 0.9793\n",
            "Epoch 43/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0747 - accuracy: 0.9782\n",
            "Epoch 44/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0673 - accuracy: 0.9804\n",
            "Epoch 45/200\n",
            "231/231 [==============================] - 16s 68ms/step - loss: 0.0703 - accuracy: 0.9800\n",
            "Epoch 46/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0596 - accuracy: 0.9823\n",
            "Epoch 47/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0769 - accuracy: 0.9778\n",
            "Epoch 48/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0676 - accuracy: 0.9801\n",
            "Epoch 49/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0628 - accuracy: 0.9822\n",
            "Epoch 50/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0668 - accuracy: 0.9802\n",
            "Epoch 51/200\n",
            "231/231 [==============================] - 16s 69ms/step - loss: 0.0665 - accuracy: 0.9800\n",
            "Score for fold 5: loss of 0.08941493183374405; accuracy of 97.97773957252502%\n",
            "Model: \"model_12\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)           [(None, 128, 6)]     0           []                               \n",
            "                                                                                                  \n",
            " spatial_dropout1d_14 (SpatialD  (None, 128, 6)      0           ['input_7[0][0]']                \n",
            " ropout1D)                                                                                        \n",
            "                                                                                                  \n",
            " tnc1_initial_conv (Conv1D)     (None, 128, 128)     896         ['spatial_dropout1d_14[0][0]']   \n",
            "                                                                                                  \n",
            " tnc1_dilated_conv_1_tanh_s0 (C  (None, 128, 128)    49280       ['tnc1_initial_conv[0][0]']      \n",
            " onv1D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_144 (Activation)    (None, 128, 128)     0           ['tnc1_dilated_conv_1_tanh_s0[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " lambda_120 (Lambda)            (None, 128, 128)     0           ['activation_144[0][0]']         \n",
            "                                                                                                  \n",
            " tnc1_spatial_dropout1d_1_s0_0.  (None, 128, 128)    0           ['lambda_120[0][0]']             \n",
            " 000000 (SpatialDropout1D)                                                                        \n",
            "                                                                                                  \n",
            " conv1d_120 (Conv1D)            (None, 128, 128)     16512       ['tnc1_spatial_dropout1d_1_s0_0.0\n",
            "                                                                 00000[0][0]']                    \n",
            "                                                                                                  \n",
            " add_144 (Add)                  (None, 128, 128)     0           ['tnc1_initial_conv[0][0]',      \n",
            "                                                                  'conv1d_120[0][0]']             \n",
            "                                                                                                  \n",
            " tnc1_dilated_conv_2_tanh_s0 (C  (None, 128, 128)    49280       ['add_144[0][0]']                \n",
            " onv1D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_145 (Activation)    (None, 128, 128)     0           ['tnc1_dilated_conv_2_tanh_s0[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " lambda_121 (Lambda)            (None, 128, 128)     0           ['activation_145[0][0]']         \n",
            "                                                                                                  \n",
            " tnc1_spatial_dropout1d_2_s0_0.  (None, 128, 128)    0           ['lambda_121[0][0]']             \n",
            " 000000 (SpatialDropout1D)                                                                        \n",
            "                                                                                                  \n",
            " conv1d_121 (Conv1D)            (None, 128, 128)     16512       ['tnc1_spatial_dropout1d_2_s0_0.0\n",
            "                                                                 00000[0][0]']                    \n",
            "                                                                                                  \n",
            " add_145 (Add)                  (None, 128, 128)     0           ['add_144[0][0]',                \n",
            "                                                                  'conv1d_121[0][0]']             \n",
            "                                                                                                  \n",
            " tnc1_dilated_conv_4_tanh_s0 (C  (None, 128, 128)    49280       ['add_145[0][0]']                \n",
            " onv1D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_146 (Activation)    (None, 128, 128)     0           ['tnc1_dilated_conv_4_tanh_s0[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " lambda_122 (Lambda)            (None, 128, 128)     0           ['activation_146[0][0]']         \n",
            "                                                                                                  \n",
            " tnc1_spatial_dropout1d_4_s0_0.  (None, 128, 128)    0           ['lambda_122[0][0]']             \n",
            " 000000 (SpatialDropout1D)                                                                        \n",
            "                                                                                                  \n",
            " conv1d_122 (Conv1D)            (None, 128, 128)     16512       ['tnc1_spatial_dropout1d_4_s0_0.0\n",
            "                                                                 00000[0][0]']                    \n",
            "                                                                                                  \n",
            " add_146 (Add)                  (None, 128, 128)     0           ['add_145[0][0]',                \n",
            "                                                                  'conv1d_122[0][0]']             \n",
            "                                                                                                  \n",
            " tnc1_dilated_conv_8_tanh_s0 (C  (None, 128, 128)    49280       ['add_146[0][0]']                \n",
            " onv1D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_147 (Activation)    (None, 128, 128)     0           ['tnc1_dilated_conv_8_tanh_s0[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " lambda_123 (Lambda)            (None, 128, 128)     0           ['activation_147[0][0]']         \n",
            "                                                                                                  \n",
            " tnc1_spatial_dropout1d_8_s0_0.  (None, 128, 128)    0           ['lambda_123[0][0]']             \n",
            " 000000 (SpatialDropout1D)                                                                        \n",
            "                                                                                                  \n",
            " conv1d_123 (Conv1D)            (None, 128, 128)     16512       ['tnc1_spatial_dropout1d_8_s0_0.0\n",
            "                                                                 00000[0][0]']                    \n",
            "                                                                                                  \n",
            " add_147 (Add)                  (None, 128, 128)     0           ['add_146[0][0]',                \n",
            "                                                                  'conv1d_123[0][0]']             \n",
            "                                                                                                  \n",
            " tnc1_dilated_conv_16_tanh_s0 (  (None, 128, 128)    49280       ['add_147[0][0]']                \n",
            " Conv1D)                                                                                          \n",
            "                                                                                                  \n",
            " activation_148 (Activation)    (None, 128, 128)     0           ['tnc1_dilated_conv_16_tanh_s0[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " lambda_124 (Lambda)            (None, 128, 128)     0           ['activation_148[0][0]']         \n",
            "                                                                                                  \n",
            " tnc1_spatial_dropout1d_16_s0_0  (None, 128, 128)    0           ['lambda_124[0][0]']             \n",
            " .000000 (SpatialDropout1D)                                                                       \n",
            "                                                                                                  \n",
            " conv1d_124 (Conv1D)            (None, 128, 128)     16512       ['tnc1_spatial_dropout1d_16_s0_0.\n",
            "                                                                 000000[0][0]']                   \n",
            "                                                                                                  \n",
            " add_149 (Add)                  (None, 128, 128)     0           ['conv1d_120[0][0]',             \n",
            "                                                                  'conv1d_121[0][0]',             \n",
            "                                                                  'conv1d_122[0][0]',             \n",
            "                                                                  'conv1d_123[0][0]',             \n",
            "                                                                  'conv1d_124[0][0]']             \n",
            "                                                                                                  \n",
            " activation_149 (Activation)    (None, 128, 128)     0           ['add_149[0][0]']                \n",
            "                                                                                                  \n",
            " tnc2_initial_conv (Conv1D)     (None, 128, 128)     16512       ['activation_149[0][0]']         \n",
            "                                                                                                  \n",
            " tnc2_dilated_conv_1_tanh_s0 (C  (None, 128, 128)    49280       ['tnc2_initial_conv[0][0]']      \n",
            " onv1D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_150 (Activation)    (None, 128, 128)     0           ['tnc2_dilated_conv_1_tanh_s0[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " lambda_125 (Lambda)            (None, 128, 128)     0           ['activation_150[0][0]']         \n",
            "                                                                                                  \n",
            " tnc2_spatial_dropout1d_1_s0_0.  (None, 128, 128)    0           ['lambda_125[0][0]']             \n",
            " 000000 (SpatialDropout1D)                                                                        \n",
            "                                                                                                  \n",
            " conv1d_125 (Conv1D)            (None, 128, 128)     16512       ['tnc2_spatial_dropout1d_1_s0_0.0\n",
            "                                                                 00000[0][0]']                    \n",
            "                                                                                                  \n",
            " add_150 (Add)                  (None, 128, 128)     0           ['tnc2_initial_conv[0][0]',      \n",
            "                                                                  'conv1d_125[0][0]']             \n",
            "                                                                                                  \n",
            " tnc2_dilated_conv_2_tanh_s0 (C  (None, 128, 128)    49280       ['add_150[0][0]']                \n",
            " onv1D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_151 (Activation)    (None, 128, 128)     0           ['tnc2_dilated_conv_2_tanh_s0[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " lambda_126 (Lambda)            (None, 128, 128)     0           ['activation_151[0][0]']         \n",
            "                                                                                                  \n",
            " tnc2_spatial_dropout1d_2_s0_0.  (None, 128, 128)    0           ['lambda_126[0][0]']             \n",
            " 000000 (SpatialDropout1D)                                                                        \n",
            "                                                                                                  \n",
            " conv1d_126 (Conv1D)            (None, 128, 128)     16512       ['tnc2_spatial_dropout1d_2_s0_0.0\n",
            "                                                                 00000[0][0]']                    \n",
            "                                                                                                  \n",
            " add_151 (Add)                  (None, 128, 128)     0           ['add_150[0][0]',                \n",
            "                                                                  'conv1d_126[0][0]']             \n",
            "                                                                                                  \n",
            " tnc2_dilated_conv_4_tanh_s0 (C  (None, 128, 128)    49280       ['add_151[0][0]']                \n",
            " onv1D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_152 (Activation)    (None, 128, 128)     0           ['tnc2_dilated_conv_4_tanh_s0[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " lambda_127 (Lambda)            (None, 128, 128)     0           ['activation_152[0][0]']         \n",
            "                                                                                                  \n",
            " tnc2_spatial_dropout1d_4_s0_0.  (None, 128, 128)    0           ['lambda_127[0][0]']             \n",
            " 000000 (SpatialDropout1D)                                                                        \n",
            "                                                                                                  \n",
            " conv1d_127 (Conv1D)            (None, 128, 128)     16512       ['tnc2_spatial_dropout1d_4_s0_0.0\n",
            "                                                                 00000[0][0]']                    \n",
            "                                                                                                  \n",
            " add_152 (Add)                  (None, 128, 128)     0           ['add_151[0][0]',                \n",
            "                                                                  'conv1d_127[0][0]']             \n",
            "                                                                                                  \n",
            " tnc2_dilated_conv_8_tanh_s0 (C  (None, 128, 128)    49280       ['add_152[0][0]']                \n",
            " onv1D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_153 (Activation)    (None, 128, 128)     0           ['tnc2_dilated_conv_8_tanh_s0[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " lambda_128 (Lambda)            (None, 128, 128)     0           ['activation_153[0][0]']         \n",
            "                                                                                                  \n",
            " tnc2_spatial_dropout1d_8_s0_0.  (None, 128, 128)    0           ['lambda_128[0][0]']             \n",
            " 000000 (SpatialDropout1D)                                                                        \n",
            "                                                                                                  \n",
            " conv1d_128 (Conv1D)            (None, 128, 128)     16512       ['tnc2_spatial_dropout1d_8_s0_0.0\n",
            "                                                                 00000[0][0]']                    \n",
            "                                                                                                  \n",
            " add_153 (Add)                  (None, 128, 128)     0           ['add_152[0][0]',                \n",
            "                                                                  'conv1d_128[0][0]']             \n",
            "                                                                                                  \n",
            " tnc2_dilated_conv_16_tanh_s0 (  (None, 128, 128)    49280       ['add_153[0][0]']                \n",
            " Conv1D)                                                                                          \n",
            "                                                                                                  \n",
            " activation_154 (Activation)    (None, 128, 128)     0           ['tnc2_dilated_conv_16_tanh_s0[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " lambda_129 (Lambda)            (None, 128, 128)     0           ['activation_154[0][0]']         \n",
            "                                                                                                  \n",
            " tnc2_spatial_dropout1d_16_s0_0  (None, 128, 128)    0           ['lambda_129[0][0]']             \n",
            " .000000 (SpatialDropout1D)                                                                       \n",
            "                                                                                                  \n",
            " conv1d_129 (Conv1D)            (None, 128, 128)     16512       ['tnc2_spatial_dropout1d_16_s0_0.\n",
            "                                                                 000000[0][0]']                   \n",
            "                                                                                                  \n",
            " add_155 (Add)                  (None, 128, 128)     0           ['conv1d_125[0][0]',             \n",
            "                                                                  'conv1d_126[0][0]',             \n",
            "                                                                  'conv1d_127[0][0]',             \n",
            "                                                                  'conv1d_128[0][0]',             \n",
            "                                                                  'conv1d_129[0][0]']             \n",
            "                                                                                                  \n",
            " activation_155 (Activation)    (None, 128, 128)     0           ['add_155[0][0]']                \n",
            "                                                                                                  \n",
            " global_max_pooling1d_12 (Globa  (None, 128)         0           ['activation_155[0][0]']         \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " dense_24 (Dense)               (None, 128)          16512       ['global_max_pooling1d_12[0][0]']\n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)           (None, 128)          0           ['dense_24[0][0]']               \n",
            "                                                                                                  \n",
            " dense_25 (Dense)               (None, 118)          15222       ['dropout_12[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 707,062\n",
            "Trainable params: 707,062\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Define per-fold score containers \n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "num_folds = 5\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "fold_no = 1\n",
        "input_shape = x_train.shape[1:]\n",
        "input = Input(shape=x_train.shape[1:])\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "  x = SpatialDropout1D(0.2)(input)\n",
        "  x = TCN(128,dilations = [1, 2, 4, 8, 16],kernel_size = 3, return_sequences=True, name = 'tnc1')(x)\n",
        "  x = TCN(128,dilations = [1, 2, 4, 8, 16],kernel_size = 3, return_sequences=True, name = 'tnc2')(x)\n",
        "  max_pool = GlobalMaxPooling1D()(x)\n",
        "  x = Dense(128, activation=\"relu\")(max_pool)\n",
        "  x = Dropout(0.2)(x)\n",
        "  output = Dense(118, activation=\"softmax\")(x)    \n",
        "  model = Model(inputs=input, outputs=output)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  history = model.fit(inputs[train], targets[train], epochs=200, verbose=True, batch_size=128,callbacks=[keras.callbacks.EarlyStopping(monitor='loss', patience=5,restore_best_weights=True)])\n",
        "    # Generate generalization metrics\n",
        "  scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaWCy3YG9Vlu",
        "outputId": "c44e295a-6d5a-447b-9e13-18cfedf504e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.10499321669340134 - Accuracy: 97.5709080696106%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.09094121307134628 - Accuracy: 97.81517386436462%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.09887058287858963 - Accuracy: 97.67946600914001%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.08894988149404526 - Accuracy: 97.97801375389099%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.08941493183374405 - Accuracy: 97.97773957252502%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 97.80426025390625 (+- 0.16151241752939427)\n",
            "> Loss: 0.09463396519422532\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Average scores \n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TCN_dataset_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}