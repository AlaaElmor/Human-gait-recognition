{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "2jIRhLtI55E6"
      },
      "outputs": [],
      "source": [
        "from numpy import save, load\n",
        "from pandas import read_csv\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/'  #change dir to your project folder\n",
        "\n",
        "import numpy as np\n",
        "x_train = np.load('gdrive/My Drive/dataset/dataset5/trainX.npy')\n",
        "y_train = np.load('gdrive/My Drive/dataset/dataset5/trainy.npy')\n",
        "x_test = np.load('gdrive/My Drive/dataset/dataset5/testX.npy')\n",
        "y_test = np.load('gdrive/My Drive/dataset/dataset5/testy.npy')\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
        "'''\n",
        "x_train = np.load('gdrive/My Drive/dataset1/acc+gyr/trainX.npy')\n",
        "y_train = np.load('gdrive/My Drive/dataset1/acc+gyr/trainy.npy')\n",
        "x_test = np.load('gdrive/My Drive/dataset1/acc+gyr/testX.npy')\n",
        "y_test = np.load('gdrive/My Drive/dataset1/acc+gyr/testy.npy')\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "oZao1z7K6BSL",
        "outputId": "831c847e-b250-4a44-e044-9a7604c016b0"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Mounted at /content/gdrive\n",
            "(66542, 256, 6) (66542, 2) (7600, 256, 6) (7600, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nx_train = np.load('gdrive/My Drive/dataset1/acc+gyr/trainX.npy')\\ny_train = np.load('gdrive/My Drive/dataset1/acc+gyr/trainy.npy')\\nx_test = np.load('gdrive/My Drive/dataset1/acc+gyr/testX.npy')\\ny_test = np.load('gdrive/My Drive/dataset1/acc+gyr/testy.npy')\\nprint(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.random.permutation(len(x_train))\n",
        "x_train = x_train[idx]\n",
        "y_train = y_train[idx]"
      ],
      "metadata": {
        "id": "GMlK0Tf36BVc"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Attention and Normalization\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(inputs, inputs)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    return x + res"
      ],
      "metadata": {
        "id": "yaF5YS6-6BY1"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def transformer_decoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    #x = layers.Embedding(input_dim=inputs.shape[:] , output_dim=inputs.shape[:])\n",
        "    x = layers.Embedding(1000, 64, input_length=10)(inputs, inputs)\n",
        "\n",
        "    # Attention and Normalization\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1,activation=\"relu\")(x)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    return x + res\n",
        "'''"
      ],
      "metadata": {
        "id": "KxNKX-fkCO93"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(\n",
        "    input_shape,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    ff_dim,\n",
        "    num_transformer_blocks,\n",
        "    mlp_units,\n",
        "    dropout=0,\n",
        "    mlp_dropout=0,\n",
        "):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "    return keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "KOFWDE8C6BcK"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "model = build_model(\n",
        "    input_shape,\n",
        "    head_size=256,\n",
        "    num_heads=4,\n",
        "    ff_dim=4,\n",
        "    num_transformer_blocks=4,\n",
        "    mlp_units=[128],\n",
        "    mlp_dropout=0.4,\n",
        "    dropout=0.25,\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "model.summary()\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQQWrG_f6Bhz",
        "outputId": "c12e29f8-2182-4d31-e8b4-e17a285e8247"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_21 (InputLayer)          [(None, 256, 6)]     0           []                               \n",
            "                                                                                                  \n",
            " multi_head_attention_33 (Multi  (None, 256, 6)      27654       ['input_21[0][0]',               \n",
            " HeadAttention)                                                   'input_21[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_70 (Dropout)           (None, 256, 6)       0           ['multi_head_attention_33[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_64 (LayerN  (None, 256, 6)      12          ['dropout_70[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.__operators__.add_64 (TFOpL  (None, 256, 6)      0           ['layer_normalization_64[0][0]', \n",
            " ambda)                                                           'input_21[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_64 (Conv1D)             (None, 256, 4)       28          ['tf.__operators__.add_64[0][0]']\n",
            "                                                                                                  \n",
            " dropout_71 (Dropout)           (None, 256, 4)       0           ['conv1d_64[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_65 (Conv1D)             (None, 256, 6)       30          ['dropout_71[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_65 (LayerN  (None, 256, 6)      12          ['conv1d_65[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.__operators__.add_65 (TFOpL  (None, 256, 6)      0           ['layer_normalization_65[0][0]', \n",
            " ambda)                                                           'tf.__operators__.add_64[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_34 (Multi  (None, 256, 6)      27654       ['tf.__operators__.add_65[0][0]',\n",
            " HeadAttention)                                                   'tf.__operators__.add_65[0][0]']\n",
            "                                                                                                  \n",
            " dropout_72 (Dropout)           (None, 256, 6)       0           ['multi_head_attention_34[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_66 (LayerN  (None, 256, 6)      12          ['dropout_72[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.__operators__.add_66 (TFOpL  (None, 256, 6)      0           ['layer_normalization_66[0][0]', \n",
            " ambda)                                                           'tf.__operators__.add_65[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_66 (Conv1D)             (None, 256, 4)       28          ['tf.__operators__.add_66[0][0]']\n",
            "                                                                                                  \n",
            " dropout_73 (Dropout)           (None, 256, 4)       0           ['conv1d_66[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_67 (Conv1D)             (None, 256, 6)       30          ['dropout_73[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_67 (LayerN  (None, 256, 6)      12          ['conv1d_67[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.__operators__.add_67 (TFOpL  (None, 256, 6)      0           ['layer_normalization_67[0][0]', \n",
            " ambda)                                                           'tf.__operators__.add_66[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_35 (Multi  (None, 256, 6)      27654       ['tf.__operators__.add_67[0][0]',\n",
            " HeadAttention)                                                   'tf.__operators__.add_67[0][0]']\n",
            "                                                                                                  \n",
            " dropout_74 (Dropout)           (None, 256, 6)       0           ['multi_head_attention_35[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_68 (LayerN  (None, 256, 6)      12          ['dropout_74[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.__operators__.add_68 (TFOpL  (None, 256, 6)      0           ['layer_normalization_68[0][0]', \n",
            " ambda)                                                           'tf.__operators__.add_67[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_68 (Conv1D)             (None, 256, 4)       28          ['tf.__operators__.add_68[0][0]']\n",
            "                                                                                                  \n",
            " dropout_75 (Dropout)           (None, 256, 4)       0           ['conv1d_68[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_69 (Conv1D)             (None, 256, 6)       30          ['dropout_75[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_69 (LayerN  (None, 256, 6)      12          ['conv1d_69[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.__operators__.add_69 (TFOpL  (None, 256, 6)      0           ['layer_normalization_69[0][0]', \n",
            " ambda)                                                           'tf.__operators__.add_68[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_36 (Multi  (None, 256, 6)      27654       ['tf.__operators__.add_69[0][0]',\n",
            " HeadAttention)                                                   'tf.__operators__.add_69[0][0]']\n",
            "                                                                                                  \n",
            " dropout_76 (Dropout)           (None, 256, 6)       0           ['multi_head_attention_36[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_70 (LayerN  (None, 256, 6)      12          ['dropout_76[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.__operators__.add_70 (TFOpL  (None, 256, 6)      0           ['layer_normalization_70[0][0]', \n",
            " ambda)                                                           'tf.__operators__.add_69[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_70 (Conv1D)             (None, 256, 4)       28          ['tf.__operators__.add_70[0][0]']\n",
            "                                                                                                  \n",
            " dropout_77 (Dropout)           (None, 256, 4)       0           ['conv1d_70[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_71 (Conv1D)             (None, 256, 6)       30          ['dropout_77[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_71 (LayerN  (None, 256, 6)      12          ['conv1d_71[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.__operators__.add_71 (TFOpL  (None, 256, 6)      0           ['layer_normalization_71[0][0]', \n",
            " ambda)                                                           'tf.__operators__.add_70[0][0]']\n",
            "                                                                                                  \n",
            " global_average_pooling1d_7 (Gl  (None, 256)         0           ['tf.__operators__.add_71[0][0]']\n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 128)          32896       ['global_average_pooling1d_7[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_78 (Dropout)           (None, 128)          0           ['dense_12[0][0]']               \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 2)            258         ['dropout_78[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 144,098\n",
            "Trainable params: 144,098\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    verbose=True,\n",
        "    validation_split=0.2,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    callbacks=callbacks,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBDRw2UA6BlK",
        "outputId": "b92f2c6e-edf1-4cb6-c293-39327124d82b"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "416/416 [==============================] - 487s 1s/step - loss: 0.5779 - accuracy: 0.6655 - val_loss: 0.5715 - val_accuracy: 0.6947\n",
            "Epoch 2/10\n",
            "416/416 [==============================] - 481s 1s/step - loss: 0.5101 - accuracy: 0.7197 - val_loss: 0.5316 - val_accuracy: 0.7299\n",
            "Epoch 3/10\n",
            "416/416 [==============================] - 481s 1s/step - loss: 0.4904 - accuracy: 0.7323 - val_loss: 0.5508 - val_accuracy: 0.7211\n",
            "Epoch 4/10\n",
            "416/416 [==============================] - 482s 1s/step - loss: 0.4844 - accuracy: 0.7342 - val_loss: 0.5876 - val_accuracy: 0.7234\n",
            "Epoch 5/10\n",
            "416/416 [==============================] - 482s 1s/step - loss: 0.4820 - accuracy: 0.7353 - val_loss: 0.5667 - val_accuracy: 0.7291\n",
            "Epoch 6/10\n",
            "416/416 [==============================] - 481s 1s/step - loss: 0.4764 - accuracy: 0.7373 - val_loss: 0.5235 - val_accuracy: 0.7173\n",
            "Epoch 7/10\n",
            "416/416 [==============================] - 482s 1s/step - loss: 0.4718 - accuracy: 0.7435 - val_loss: 0.5728 - val_accuracy: 0.7098\n",
            "Epoch 8/10\n",
            "416/416 [==============================] - 482s 1s/step - loss: 0.4685 - accuracy: 0.7429 - val_loss: 0.5603 - val_accuracy: 0.7367\n",
            "Epoch 9/10\n",
            "416/416 [==============================] - 482s 1s/step - loss: 0.4676 - accuracy: 0.7469 - val_loss: 0.5354 - val_accuracy: 0.7536\n",
            "Epoch 10/10\n",
            "416/416 [==============================] - 482s 1s/step - loss: 0.4684 - accuracy: 0.7461 - val_loss: 0.5204 - val_accuracy: 0.7502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy: {:.4f}\".format(accuracy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCFnbNpO6Woo",
        "outputId": "417a2621-0172-4271-bfe6-9cd6d02a43f7"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Accuracy: 0.7138\n"
          ]
        }
      ]
    }
  ]
}