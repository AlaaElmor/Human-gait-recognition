{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "2jIRhLtI55E6"
      },
      "outputs": [],
      "source": [
        "from numpy import save, load\n",
        "from pandas import read_csv\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/'  #change dir to your project folder\n",
        "\n",
        "import numpy as np\n",
        "x_train = np.load('gdrive/My Drive/dataset1/acc+gyr/trainX.npy')\n",
        "y_train = np.load('gdrive/My Drive/dataset1/acc+gyr/trainy.npy')\n",
        "x_test = np.load('gdrive/My Drive/dataset1/acc+gyr/testX.npy')\n",
        "y_test = np.load('gdrive/My Drive/dataset1/acc+gyr/testy.npy')\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZao1z7K6BSL",
        "outputId": "fb693c3a-7fe8-434c-d900-7c3abaf7d495"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Mounted at /content/gdrive\n",
            "(33104, 128, 6) (33104, 118) (3740, 128, 6) (3740, 118)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.transpose(0,2,1)\n",
        "x_test = x_test.transpose(0,2,1)\n",
        "\n",
        "idx = np.random.permutation(len(x_train))\n",
        "x_train = x_train[idx]\n",
        "y_train = y_train[idx]\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=0.20, random_state=42)\n"
      ],
      "metadata": {
        "id": "GMlK0Tf36BVc"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PjnE9_G-dUl",
        "outputId": "b2c99e08-d61f-44e8-9312-6f567dc713fa"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26483, 6, 128)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = x_train.shape[-1]   # Embedding size for each token\n",
        "num_heads = 4  # Number of attention heads\n",
        "ff_dim = x_train.shape[-1]  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads,\n",
        "                                             key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)  # self-attention layer\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)  # layer norm\n",
        "        ffn_output = self.ffn(out1)  #feed-forward layer\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)  # layer norm\n",
        "model = keras.Sequential()\n",
        "model.add(layers.Input(shape= x_train.shape[1:]))\n",
        "model.add(TransformerBlock(embed_dim, num_heads, ff_dim))\n",
        "model.add(layers.GlobalAveragePooling1D())\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(ff_dim, activation='relu'))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(118, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "model.summary()\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQQWrG_f6Bhz",
        "outputId": "df929574-228d-46e4-cab9-ca67706b6fce"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " transformer_block_6 (Transf  (None, 6, 128)           297344    \n",
            " ormerBlock)                                                     \n",
            "                                                                 \n",
            " global_average_pooling1d_6   (None, 128)              0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dropout_26 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_27 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 118)               15222     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 329,078\n",
            "Trainable params: 329,078\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, epochs=200, verbose=True, validation_data=(x_validation, y_validation), batch_size=128,callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBDRw2UA6BlK",
        "outputId": "6232a102-69f0-42a2-8388-db9d6bdc76ce"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "207/207 [==============================] - 5s 17ms/step - loss: 4.5446 - accuracy: 0.0318 - val_loss: 4.1873 - val_accuracy: 0.0825\n",
            "Epoch 2/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 4.0221 - accuracy: 0.1054 - val_loss: 3.6051 - val_accuracy: 0.2148\n",
            "Epoch 3/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 3.4314 - accuracy: 0.2246 - val_loss: 2.8923 - val_accuracy: 0.3950\n",
            "Epoch 4/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 2.8175 - accuracy: 0.3630 - val_loss: 2.2828 - val_accuracy: 0.5327\n",
            "Epoch 5/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 2.3188 - accuracy: 0.4730 - val_loss: 1.8041 - val_accuracy: 0.6439\n",
            "Epoch 6/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 1.9511 - accuracy: 0.5527 - val_loss: 1.4838 - val_accuracy: 0.7034\n",
            "Epoch 7/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 1.6868 - accuracy: 0.6144 - val_loss: 1.2758 - val_accuracy: 0.7414\n",
            "Epoch 8/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 1.4905 - accuracy: 0.6563 - val_loss: 1.1226 - val_accuracy: 0.7659\n",
            "Epoch 9/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 1.3544 - accuracy: 0.6844 - val_loss: 1.0207 - val_accuracy: 0.7762\n",
            "Epoch 10/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 1.2395 - accuracy: 0.7083 - val_loss: 0.9398 - val_accuracy: 0.7929\n",
            "Epoch 11/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 1.1569 - accuracy: 0.7282 - val_loss: 0.8832 - val_accuracy: 0.8006\n",
            "Epoch 12/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 1.0841 - accuracy: 0.7427 - val_loss: 0.8352 - val_accuracy: 0.8073\n",
            "Epoch 13/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 1.0269 - accuracy: 0.7524 - val_loss: 0.7969 - val_accuracy: 0.8138\n",
            "Epoch 14/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.9766 - accuracy: 0.7651 - val_loss: 0.7626 - val_accuracy: 0.8239\n",
            "Epoch 15/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.9283 - accuracy: 0.7777 - val_loss: 0.7302 - val_accuracy: 0.8284\n",
            "Epoch 16/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.8913 - accuracy: 0.7850 - val_loss: 0.7010 - val_accuracy: 0.8308\n",
            "Epoch 17/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.8588 - accuracy: 0.7906 - val_loss: 0.6900 - val_accuracy: 0.8348\n",
            "Epoch 18/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.8256 - accuracy: 0.7986 - val_loss: 0.6715 - val_accuracy: 0.8351\n",
            "Epoch 19/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.7963 - accuracy: 0.8044 - val_loss: 0.6602 - val_accuracy: 0.8369\n",
            "Epoch 20/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.7663 - accuracy: 0.8118 - val_loss: 0.6359 - val_accuracy: 0.8408\n",
            "Epoch 21/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.7489 - accuracy: 0.8143 - val_loss: 0.6150 - val_accuracy: 0.8473\n",
            "Epoch 22/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.7283 - accuracy: 0.8191 - val_loss: 0.6196 - val_accuracy: 0.8422\n",
            "Epoch 23/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.7004 - accuracy: 0.8273 - val_loss: 0.5970 - val_accuracy: 0.8538\n",
            "Epoch 24/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.6819 - accuracy: 0.8300 - val_loss: 0.5805 - val_accuracy: 0.8556\n",
            "Epoch 25/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.6696 - accuracy: 0.8336 - val_loss: 0.5647 - val_accuracy: 0.8564\n",
            "Epoch 26/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.6483 - accuracy: 0.8367 - val_loss: 0.5548 - val_accuracy: 0.8624\n",
            "Epoch 27/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.6345 - accuracy: 0.8378 - val_loss: 0.5510 - val_accuracy: 0.8604\n",
            "Epoch 28/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.6152 - accuracy: 0.8433 - val_loss: 0.5325 - val_accuracy: 0.8671\n",
            "Epoch 29/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.5978 - accuracy: 0.8479 - val_loss: 0.5289 - val_accuracy: 0.8636\n",
            "Epoch 30/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.5912 - accuracy: 0.8500 - val_loss: 0.5150 - val_accuracy: 0.8740\n",
            "Epoch 31/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.5773 - accuracy: 0.8541 - val_loss: 0.5136 - val_accuracy: 0.8692\n",
            "Epoch 32/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.5619 - accuracy: 0.8562 - val_loss: 0.5086 - val_accuracy: 0.8715\n",
            "Epoch 33/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.5537 - accuracy: 0.8582 - val_loss: 0.5202 - val_accuracy: 0.8700\n",
            "Epoch 34/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.5353 - accuracy: 0.8610 - val_loss: 0.4942 - val_accuracy: 0.8758\n",
            "Epoch 35/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.5346 - accuracy: 0.8587 - val_loss: 0.4772 - val_accuracy: 0.8801\n",
            "Epoch 36/200\n",
            "207/207 [==============================] - 3s 14ms/step - loss: 0.5240 - accuracy: 0.8649 - val_loss: 0.4817 - val_accuracy: 0.8765\n",
            "Epoch 37/200\n",
            "207/207 [==============================] - 3s 14ms/step - loss: 0.5077 - accuracy: 0.8677 - val_loss: 0.4817 - val_accuracy: 0.8799\n",
            "Epoch 38/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.5010 - accuracy: 0.8695 - val_loss: 0.4632 - val_accuracy: 0.8846\n",
            "Epoch 39/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.4975 - accuracy: 0.8696 - val_loss: 0.4821 - val_accuracy: 0.8789\n",
            "Epoch 40/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.4821 - accuracy: 0.8722 - val_loss: 0.4703 - val_accuracy: 0.8784\n",
            "Epoch 41/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.4776 - accuracy: 0.8735 - val_loss: 0.4822 - val_accuracy: 0.8725\n",
            "Epoch 42/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.4683 - accuracy: 0.8786 - val_loss: 0.4501 - val_accuracy: 0.8881\n",
            "Epoch 43/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.4535 - accuracy: 0.8792 - val_loss: 0.4595 - val_accuracy: 0.8855\n",
            "Epoch 44/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.4491 - accuracy: 0.8813 - val_loss: 0.4407 - val_accuracy: 0.8894\n",
            "Epoch 45/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.4445 - accuracy: 0.8817 - val_loss: 0.4466 - val_accuracy: 0.8849\n",
            "Epoch 46/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.4341 - accuracy: 0.8863 - val_loss: 0.4365 - val_accuracy: 0.8894\n",
            "Epoch 47/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.4319 - accuracy: 0.8851 - val_loss: 0.4683 - val_accuracy: 0.8816\n",
            "Epoch 48/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.4224 - accuracy: 0.8847 - val_loss: 0.4463 - val_accuracy: 0.8870\n",
            "Epoch 49/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.4165 - accuracy: 0.8885 - val_loss: 0.4413 - val_accuracy: 0.8884\n",
            "Epoch 50/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.4098 - accuracy: 0.8900 - val_loss: 0.4314 - val_accuracy: 0.8897\n",
            "Epoch 51/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.3993 - accuracy: 0.8916 - val_loss: 0.4252 - val_accuracy: 0.8922\n",
            "Epoch 52/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.3968 - accuracy: 0.8931 - val_loss: 0.4293 - val_accuracy: 0.8907\n",
            "Epoch 53/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.3886 - accuracy: 0.8971 - val_loss: 0.4171 - val_accuracy: 0.8953\n",
            "Epoch 54/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.3842 - accuracy: 0.8950 - val_loss: 0.4084 - val_accuracy: 0.8961\n",
            "Epoch 55/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.3781 - accuracy: 0.8961 - val_loss: 0.4148 - val_accuracy: 0.8953\n",
            "Epoch 56/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.3763 - accuracy: 0.8956 - val_loss: 0.4099 - val_accuracy: 0.8970\n",
            "Epoch 57/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.3654 - accuracy: 0.9002 - val_loss: 0.4032 - val_accuracy: 0.8965\n",
            "Epoch 58/200\n",
            "207/207 [==============================] - 3s 14ms/step - loss: 0.3687 - accuracy: 0.8980 - val_loss: 0.4069 - val_accuracy: 0.8981\n",
            "Epoch 59/200\n",
            "207/207 [==============================] - 3s 14ms/step - loss: 0.3615 - accuracy: 0.8994 - val_loss: 0.3985 - val_accuracy: 0.9005\n",
            "Epoch 60/200\n",
            "207/207 [==============================] - 3s 14ms/step - loss: 0.3483 - accuracy: 0.9036 - val_loss: 0.4088 - val_accuracy: 0.8962\n",
            "Epoch 61/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.3535 - accuracy: 0.9013 - val_loss: 0.4010 - val_accuracy: 0.9006\n",
            "Epoch 62/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.3429 - accuracy: 0.9057 - val_loss: 0.3965 - val_accuracy: 0.9030\n",
            "Epoch 63/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.3350 - accuracy: 0.9069 - val_loss: 0.3998 - val_accuracy: 0.9012\n",
            "Epoch 64/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.3407 - accuracy: 0.9046 - val_loss: 0.3995 - val_accuracy: 0.9012\n",
            "Epoch 65/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.3342 - accuracy: 0.9071 - val_loss: 0.3991 - val_accuracy: 0.9035\n",
            "Epoch 66/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.3260 - accuracy: 0.9100 - val_loss: 0.3889 - val_accuracy: 0.9015\n",
            "Epoch 67/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.3278 - accuracy: 0.9075 - val_loss: 0.3879 - val_accuracy: 0.9032\n",
            "Epoch 68/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.3163 - accuracy: 0.9125 - val_loss: 0.3903 - val_accuracy: 0.9020\n",
            "Epoch 69/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.3109 - accuracy: 0.9139 - val_loss: 0.3858 - val_accuracy: 0.9052\n",
            "Epoch 70/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.3093 - accuracy: 0.9135 - val_loss: 0.3975 - val_accuracy: 0.9012\n",
            "Epoch 71/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.3071 - accuracy: 0.9151 - val_loss: 0.3924 - val_accuracy: 0.8990\n",
            "Epoch 72/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.3028 - accuracy: 0.9140 - val_loss: 0.3802 - val_accuracy: 0.9059\n",
            "Epoch 73/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.2984 - accuracy: 0.9162 - val_loss: 0.3767 - val_accuracy: 0.9079\n",
            "Epoch 74/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.2961 - accuracy: 0.9166 - val_loss: 0.3801 - val_accuracy: 0.9056\n",
            "Epoch 75/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.2920 - accuracy: 0.9194 - val_loss: 0.3909 - val_accuracy: 0.9044\n",
            "Epoch 76/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.2925 - accuracy: 0.9173 - val_loss: 0.3855 - val_accuracy: 0.9059\n",
            "Epoch 77/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.2808 - accuracy: 0.9200 - val_loss: 0.3864 - val_accuracy: 0.9042\n",
            "Epoch 78/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.2792 - accuracy: 0.9215 - val_loss: 0.3815 - val_accuracy: 0.9042\n",
            "Epoch 79/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.2760 - accuracy: 0.9209 - val_loss: 0.3807 - val_accuracy: 0.9059\n",
            "Epoch 80/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.2783 - accuracy: 0.9199 - val_loss: 0.3772 - val_accuracy: 0.9068\n",
            "Epoch 81/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.2740 - accuracy: 0.9217 - val_loss: 0.3809 - val_accuracy: 0.9048\n",
            "Epoch 82/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.2669 - accuracy: 0.9230 - val_loss: 0.3843 - val_accuracy: 0.9053\n",
            "Epoch 83/200\n",
            "207/207 [==============================] - 3s 15ms/step - loss: 0.2610 - accuracy: 0.9260 - val_loss: 0.3776 - val_accuracy: 0.9089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(x_validation, y_validation, verbose=False)\n",
        "print(\"Validation Accuracy:  {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy: {:.4f}\".format(accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCFnbNpO6Woo",
        "outputId": "9230235a-5bf7-4644-b700-3a52ca7c4593"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy:  0.9079\n",
            "Testing Accuracy: 0.8257\n"
          ]
        }
      ]
    }
  ]
}