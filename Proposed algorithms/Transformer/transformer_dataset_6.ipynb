{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "2jIRhLtI55E6"
      },
      "outputs": [],
      "source": [
        "from numpy import save, load\n",
        "from pandas import read_csv\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/'  #change dir to your project folder\n",
        "\n",
        "import numpy as np\n",
        "x_train = np.load('gdrive/My Drive/dataset/dataset6/trainX.npy')\n",
        "y_train = np.load('gdrive/My Drive/dataset/dataset6/trainy.npy')\n",
        "x_test = np.load('gdrive/My Drive/dataset/dataset6/testX.npy')\n",
        "y_test = np.load('gdrive/My Drive/dataset/dataset6/testy.npy')\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZao1z7K6BSL",
        "outputId": "56364a52-5d4e-4ae1-966c-614886c73fbd"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Mounted at /content/gdrive\n",
            "(66542, 128, 12) (66542, 2) (7600, 128, 12) (7600, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.transpose(0,2,1)\n",
        "x_test = x_test.transpose(0,2,1)\n",
        "\n",
        "idx = np.random.permutation(len(x_train))\n",
        "x_train = x_train[idx]\n",
        "y_train = y_train[idx]\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=0.20, random_state=42)\n"
      ],
      "metadata": {
        "id": "GMlK0Tf36BVc"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PjnE9_G-dUl",
        "outputId": "5ecaf77e-b8e6-4ed0-e5ae-6277a23a544e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(53233, 12, 128)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = x_train.shape[-1]   # Embedding size for each token\n",
        "num_heads = 4  # Number of attention heads\n",
        "ff_dim = x_train.shape[-1]  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads,\n",
        "                                             key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)  # self-attention layer\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)  # layer norm\n",
        "        ffn_output = self.ffn(out1)  #feed-forward layer\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)  # layer norm\n",
        "model = keras.Sequential()\n",
        "model.add(layers.Input(shape= x_train.shape[1:]))\n",
        "model.add(TransformerBlock(embed_dim, num_heads, ff_dim))\n",
        "model.add(layers.GlobalAveragePooling1D())\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(ff_dim, activation='relu'))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "model.summary()\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQQWrG_f6Bhz",
        "outputId": "caa9ca40-9c2e-4097-b86f-5713b148bea3"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " transformer_block_9 (Transf  (None, 12, 128)          297344    \n",
            " ormerBlock)                                                     \n",
            "                                                                 \n",
            " global_average_pooling1d_9   (None, 128)              0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dropout_38 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_39 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 314,114\n",
            "Trainable params: 314,114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, epochs=200, verbose=True, validation_data=(x_validation, y_validation), batch_size=128,callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBDRw2UA6BlK",
        "outputId": "f20ebb95-aeae-478e-eb9f-c69ff86767bf"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "416/416 [==============================] - 9s 17ms/step - loss: 0.6007 - accuracy: 0.6632 - val_loss: 0.5233 - val_accuracy: 0.7381\n",
            "Epoch 2/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.5271 - accuracy: 0.7299 - val_loss: 0.5087 - val_accuracy: 0.7497\n",
            "Epoch 3/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.4930 - accuracy: 0.7567 - val_loss: 0.4609 - val_accuracy: 0.7792\n",
            "Epoch 4/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.4631 - accuracy: 0.7786 - val_loss: 0.4223 - val_accuracy: 0.8020\n",
            "Epoch 5/200\n",
            "416/416 [==============================] - 6s 16ms/step - loss: 0.4303 - accuracy: 0.7994 - val_loss: 0.3959 - val_accuracy: 0.8185\n",
            "Epoch 6/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.4115 - accuracy: 0.8111 - val_loss: 0.3919 - val_accuracy: 0.8234\n",
            "Epoch 7/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.3958 - accuracy: 0.8211 - val_loss: 0.3802 - val_accuracy: 0.8307\n",
            "Epoch 8/200\n",
            "416/416 [==============================] - 6s 16ms/step - loss: 0.3804 - accuracy: 0.8317 - val_loss: 0.3660 - val_accuracy: 0.8366\n",
            "Epoch 9/200\n",
            "416/416 [==============================] - 6s 15ms/step - loss: 0.3708 - accuracy: 0.8357 - val_loss: 0.3469 - val_accuracy: 0.8468\n",
            "Epoch 10/200\n",
            "416/416 [==============================] - 6s 16ms/step - loss: 0.3619 - accuracy: 0.8401 - val_loss: 0.3501 - val_accuracy: 0.8464\n",
            "Epoch 11/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.3532 - accuracy: 0.8435 - val_loss: 0.3769 - val_accuracy: 0.8327\n",
            "Epoch 12/200\n",
            "416/416 [==============================] - 6s 16ms/step - loss: 0.3451 - accuracy: 0.8490 - val_loss: 0.3396 - val_accuracy: 0.8512\n",
            "Epoch 13/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.3387 - accuracy: 0.8521 - val_loss: 0.3315 - val_accuracy: 0.8572\n",
            "Epoch 14/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.3328 - accuracy: 0.8548 - val_loss: 0.3201 - val_accuracy: 0.8623\n",
            "Epoch 15/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.3259 - accuracy: 0.8577 - val_loss: 0.3211 - val_accuracy: 0.8623\n",
            "Epoch 16/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.3210 - accuracy: 0.8598 - val_loss: 0.3383 - val_accuracy: 0.8494\n",
            "Epoch 17/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.3150 - accuracy: 0.8643 - val_loss: 0.3185 - val_accuracy: 0.8636\n",
            "Epoch 18/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.3096 - accuracy: 0.8665 - val_loss: 0.3050 - val_accuracy: 0.8708\n",
            "Epoch 19/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.3070 - accuracy: 0.8675 - val_loss: 0.3039 - val_accuracy: 0.8705\n",
            "Epoch 20/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.3002 - accuracy: 0.8701 - val_loss: 0.3164 - val_accuracy: 0.8624\n",
            "Epoch 21/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2932 - accuracy: 0.8757 - val_loss: 0.2879 - val_accuracy: 0.8778\n",
            "Epoch 22/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2925 - accuracy: 0.8757 - val_loss: 0.3001 - val_accuracy: 0.8745\n",
            "Epoch 23/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2851 - accuracy: 0.8780 - val_loss: 0.3054 - val_accuracy: 0.8732\n",
            "Epoch 24/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2824 - accuracy: 0.8803 - val_loss: 0.2910 - val_accuracy: 0.8738\n",
            "Epoch 25/200\n",
            "416/416 [==============================] - 6s 15ms/step - loss: 0.2802 - accuracy: 0.8806 - val_loss: 0.2906 - val_accuracy: 0.8763\n",
            "Epoch 26/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2792 - accuracy: 0.8808 - val_loss: 0.2870 - val_accuracy: 0.8761\n",
            "Epoch 27/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2711 - accuracy: 0.8843 - val_loss: 0.2937 - val_accuracy: 0.8764\n",
            "Epoch 28/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2681 - accuracy: 0.8878 - val_loss: 0.2781 - val_accuracy: 0.8849\n",
            "Epoch 29/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2652 - accuracy: 0.8885 - val_loss: 0.2745 - val_accuracy: 0.8847\n",
            "Epoch 30/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2596 - accuracy: 0.8928 - val_loss: 0.2894 - val_accuracy: 0.8787\n",
            "Epoch 31/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2580 - accuracy: 0.8916 - val_loss: 0.2881 - val_accuracy: 0.8801\n",
            "Epoch 32/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2533 - accuracy: 0.8941 - val_loss: 0.2853 - val_accuracy: 0.8814\n",
            "Epoch 33/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2526 - accuracy: 0.8948 - val_loss: 0.2765 - val_accuracy: 0.8861\n",
            "Epoch 34/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2483 - accuracy: 0.8973 - val_loss: 0.2695 - val_accuracy: 0.8880\n",
            "Epoch 35/200\n",
            "416/416 [==============================] - 6s 16ms/step - loss: 0.2483 - accuracy: 0.8984 - val_loss: 0.2680 - val_accuracy: 0.8897\n",
            "Epoch 36/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2404 - accuracy: 0.8995 - val_loss: 0.2622 - val_accuracy: 0.8905\n",
            "Epoch 37/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2412 - accuracy: 0.8992 - val_loss: 0.2660 - val_accuracy: 0.8920\n",
            "Epoch 38/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2345 - accuracy: 0.9025 - val_loss: 0.2552 - val_accuracy: 0.8951\n",
            "Epoch 39/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2343 - accuracy: 0.9025 - val_loss: 0.2646 - val_accuracy: 0.8908\n",
            "Epoch 40/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2331 - accuracy: 0.9046 - val_loss: 0.2795 - val_accuracy: 0.8899\n",
            "Epoch 41/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2303 - accuracy: 0.9053 - val_loss: 0.2706 - val_accuracy: 0.8896\n",
            "Epoch 42/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2270 - accuracy: 0.9073 - val_loss: 0.2626 - val_accuracy: 0.8938\n",
            "Epoch 43/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2232 - accuracy: 0.9088 - val_loss: 0.2576 - val_accuracy: 0.8961\n",
            "Epoch 44/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2235 - accuracy: 0.9084 - val_loss: 0.2558 - val_accuracy: 0.8965\n",
            "Epoch 45/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2181 - accuracy: 0.9098 - val_loss: 0.2518 - val_accuracy: 0.8992\n",
            "Epoch 46/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.2166 - accuracy: 0.9119 - val_loss: 0.2580 - val_accuracy: 0.8959\n",
            "Epoch 47/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2161 - accuracy: 0.9108 - val_loss: 0.2687 - val_accuracy: 0.8868\n",
            "Epoch 48/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2130 - accuracy: 0.9141 - val_loss: 0.2521 - val_accuracy: 0.8968\n",
            "Epoch 49/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2084 - accuracy: 0.9149 - val_loss: 0.2590 - val_accuracy: 0.8929\n",
            "Epoch 50/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2060 - accuracy: 0.9161 - val_loss: 0.2540 - val_accuracy: 0.8959\n",
            "Epoch 51/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2055 - accuracy: 0.9157 - val_loss: 0.2512 - val_accuracy: 0.8968\n",
            "Epoch 52/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2028 - accuracy: 0.9167 - val_loss: 0.2478 - val_accuracy: 0.8995\n",
            "Epoch 53/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.2019 - accuracy: 0.9175 - val_loss: 0.2802 - val_accuracy: 0.8891\n",
            "Epoch 54/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.1992 - accuracy: 0.9183 - val_loss: 0.2452 - val_accuracy: 0.9012\n",
            "Epoch 55/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.1947 - accuracy: 0.9216 - val_loss: 0.2610 - val_accuracy: 0.8968\n",
            "Epoch 56/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.1956 - accuracy: 0.9205 - val_loss: 0.2823 - val_accuracy: 0.8810\n",
            "Epoch 57/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.1926 - accuracy: 0.9224 - val_loss: 0.2506 - val_accuracy: 0.9004\n",
            "Epoch 58/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.1914 - accuracy: 0.9228 - val_loss: 0.2548 - val_accuracy: 0.9001\n",
            "Epoch 59/200\n",
            "416/416 [==============================] - 6s 16ms/step - loss: 0.1924 - accuracy: 0.9211 - val_loss: 0.2544 - val_accuracy: 0.8970\n",
            "Epoch 60/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.1872 - accuracy: 0.9243 - val_loss: 0.2512 - val_accuracy: 0.8983\n",
            "Epoch 61/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.1878 - accuracy: 0.9243 - val_loss: 0.2463 - val_accuracy: 0.9033\n",
            "Epoch 62/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.1825 - accuracy: 0.9257 - val_loss: 0.2487 - val_accuracy: 0.8989\n",
            "Epoch 63/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.1844 - accuracy: 0.9254 - val_loss: 0.2431 - val_accuracy: 0.9010\n",
            "Epoch 64/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.1825 - accuracy: 0.9256 - val_loss: 0.2437 - val_accuracy: 0.8985\n",
            "Epoch 65/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.1811 - accuracy: 0.9289 - val_loss: 0.2538 - val_accuracy: 0.8968\n",
            "Epoch 66/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.1785 - accuracy: 0.9284 - val_loss: 0.2414 - val_accuracy: 0.9044\n",
            "Epoch 67/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.1752 - accuracy: 0.9291 - val_loss: 0.2417 - val_accuracy: 0.8992\n",
            "Epoch 68/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.1736 - accuracy: 0.9291 - val_loss: 0.2574 - val_accuracy: 0.8989\n",
            "Epoch 69/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.1718 - accuracy: 0.9306 - val_loss: 0.2479 - val_accuracy: 0.9056\n",
            "Epoch 70/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.1704 - accuracy: 0.9320 - val_loss: 0.2512 - val_accuracy: 0.8989\n",
            "Epoch 71/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.1666 - accuracy: 0.9335 - val_loss: 0.2501 - val_accuracy: 0.8995\n",
            "Epoch 72/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.1671 - accuracy: 0.9328 - val_loss: 0.2505 - val_accuracy: 0.9044\n",
            "Epoch 73/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.1640 - accuracy: 0.9347 - val_loss: 0.2481 - val_accuracy: 0.9034\n",
            "Epoch 74/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.1661 - accuracy: 0.9340 - val_loss: 0.2436 - val_accuracy: 0.9047\n",
            "Epoch 75/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.1628 - accuracy: 0.9354 - val_loss: 0.2559 - val_accuracy: 0.9052\n",
            "Epoch 76/200\n",
            "416/416 [==============================] - 7s 16ms/step - loss: 0.1605 - accuracy: 0.9366 - val_loss: 0.2724 - val_accuracy: 0.8961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(x_validation, y_validation, verbose=False)\n",
        "print(\"Validation Accuracy:  {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy: {:.4f}\".format(accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCFnbNpO6Woo",
        "outputId": "2c077f8c-d236-4b55-c7d1-71081c71c5cf"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy:  0.9044\n",
            "Testing Accuracy: 0.8308\n"
          ]
        }
      ]
    }
  ]
}