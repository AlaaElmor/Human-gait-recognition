{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STR_Transformer_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tkKetVwD2_TP"
      },
      "outputs": [],
      "source": [
        "from numpy import save, load\n",
        "from pandas import read_csv\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import keras.layers \n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import linear_model\n",
        "from sklearn import datasets\n",
        "from numpy import save, load\n",
        "from pandas import read_csv\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras import Model\n",
        "from keras.layers import Conv1D, SpatialDropout1D\n",
        "from keras.layers import Convolution1D, Dense,Activation\n",
        "from keras.models import Input, Model\n",
        "from keras.layers import LSTM, Conv1D, concatenate,GlobalMaxPooling1D,GlobalAveragePooling1D,TimeDistributed, MaxPooling1D\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "import keras.layers\n",
        "from keras import optimizers\n",
        "from keras.layers import Activation, Lambda\n",
        "from keras.layers import Convolution1D, Dense\n",
        "from keras.models import Input, Model\n",
        "from typing import List, Tuple \n",
        "from statistics import mean, stdev\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import linear_model\n",
        "from sklearn import datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/'  #change dir to your project folder\n",
        "\n",
        "import numpy as np\n",
        "x_train = np.load('gdrive/MyDrive/Colab Notebooks/Dataset1/trainX.npy')\n",
        "y_train = np.load('gdrive/My Drive/Colab Notebooks/Dataset1/trainy.npy')\n",
        "x_test = np.load('gdrive/My Drive/Colab Notebooks/Dataset1/testX.npy')\n",
        "y_test = np.load('gdrive/My Drive/Colab Notebooks/Dataset1/testy.npy')\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0dvBDLq3Bel",
        "outputId": "2d43108b-fc7b-4a8e-b3a4-7594dec6c164"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Mounted at /content/gdrive\n",
            "(33104, 128, 3) (33104, 118) (3740, 128, 3) (3740, 118)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.transpose(0,2,1)\n",
        "x_test = x_test.transpose(0,2,1)\n",
        "\n",
        "inputs = np.concatenate((x_train, x_test), axis=0)\n",
        "targets = np.concatenate((y_train, y_test), axis=0)\n",
        "#from sklearn.model_selection import train_test_split\n",
        "#x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=0.20, random_state=42)"
      ],
      "metadata": {
        "id": "n9czbi3y3BTn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTZkQ0vw3BMm",
        "outputId": "dead6fa4-d164-4b7b-efca-f86a79cded46"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(33104, 3, 128)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = (np.argmax(y_train, axis=1)).reshape(-1, 1)\n",
        "y_test =  (np.argmax(y_test, axis=1)).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "XxaL3HTjAFla"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Zt3bHnPAG8f",
        "outputId": "f60fea3d-1a06-4e41-9f6f-7cf789d6ce57"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(33104, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = x_train.shape[-1]   # Embedding size for each token\n",
        "num_heads = 4  # Number of attention heads\n",
        "ff_dim = x_train.shape[-1]  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads,\n",
        "                                             key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)  # self-attention layer\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)  # layer norm\n",
        "        ffn_output = self.ffn(out1)  #feed-forward layer\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)  # layer norm\n"
      ],
      "metadata": {
        "id": "jeqGmJkf3BFk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "num_folds = 2\n",
        "inputs = np.concatenate((x_train, x_test), axis=0)\n",
        "targets = np.concatenate((y_train, y_test), axis=0)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "fold_no = 1\n",
        "input_shape = x_train.shape[1:]\n",
        "input = Input(shape=x_train.shape[1:])\n",
        "for train, test in skf.split(inputs, targets):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Input(shape= x_train.shape[1:]))\n",
        "    model.add(TransformerBlock(embed_dim, num_heads, ff_dim))\n",
        "    model.add(layers.GlobalAveragePooling1D())\n",
        "    model.add(layers.Dropout(0.2))\n",
        "    model.add(layers.Dense(ff_dim, activation='relu'))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "    model.add(layers.Dense(118, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    history = model.fit(inputs[train], targets[train], epochs=200, verbose=True, batch_size=128,callbacks=[keras.callbacks.EarlyStopping(monitor='loss', patience=5,restore_best_weights=True)])\n",
        "    # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "model.summary()\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yv5eH0Dr8UvC",
        "outputId": "edd588e9-3d19-4b7f-eee3-1b3b29886805"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "231/231 [==============================] - 4s 11ms/step - loss: 2.4168 - accuracy: 0.4631\n",
            "Epoch 2/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 1.0636 - accuracy: 0.7477\n",
            "Epoch 3/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.7762 - accuracy: 0.8107\n",
            "Epoch 4/200\n",
            "231/231 [==============================] - 2s 11ms/step - loss: 0.6374 - accuracy: 0.8379\n",
            "Epoch 5/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.5534 - accuracy: 0.8577\n",
            "Epoch 6/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.4829 - accuracy: 0.8740\n",
            "Epoch 7/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.4443 - accuracy: 0.8812\n",
            "Epoch 8/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.4084 - accuracy: 0.8900\n",
            "Epoch 9/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.3762 - accuracy: 0.8960\n",
            "Epoch 10/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.3558 - accuracy: 0.8988\n",
            "Epoch 11/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.3382 - accuracy: 0.9055\n",
            "Epoch 12/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.3138 - accuracy: 0.9114\n",
            "Epoch 13/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.3086 - accuracy: 0.9128\n",
            "Epoch 14/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2910 - accuracy: 0.9153\n",
            "Epoch 15/200\n",
            "231/231 [==============================] - 3s 13ms/step - loss: 0.2711 - accuracy: 0.9207\n",
            "Epoch 16/200\n",
            "231/231 [==============================] - 3s 12ms/step - loss: 0.2704 - accuracy: 0.9211\n",
            "Epoch 17/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2568 - accuracy: 0.9249\n",
            "Epoch 18/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2535 - accuracy: 0.9243\n",
            "Epoch 19/200\n",
            "231/231 [==============================] - 3s 12ms/step - loss: 0.2418 - accuracy: 0.9284\n",
            "Epoch 20/200\n",
            "231/231 [==============================] - 3s 12ms/step - loss: 0.2407 - accuracy: 0.9282\n",
            "Epoch 21/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2300 - accuracy: 0.9294\n",
            "Epoch 22/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2213 - accuracy: 0.9332\n",
            "Epoch 23/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2219 - accuracy: 0.9328\n",
            "Epoch 24/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2038 - accuracy: 0.9371\n",
            "Epoch 25/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2094 - accuracy: 0.9365\n",
            "Epoch 26/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2067 - accuracy: 0.9363\n",
            "Epoch 27/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1969 - accuracy: 0.9393\n",
            "Epoch 28/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2001 - accuracy: 0.9389\n",
            "Epoch 29/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1897 - accuracy: 0.9419\n",
            "Epoch 30/200\n",
            "231/231 [==============================] - 2s 11ms/step - loss: 0.1906 - accuracy: 0.9417\n",
            "Epoch 31/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1843 - accuracy: 0.9423\n",
            "Epoch 32/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1835 - accuracy: 0.9443\n",
            "Epoch 33/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1839 - accuracy: 0.9425\n",
            "Epoch 34/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1783 - accuracy: 0.9456\n",
            "Epoch 35/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1769 - accuracy: 0.9451\n",
            "Epoch 36/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1681 - accuracy: 0.9471\n",
            "Epoch 37/200\n",
            "231/231 [==============================] - 2s 11ms/step - loss: 0.1639 - accuracy: 0.9490\n",
            "Epoch 38/200\n",
            "231/231 [==============================] - 2s 11ms/step - loss: 0.1708 - accuracy: 0.9471\n",
            "Epoch 39/200\n",
            "231/231 [==============================] - 2s 11ms/step - loss: 0.1692 - accuracy: 0.9462\n",
            "Epoch 40/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1610 - accuracy: 0.9498\n",
            "Epoch 41/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1659 - accuracy: 0.9484\n",
            "Epoch 42/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1613 - accuracy: 0.9492\n",
            "Epoch 43/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1550 - accuracy: 0.9503\n",
            "Epoch 44/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1552 - accuracy: 0.9517\n",
            "Epoch 45/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1584 - accuracy: 0.9508\n",
            "Epoch 46/200\n",
            "231/231 [==============================] - 2s 11ms/step - loss: 0.1567 - accuracy: 0.9498\n",
            "Epoch 47/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1530 - accuracy: 0.9514\n",
            "Epoch 48/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1421 - accuracy: 0.9548\n",
            "Epoch 49/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1526 - accuracy: 0.9525\n",
            "Epoch 50/200\n",
            "231/231 [==============================] - 2s 11ms/step - loss: 0.1499 - accuracy: 0.9528\n",
            "Epoch 51/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1352 - accuracy: 0.9574\n",
            "Epoch 52/200\n",
            "231/231 [==============================] - 2s 11ms/step - loss: 0.1360 - accuracy: 0.9569\n",
            "Epoch 53/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1429 - accuracy: 0.9536\n",
            "Epoch 54/200\n",
            "231/231 [==============================] - 2s 11ms/step - loss: 0.1459 - accuracy: 0.9535\n",
            "Epoch 55/200\n",
            "231/231 [==============================] - 2s 11ms/step - loss: 0.1374 - accuracy: 0.9564\n",
            "Epoch 56/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1362 - accuracy: 0.9574\n",
            "Score for fold 1: loss of 0.4109150171279907; accuracy of 92.0341968536377%\n",
            "Epoch 1/200\n",
            "231/231 [==============================] - 4s 11ms/step - loss: 2.5034 - accuracy: 0.4325\n",
            "Epoch 2/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 1.1398 - accuracy: 0.7258\n",
            "Epoch 3/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.8139 - accuracy: 0.7995\n",
            "Epoch 4/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.6535 - accuracy: 0.8345\n",
            "Epoch 5/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.5689 - accuracy: 0.8532\n",
            "Epoch 6/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.5038 - accuracy: 0.8668\n",
            "Epoch 7/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.4511 - accuracy: 0.8789\n",
            "Epoch 8/200\n",
            "231/231 [==============================] - 2s 11ms/step - loss: 0.4079 - accuracy: 0.8903\n",
            "Epoch 9/200\n",
            "231/231 [==============================] - 2s 11ms/step - loss: 0.3798 - accuracy: 0.8959\n",
            "Epoch 10/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.3515 - accuracy: 0.9024\n",
            "Epoch 11/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.3398 - accuracy: 0.9040\n",
            "Epoch 12/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.3209 - accuracy: 0.9088\n",
            "Epoch 13/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.3118 - accuracy: 0.9113\n",
            "Epoch 14/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2889 - accuracy: 0.9179\n",
            "Epoch 15/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2800 - accuracy: 0.9185\n",
            "Epoch 16/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2676 - accuracy: 0.9213\n",
            "Epoch 17/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2620 - accuracy: 0.9223\n",
            "Epoch 18/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2531 - accuracy: 0.9260\n",
            "Epoch 19/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2446 - accuracy: 0.9270\n",
            "Epoch 20/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2362 - accuracy: 0.9307\n",
            "Epoch 21/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2303 - accuracy: 0.9319\n",
            "Epoch 22/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2257 - accuracy: 0.9315\n",
            "Epoch 23/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2224 - accuracy: 0.9341\n",
            "Epoch 24/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2120 - accuracy: 0.9352\n",
            "Epoch 25/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2102 - accuracy: 0.9365\n",
            "Epoch 26/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2072 - accuracy: 0.9349\n",
            "Epoch 27/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1929 - accuracy: 0.9406\n",
            "Epoch 28/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1996 - accuracy: 0.9382\n",
            "Epoch 29/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2015 - accuracy: 0.9394\n",
            "Epoch 30/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1842 - accuracy: 0.9435\n",
            "Epoch 31/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1861 - accuracy: 0.9424\n",
            "Epoch 32/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1845 - accuracy: 0.9424\n",
            "Epoch 33/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1840 - accuracy: 0.9430\n",
            "Epoch 34/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1742 - accuracy: 0.9449\n",
            "Epoch 35/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1765 - accuracy: 0.9462\n",
            "Epoch 36/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1707 - accuracy: 0.9459\n",
            "Epoch 37/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1725 - accuracy: 0.9451\n",
            "Epoch 38/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1785 - accuracy: 0.9439\n",
            "Epoch 39/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1782 - accuracy: 0.9460\n",
            "Epoch 40/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1564 - accuracy: 0.9510\n",
            "Epoch 41/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1679 - accuracy: 0.9475\n",
            "Epoch 42/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1584 - accuracy: 0.9499\n",
            "Epoch 43/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1613 - accuracy: 0.9495\n",
            "Epoch 44/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1551 - accuracy: 0.9512\n",
            "Epoch 45/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1521 - accuracy: 0.9515\n",
            "Epoch 46/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1468 - accuracy: 0.9534\n",
            "Epoch 47/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1409 - accuracy: 0.9554\n",
            "Epoch 48/200\n",
            "231/231 [==============================] - 2s 11ms/step - loss: 0.1544 - accuracy: 0.9509\n",
            "Epoch 49/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1530 - accuracy: 0.9523\n",
            "Epoch 50/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1461 - accuracy: 0.9536\n",
            "Epoch 51/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1416 - accuracy: 0.9556\n",
            "Epoch 52/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1458 - accuracy: 0.9527\n",
            "Score for fold 2: loss of 0.38824936747550964; accuracy of 92.40059852600098%\n",
            "Epoch 1/200\n",
            "231/231 [==============================] - 4s 12ms/step - loss: 2.4601 - accuracy: 0.4422\n",
            "Epoch 2/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 1.1132 - accuracy: 0.7336\n",
            "Epoch 3/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.8065 - accuracy: 0.8016\n",
            "Epoch 4/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.6557 - accuracy: 0.8351\n",
            "Epoch 5/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.5635 - accuracy: 0.8563\n",
            "Epoch 6/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.5088 - accuracy: 0.8673\n",
            "Epoch 7/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.4516 - accuracy: 0.8812\n",
            "Epoch 8/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.4142 - accuracy: 0.8882\n",
            "Epoch 9/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.3813 - accuracy: 0.8975\n",
            "Epoch 10/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.3514 - accuracy: 0.9035\n",
            "Epoch 11/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.3367 - accuracy: 0.9064\n",
            "Epoch 12/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.3204 - accuracy: 0.9088\n",
            "Epoch 13/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.3041 - accuracy: 0.9143\n",
            "Epoch 14/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2944 - accuracy: 0.9163\n",
            "Epoch 15/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2845 - accuracy: 0.9185\n",
            "Epoch 16/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2676 - accuracy: 0.9228\n",
            "Epoch 17/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2561 - accuracy: 0.9247\n",
            "Epoch 18/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2462 - accuracy: 0.9283\n",
            "Epoch 19/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2356 - accuracy: 0.9307\n",
            "Epoch 20/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2358 - accuracy: 0.9302\n",
            "Epoch 21/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2270 - accuracy: 0.9333\n",
            "Epoch 22/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2188 - accuracy: 0.9344\n",
            "Epoch 23/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2153 - accuracy: 0.9353\n",
            "Epoch 24/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2097 - accuracy: 0.9368\n",
            "Epoch 25/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2070 - accuracy: 0.9376\n",
            "Epoch 26/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2036 - accuracy: 0.9389\n",
            "Epoch 27/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1990 - accuracy: 0.9399\n",
            "Epoch 28/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1919 - accuracy: 0.9410\n",
            "Epoch 29/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1902 - accuracy: 0.9404\n",
            "Epoch 30/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1884 - accuracy: 0.9417\n",
            "Epoch 31/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1803 - accuracy: 0.9441\n",
            "Epoch 32/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1750 - accuracy: 0.9468\n",
            "Epoch 33/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1738 - accuracy: 0.9473\n",
            "Epoch 34/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1776 - accuracy: 0.9451\n",
            "Epoch 35/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1660 - accuracy: 0.9488\n",
            "Epoch 36/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1700 - accuracy: 0.9466\n",
            "Epoch 37/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1628 - accuracy: 0.9504\n",
            "Epoch 38/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1684 - accuracy: 0.9471\n",
            "Epoch 39/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1672 - accuracy: 0.9482\n",
            "Epoch 40/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1585 - accuracy: 0.9496\n",
            "Epoch 41/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1705 - accuracy: 0.9473\n",
            "Epoch 42/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1568 - accuracy: 0.9509\n",
            "Epoch 43/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1486 - accuracy: 0.9520\n",
            "Epoch 44/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1539 - accuracy: 0.9518\n",
            "Epoch 45/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1527 - accuracy: 0.9532\n",
            "Epoch 46/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1541 - accuracy: 0.9507\n",
            "Epoch 47/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1438 - accuracy: 0.9553\n",
            "Epoch 48/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1447 - accuracy: 0.9533\n",
            "Epoch 49/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1388 - accuracy: 0.9556\n",
            "Epoch 50/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1409 - accuracy: 0.9564\n",
            "Epoch 51/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1380 - accuracy: 0.9568\n",
            "Epoch 52/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1460 - accuracy: 0.9528\n",
            "Epoch 53/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1370 - accuracy: 0.9567\n",
            "Epoch 54/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1486 - accuracy: 0.9544\n",
            "Epoch 55/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1407 - accuracy: 0.9550\n",
            "Epoch 56/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1305 - accuracy: 0.9581\n",
            "Epoch 57/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1322 - accuracy: 0.9575\n",
            "Epoch 58/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1355 - accuracy: 0.9573\n",
            "Epoch 59/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1330 - accuracy: 0.9574\n",
            "Epoch 60/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1284 - accuracy: 0.9578\n",
            "Epoch 61/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1430 - accuracy: 0.9553\n",
            "Epoch 62/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1264 - accuracy: 0.9584\n",
            "Epoch 63/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1316 - accuracy: 0.9577\n",
            "Epoch 64/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1250 - accuracy: 0.9596\n",
            "Epoch 65/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1273 - accuracy: 0.9589\n",
            "Epoch 66/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1211 - accuracy: 0.9606\n",
            "Epoch 67/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1297 - accuracy: 0.9592\n",
            "Epoch 68/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1287 - accuracy: 0.9595\n",
            "Epoch 69/200\n",
            "231/231 [==============================] - 3s 12ms/step - loss: 0.1282 - accuracy: 0.9584\n",
            "Epoch 70/200\n",
            "231/231 [==============================] - 3s 12ms/step - loss: 0.1229 - accuracy: 0.9609\n",
            "Epoch 71/200\n",
            "231/231 [==============================] - 3s 12ms/step - loss: 0.1172 - accuracy: 0.9625\n",
            "Epoch 72/200\n",
            "231/231 [==============================] - 3s 12ms/step - loss: 0.1139 - accuracy: 0.9645\n",
            "Epoch 73/200\n",
            "231/231 [==============================] - 3s 12ms/step - loss: 0.1214 - accuracy: 0.9605\n",
            "Epoch 74/200\n",
            "231/231 [==============================] - 3s 12ms/step - loss: 0.1217 - accuracy: 0.9600\n",
            "Epoch 75/200\n",
            "231/231 [==============================] - 3s 12ms/step - loss: 0.1134 - accuracy: 0.9629\n",
            "Epoch 76/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1183 - accuracy: 0.9621\n",
            "Epoch 77/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1187 - accuracy: 0.9624\n",
            "Epoch 78/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1129 - accuracy: 0.9628\n",
            "Epoch 79/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1171 - accuracy: 0.9616\n",
            "Epoch 80/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1117 - accuracy: 0.9632\n",
            "Epoch 81/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1173 - accuracy: 0.9616\n",
            "Epoch 82/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1102 - accuracy: 0.9639\n",
            "Epoch 83/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1124 - accuracy: 0.9645\n",
            "Epoch 84/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1128 - accuracy: 0.9635\n",
            "Epoch 85/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1111 - accuracy: 0.9636\n",
            "Epoch 86/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1204 - accuracy: 0.9616\n",
            "Epoch 87/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1159 - accuracy: 0.9629\n",
            "Score for fold 3: loss of 0.4437847137451172; accuracy of 92.35988855361938%\n",
            "Epoch 1/200\n",
            "231/231 [==============================] - 4s 11ms/step - loss: 2.4125 - accuracy: 0.4551\n",
            "Epoch 2/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 1.0532 - accuracy: 0.7450\n",
            "Epoch 3/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.7676 - accuracy: 0.8106\n",
            "Epoch 4/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.6291 - accuracy: 0.8413\n",
            "Epoch 5/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.5380 - accuracy: 0.8612\n",
            "Epoch 6/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.4808 - accuracy: 0.8740\n",
            "Epoch 7/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.4362 - accuracy: 0.8839\n",
            "Epoch 8/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.4044 - accuracy: 0.8910\n",
            "Epoch 9/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.3712 - accuracy: 0.8984\n",
            "Epoch 10/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.3543 - accuracy: 0.9030\n",
            "Epoch 11/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.3290 - accuracy: 0.9061\n",
            "Epoch 12/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.3121 - accuracy: 0.9117\n",
            "Epoch 13/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2931 - accuracy: 0.9156\n",
            "Epoch 14/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2827 - accuracy: 0.9196\n",
            "Epoch 15/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2690 - accuracy: 0.9236\n",
            "Epoch 16/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2652 - accuracy: 0.9218\n",
            "Epoch 17/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2454 - accuracy: 0.9289\n",
            "Epoch 18/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2307 - accuracy: 0.9321\n",
            "Epoch 19/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2308 - accuracy: 0.9320\n",
            "Epoch 20/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2338 - accuracy: 0.9303\n",
            "Epoch 21/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2166 - accuracy: 0.9349\n",
            "Epoch 22/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2159 - accuracy: 0.9352\n",
            "Epoch 23/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2169 - accuracy: 0.9349\n",
            "Epoch 24/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2034 - accuracy: 0.9378\n",
            "Epoch 25/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2014 - accuracy: 0.9389\n",
            "Epoch 26/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2083 - accuracy: 0.9366\n",
            "Epoch 27/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1929 - accuracy: 0.9412\n",
            "Epoch 28/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1857 - accuracy: 0.9421\n",
            "Epoch 29/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1855 - accuracy: 0.9427\n",
            "Epoch 30/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1822 - accuracy: 0.9430\n",
            "Epoch 31/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1809 - accuracy: 0.9442\n",
            "Epoch 32/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1853 - accuracy: 0.9428\n",
            "Epoch 33/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1694 - accuracy: 0.9481\n",
            "Epoch 34/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1682 - accuracy: 0.9473\n",
            "Epoch 35/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1694 - accuracy: 0.9481\n",
            "Epoch 36/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1578 - accuracy: 0.9507\n",
            "Epoch 37/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1613 - accuracy: 0.9486\n",
            "Epoch 38/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1590 - accuracy: 0.9501\n",
            "Epoch 39/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1608 - accuracy: 0.9484\n",
            "Epoch 40/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1608 - accuracy: 0.9499\n",
            "Epoch 41/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1549 - accuracy: 0.9519\n",
            "Epoch 42/200\n",
            "231/231 [==============================] - 3s 12ms/step - loss: 0.1499 - accuracy: 0.9529\n",
            "Epoch 43/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1602 - accuracy: 0.9498\n",
            "Epoch 44/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1560 - accuracy: 0.9496\n",
            "Epoch 45/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1394 - accuracy: 0.9555\n",
            "Epoch 46/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1641 - accuracy: 0.9495\n",
            "Epoch 47/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1509 - accuracy: 0.9524\n",
            "Epoch 48/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1450 - accuracy: 0.9529\n",
            "Epoch 49/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1416 - accuracy: 0.9541\n",
            "Epoch 50/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1394 - accuracy: 0.9558\n",
            "Score for fold 4: loss of 0.38448891043663025; accuracy of 92.50916242599487%\n",
            "Epoch 1/200\n",
            "231/231 [==============================] - 4s 11ms/step - loss: 2.4102 - accuracy: 0.4579\n",
            "Epoch 2/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 1.0731 - accuracy: 0.7448\n",
            "Epoch 3/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.7823 - accuracy: 0.8101\n",
            "Epoch 4/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.6449 - accuracy: 0.8392\n",
            "Epoch 5/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.5639 - accuracy: 0.8564\n",
            "Epoch 6/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.4945 - accuracy: 0.8693\n",
            "Epoch 7/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.4428 - accuracy: 0.8831\n",
            "Epoch 8/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.4086 - accuracy: 0.8897\n",
            "Epoch 9/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.3679 - accuracy: 0.9004\n",
            "Epoch 10/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.3516 - accuracy: 0.9031\n",
            "Epoch 11/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.3342 - accuracy: 0.9060\n",
            "Epoch 12/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.3164 - accuracy: 0.9107\n",
            "Epoch 13/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2948 - accuracy: 0.9167\n",
            "Epoch 14/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2963 - accuracy: 0.9151\n",
            "Epoch 15/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2712 - accuracy: 0.9226\n",
            "Epoch 16/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2643 - accuracy: 0.9226\n",
            "Epoch 17/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2450 - accuracy: 0.9286\n",
            "Epoch 18/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2460 - accuracy: 0.9272\n",
            "Epoch 19/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2362 - accuracy: 0.9302\n",
            "Epoch 20/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2337 - accuracy: 0.9293\n",
            "Epoch 21/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2236 - accuracy: 0.9330\n",
            "Epoch 22/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2089 - accuracy: 0.9381\n",
            "Epoch 23/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2149 - accuracy: 0.9370\n",
            "Epoch 24/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2060 - accuracy: 0.9382\n",
            "Epoch 25/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.2025 - accuracy: 0.9387\n",
            "Epoch 26/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1955 - accuracy: 0.9406\n",
            "Epoch 27/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1898 - accuracy: 0.9417\n",
            "Epoch 28/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1889 - accuracy: 0.9424\n",
            "Epoch 29/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1881 - accuracy: 0.9423\n",
            "Epoch 30/200\n",
            "231/231 [==============================] - 3s 12ms/step - loss: 0.1824 - accuracy: 0.9438\n",
            "Epoch 31/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1792 - accuracy: 0.9447\n",
            "Epoch 32/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1695 - accuracy: 0.9464\n",
            "Epoch 33/200\n",
            "231/231 [==============================] - 3s 12ms/step - loss: 0.1825 - accuracy: 0.9454\n",
            "Epoch 34/200\n",
            "231/231 [==============================] - 3s 12ms/step - loss: 0.1801 - accuracy: 0.9438\n",
            "Epoch 35/200\n",
            "231/231 [==============================] - 3s 12ms/step - loss: 0.1654 - accuracy: 0.9490\n",
            "Epoch 36/200\n",
            "231/231 [==============================] - 3s 12ms/step - loss: 0.1600 - accuracy: 0.9499\n",
            "Epoch 37/200\n",
            "231/231 [==============================] - 3s 12ms/step - loss: 0.1574 - accuracy: 0.9515\n",
            "Epoch 38/200\n",
            "231/231 [==============================] - 3s 12ms/step - loss: 0.1546 - accuracy: 0.9524\n",
            "Epoch 39/200\n",
            "231/231 [==============================] - 3s 12ms/step - loss: 0.1600 - accuracy: 0.9483\n",
            "Epoch 40/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1596 - accuracy: 0.9508\n",
            "Epoch 41/200\n",
            "231/231 [==============================] - 3s 12ms/step - loss: 0.1608 - accuracy: 0.9478\n",
            "Epoch 42/200\n",
            "231/231 [==============================] - 3s 12ms/step - loss: 0.1543 - accuracy: 0.9513\n",
            "Epoch 43/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1522 - accuracy: 0.9516\n",
            "Epoch 44/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1561 - accuracy: 0.9504\n",
            "Epoch 45/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1566 - accuracy: 0.9516\n",
            "Epoch 46/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1494 - accuracy: 0.9537\n",
            "Epoch 47/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1434 - accuracy: 0.9553\n",
            "Epoch 48/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1364 - accuracy: 0.9563\n",
            "Epoch 49/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1444 - accuracy: 0.9538\n",
            "Epoch 50/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1400 - accuracy: 0.9557\n",
            "Epoch 51/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1383 - accuracy: 0.9563\n",
            "Epoch 52/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1377 - accuracy: 0.9557\n",
            "Epoch 53/200\n",
            "231/231 [==============================] - 3s 11ms/step - loss: 0.1459 - accuracy: 0.9546\n",
            "Score for fold 5: loss of 0.35381293296813965; accuracy of 92.90173649787903%\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " transformer_block_5 (Transf  (None, 3, 128)           297344    \n",
            " ormerBlock)                                                     \n",
            "                                                                 \n",
            " global_average_pooling1d_5   (None, 128)              0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dropout_22 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_23 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 118)               15222     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 329,078\n",
            "Trainable params: 329,078\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jl4ubVga3A_c",
        "outputId": "06429234-b6f0-4a5d-fe71-8e59b606f0b2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.4109150171279907 - Accuracy: 92.0341968536377%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.38824936747550964 - Accuracy: 92.40059852600098%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.4437847137451172 - Accuracy: 92.35988855361938%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.38448891043663025 - Accuracy: 92.50916242599487%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.35381293296813965 - Accuracy: 92.90173649787903%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 92.44111657142639 (+- 0.2795082519627537)\n",
            "> Loss: 0.3962501883506775\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6DYP67Sf3A4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dwUP_eAN3Av4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}