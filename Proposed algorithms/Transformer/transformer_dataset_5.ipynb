{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "2jIRhLtI55E6"
      },
      "outputs": [],
      "source": [
        "from numpy import save, load\n",
        "from pandas import read_csv\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/'  #change dir to your project folder\n",
        "\n",
        "import numpy as np\n",
        "x_train = np.load('gdrive/My Drive/dataset/dataset5/trainX.npy')\n",
        "y_train = np.load('gdrive/My Drive/dataset/dataset5/trainy.npy')\n",
        "x_test = np.load('gdrive/My Drive/dataset/dataset5/testX.npy')\n",
        "y_test = np.load('gdrive/My Drive/dataset/dataset5/testy.npy')\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZao1z7K6BSL",
        "outputId": "80c516e3-fb77-4986-9657-5fad90af2342"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Mounted at /content/gdrive\n",
            "(66542, 256, 6) (66542, 2) (7600, 256, 6) (7600, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.transpose(0,2,1)\n",
        "x_test = x_test.transpose(0,2,1)\n",
        "\n",
        "idx = np.random.permutation(len(x_train))\n",
        "x_train = x_train[idx]\n",
        "y_train = y_train[idx]\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=0.20, random_state=42)\n"
      ],
      "metadata": {
        "id": "GMlK0Tf36BVc"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PjnE9_G-dUl",
        "outputId": "e73c15d3-b6b0-492b-c64a-2e176c383311"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(53233, 6, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = x_train.shape[-1]   # Embedding size for each token\n",
        "num_heads = 4  # Number of attention heads\n",
        "ff_dim = x_train.shape[-1]  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads,\n",
        "                                             key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)  # self-attention layer\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)  # layer norm\n",
        "        ffn_output = self.ffn(out1)  #feed-forward layer\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)  # layer norm\n",
        "model = keras.Sequential()\n",
        "model.add(layers.Input(shape= x_train.shape[1:]))\n",
        "model.add(TransformerBlock(embed_dim, num_heads, ff_dim))\n",
        "model.add(layers.GlobalAveragePooling1D())\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(ff_dim, activation='relu'))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "model.summary()\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQQWrG_f6Bhz",
        "outputId": "63004a04-a7b4-45b7-9a4f-34e4831697d9"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " transformer_block_8 (Transf  (None, 6, 256)           1184512   \n",
            " ormerBlock)                                                     \n",
            "                                                                 \n",
            " global_average_pooling1d_8   (None, 256)              0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dropout_34 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_35 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,250,818\n",
            "Trainable params: 1,250,818\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, epochs=200, verbose=True, validation_data=(x_validation, y_validation), batch_size=128,callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBDRw2UA6BlK",
        "outputId": "185ffe67-f1e5-443a-c3a9-bb16ed6f99e2"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "416/416 [==============================] - 9s 19ms/step - loss: 0.3539 - accuracy: 0.8503 - val_loss: 0.2763 - val_accuracy: 0.8924\n",
            "Epoch 2/200\n",
            "416/416 [==============================] - 7s 18ms/step - loss: 0.2671 - accuracy: 0.8936 - val_loss: 0.2306 - val_accuracy: 0.9044\n",
            "Epoch 3/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.2356 - accuracy: 0.9037 - val_loss: 0.2064 - val_accuracy: 0.9167\n",
            "Epoch 4/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.2129 - accuracy: 0.9127 - val_loss: 0.1971 - val_accuracy: 0.9238\n",
            "Epoch 5/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.1984 - accuracy: 0.9193 - val_loss: 0.1804 - val_accuracy: 0.9282\n",
            "Epoch 6/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.1827 - accuracy: 0.9273 - val_loss: 0.1731 - val_accuracy: 0.9320\n",
            "Epoch 7/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.1700 - accuracy: 0.9321 - val_loss: 0.1609 - val_accuracy: 0.9345\n",
            "Epoch 8/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.1566 - accuracy: 0.9367 - val_loss: 0.1655 - val_accuracy: 0.9326\n",
            "Epoch 9/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.1468 - accuracy: 0.9405 - val_loss: 0.1580 - val_accuracy: 0.9355\n",
            "Epoch 10/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.1375 - accuracy: 0.9457 - val_loss: 0.1572 - val_accuracy: 0.9381\n",
            "Epoch 11/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.1309 - accuracy: 0.9476 - val_loss: 0.1591 - val_accuracy: 0.9385\n",
            "Epoch 12/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.1218 - accuracy: 0.9518 - val_loss: 0.1488 - val_accuracy: 0.9445\n",
            "Epoch 13/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.1143 - accuracy: 0.9547 - val_loss: 0.1443 - val_accuracy: 0.9449\n",
            "Epoch 14/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.1049 - accuracy: 0.9586 - val_loss: 0.1410 - val_accuracy: 0.9462\n",
            "Epoch 15/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.0984 - accuracy: 0.9608 - val_loss: 0.1379 - val_accuracy: 0.9475\n",
            "Epoch 16/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.0918 - accuracy: 0.9642 - val_loss: 0.1500 - val_accuracy: 0.9458\n",
            "Epoch 17/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.0869 - accuracy: 0.9657 - val_loss: 0.1363 - val_accuracy: 0.9509\n",
            "Epoch 18/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.0824 - accuracy: 0.9678 - val_loss: 0.1412 - val_accuracy: 0.9513\n",
            "Epoch 19/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.0764 - accuracy: 0.9706 - val_loss: 0.1477 - val_accuracy: 0.9467\n",
            "Epoch 20/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.0720 - accuracy: 0.9721 - val_loss: 0.1585 - val_accuracy: 0.9503\n",
            "Epoch 21/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.0662 - accuracy: 0.9740 - val_loss: 0.1511 - val_accuracy: 0.9521\n",
            "Epoch 22/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.0623 - accuracy: 0.9764 - val_loss: 0.1486 - val_accuracy: 0.9510\n",
            "Epoch 23/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.0577 - accuracy: 0.9779 - val_loss: 0.1677 - val_accuracy: 0.9482\n",
            "Epoch 24/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.0557 - accuracy: 0.9785 - val_loss: 0.1585 - val_accuracy: 0.9512\n",
            "Epoch 25/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.0505 - accuracy: 0.9809 - val_loss: 0.1657 - val_accuracy: 0.9509\n",
            "Epoch 26/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.0469 - accuracy: 0.9818 - val_loss: 0.2111 - val_accuracy: 0.9456\n",
            "Epoch 27/200\n",
            "416/416 [==============================] - 7s 17ms/step - loss: 0.0479 - accuracy: 0.9816 - val_loss: 0.1687 - val_accuracy: 0.9527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(x_validation, y_validation, verbose=False)\n",
        "print(\"Validation Accuracy:  {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy: {:.4f}\".format(accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCFnbNpO6Woo",
        "outputId": "0a760a24-78bd-4314-981d-540d1b1ef325"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy:  0.9509\n",
            "Testing Accuracy: 0.9386\n"
          ]
        }
      ]
    }
  ]
}