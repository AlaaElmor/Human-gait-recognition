{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YQFREF7L5GqL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ec98168-9ee7-4b3b-d0da-934b6f4b8298"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/'  #change dir to your project folder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import save, load\n",
        "from pandas import read_csv\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras import Model\n",
        "from keras.layers import Conv1D, SpatialDropout1D\n",
        "from keras.layers import Convolution1D, Dense,Activation\n",
        "from keras.models import Input, Model\n",
        "from keras.layers import LSTM, Conv1D, concatenate,GlobalMaxPooling1D,GlobalAveragePooling1D,TimeDistributed, MaxPooling1D\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "import keras.layers\n",
        "from keras import optimizers\n",
        "from keras.layers import Activation, Lambda\n",
        "from keras.layers import Convolution1D, Dense\n",
        "from keras.models import Input, Model\n",
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "metadata": {
        "id": "fc27_BqAfIsY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxaKZAFk5TvK",
        "outputId": "29f482da-4853-4856-80a0-3ac3d6da6840"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(44339, 128, 6) (44339, 20) (4936, 128, 6) (4936, 20)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "x_train = np.load('gdrive/My Drive/dataset/dataset2/trainX.npy')\n",
        "y_train = np.load('gdrive/My Drive/dataset/dataset2/trainy.npy')\n",
        "x_test = np.load('gdrive/My Drive/dataset/dataset2/testX.npy')\n",
        "y_test = np.load('gdrive/My Drive/dataset/dataset2/testy.npy')\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.transpose(0,2,1)\n",
        "x_test = x_test.transpose(0,2,1)"
      ],
      "metadata": {
        "id": "NqIuESQUkJAB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge inputs and targets\n",
        "inputs = np.concatenate((x_train, x_test), axis=0)\n",
        "targets = np.concatenate((y_train, y_test), axis=0)"
      ],
      "metadata": {
        "id": "kJ5-xFHXhyBV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OqP6bY3k5UZH"
      },
      "outputs": [],
      "source": [
        "embed_dim = x_train.shape[-1]   # Embedding size for each token\n",
        "num_heads = 4  # Number of attention heads\n",
        "ff_dim = x_train.shape[-1]  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads,\n",
        "                                             key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)  # self-attention layer\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)  # layer norm\n",
        "        ffn_output = self.ffn(out1)  #feed-forward layer\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)  # layer norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fampnwRy5XUm",
        "outputId": "cbca32c4-ac8b-4f57-81ef-9fd31c7b5cc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/200\n",
            "308/308 [==============================] - 8s 13ms/step - loss: 0.9026 - accuracy: 0.7452\n",
            "Epoch 2/200\n",
            "308/308 [==============================] - 4s 12ms/step - loss: 0.3155 - accuracy: 0.9128\n",
            "Epoch 3/200\n",
            "308/308 [==============================] - 4s 14ms/step - loss: 0.2437 - accuracy: 0.9311\n",
            "Epoch 4/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1931 - accuracy: 0.9458\n",
            "Epoch 5/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1723 - accuracy: 0.9508\n",
            "Epoch 6/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1565 - accuracy: 0.9545\n",
            "Epoch 7/200\n",
            "308/308 [==============================] - 4s 12ms/step - loss: 0.1401 - accuracy: 0.9584\n",
            "Epoch 8/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1270 - accuracy: 0.9625\n",
            "Epoch 9/200\n",
            "308/308 [==============================] - 4s 12ms/step - loss: 0.1163 - accuracy: 0.9647\n",
            "Epoch 10/200\n",
            "308/308 [==============================] - 4s 12ms/step - loss: 0.1123 - accuracy: 0.9656\n",
            "Epoch 11/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1117 - accuracy: 0.9664\n",
            "Epoch 12/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1020 - accuracy: 0.9686\n",
            "Epoch 13/200\n",
            "308/308 [==============================] - 4s 12ms/step - loss: 0.1029 - accuracy: 0.9678\n",
            "Epoch 14/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1108 - accuracy: 0.9655\n",
            "Epoch 15/200\n",
            "308/308 [==============================] - 4s 12ms/step - loss: 0.0874 - accuracy: 0.9726\n",
            "Epoch 16/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0763 - accuracy: 0.9761\n",
            "Epoch 17/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0801 - accuracy: 0.9749\n",
            "Epoch 18/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0853 - accuracy: 0.9737\n",
            "Epoch 19/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0748 - accuracy: 0.9771\n",
            "Epoch 20/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0842 - accuracy: 0.9743\n",
            "Epoch 21/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0797 - accuracy: 0.9752\n",
            "Epoch 22/200\n",
            "308/308 [==============================] - 4s 12ms/step - loss: 0.0707 - accuracy: 0.9770\n",
            "Epoch 23/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0724 - accuracy: 0.9767\n",
            "Epoch 24/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0658 - accuracy: 0.9794\n",
            "Epoch 25/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0714 - accuracy: 0.9772\n",
            "Epoch 26/200\n",
            "308/308 [==============================] - 4s 12ms/step - loss: 0.0761 - accuracy: 0.9758\n",
            "Epoch 27/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0640 - accuracy: 0.9799\n",
            "Epoch 28/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0676 - accuracy: 0.9788\n",
            "Epoch 29/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0755 - accuracy: 0.9761\n",
            "Epoch 30/200\n",
            "308/308 [==============================] - 4s 12ms/step - loss: 0.0695 - accuracy: 0.9787\n",
            "Epoch 31/200\n",
            "308/308 [==============================] - 4s 12ms/step - loss: 0.0563 - accuracy: 0.9818\n",
            "Epoch 32/200\n",
            "308/308 [==============================] - 4s 12ms/step - loss: 0.0612 - accuracy: 0.9810\n",
            "Epoch 33/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0722 - accuracy: 0.9778\n",
            "Epoch 34/200\n",
            "308/308 [==============================] - 4s 12ms/step - loss: 0.0606 - accuracy: 0.9809\n",
            "Epoch 35/200\n",
            "308/308 [==============================] - 4s 12ms/step - loss: 0.0518 - accuracy: 0.9834\n",
            "Epoch 36/200\n",
            "308/308 [==============================] - 4s 12ms/step - loss: 0.0531 - accuracy: 0.9829\n",
            "Epoch 37/200\n",
            "308/308 [==============================] - 4s 12ms/step - loss: 0.0921 - accuracy: 0.9718\n",
            "Epoch 38/200\n",
            "308/308 [==============================] - 4s 12ms/step - loss: 0.0516 - accuracy: 0.9843\n",
            "Epoch 39/200\n",
            "308/308 [==============================] - 4s 12ms/step - loss: 0.0495 - accuracy: 0.9842\n",
            "Epoch 40/200\n",
            "308/308 [==============================] - 4s 12ms/step - loss: 0.0631 - accuracy: 0.9795\n",
            "Epoch 41/200\n",
            "308/308 [==============================] - 4s 12ms/step - loss: 0.0552 - accuracy: 0.9825\n",
            "Epoch 42/200\n",
            "308/308 [==============================] - 4s 12ms/step - loss: 0.0497 - accuracy: 0.9839\n",
            "Epoch 43/200\n",
            "308/308 [==============================] - 4s 12ms/step - loss: 0.0479 - accuracy: 0.9851\n",
            "Epoch 44/200\n",
            "308/308 [==============================] - 4s 12ms/step - loss: 0.0586 - accuracy: 0.9816\n",
            "Epoch 45/200\n",
            "308/308 [==============================] - 4s 12ms/step - loss: 0.0709 - accuracy: 0.9777\n",
            "Epoch 46/200\n",
            "308/308 [==============================] - 4s 12ms/step - loss: 0.0524 - accuracy: 0.9840\n",
            "Epoch 47/200\n",
            "308/308 [==============================] - 4s 12ms/step - loss: 0.0624 - accuracy: 0.9800\n",
            "Epoch 48/200\n",
            "308/308 [==============================] - 4s 12ms/step - loss: 0.0562 - accuracy: 0.9819\n",
            "Score for fold 1: loss of 0.17365725338459015; accuracy of 96.19482755661011%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/200\n",
            "308/308 [==============================] - 6s 13ms/step - loss: 0.8793 - accuracy: 0.7467\n",
            "Epoch 2/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.3137 - accuracy: 0.9138\n",
            "Epoch 3/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.2344 - accuracy: 0.9352\n",
            "Epoch 4/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.2002 - accuracy: 0.9433\n",
            "Epoch 5/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1799 - accuracy: 0.9481\n",
            "Epoch 6/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1543 - accuracy: 0.9550\n",
            "Epoch 7/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1435 - accuracy: 0.9575\n",
            "Epoch 8/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1368 - accuracy: 0.9591\n",
            "Epoch 9/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1237 - accuracy: 0.9641\n",
            "Epoch 10/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1161 - accuracy: 0.9648\n",
            "Epoch 11/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1165 - accuracy: 0.9645\n",
            "Epoch 12/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1082 - accuracy: 0.9672\n",
            "Epoch 13/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1026 - accuracy: 0.9689\n",
            "Epoch 14/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0909 - accuracy: 0.9717\n",
            "Epoch 15/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0923 - accuracy: 0.9716\n",
            "Epoch 16/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0829 - accuracy: 0.9743\n",
            "Epoch 17/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0914 - accuracy: 0.9713\n",
            "Epoch 18/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0847 - accuracy: 0.9732\n",
            "Epoch 19/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0907 - accuracy: 0.9717\n",
            "Epoch 20/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0778 - accuracy: 0.9753\n",
            "Epoch 21/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0744 - accuracy: 0.9773\n",
            "Epoch 22/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0747 - accuracy: 0.9765\n",
            "Epoch 23/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0765 - accuracy: 0.9766\n",
            "Epoch 24/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0741 - accuracy: 0.9766\n",
            "Epoch 25/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0698 - accuracy: 0.9782\n",
            "Epoch 26/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0764 - accuracy: 0.9757\n",
            "Epoch 27/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0664 - accuracy: 0.9796\n",
            "Epoch 28/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0681 - accuracy: 0.9784\n",
            "Epoch 29/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0616 - accuracy: 0.9816\n",
            "Epoch 30/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0645 - accuracy: 0.9797\n",
            "Epoch 31/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0774 - accuracy: 0.9745\n",
            "Epoch 32/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0563 - accuracy: 0.9816\n",
            "Epoch 33/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0558 - accuracy: 0.9824\n",
            "Epoch 34/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0616 - accuracy: 0.9803\n",
            "Epoch 35/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0580 - accuracy: 0.9813\n",
            "Epoch 36/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0615 - accuracy: 0.9805\n",
            "Epoch 37/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0675 - accuracy: 0.9789\n",
            "Epoch 38/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0721 - accuracy: 0.9767\n",
            "Score for fold 2: loss of 0.15621525049209595; accuracy of 96.77321314811707%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/200\n",
            "308/308 [==============================] - 5s 13ms/step - loss: 0.9173 - accuracy: 0.7405\n",
            "Epoch 2/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.3217 - accuracy: 0.9102\n",
            "Epoch 3/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.2333 - accuracy: 0.9346\n",
            "Epoch 4/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1984 - accuracy: 0.9436\n",
            "Epoch 5/200\n",
            "308/308 [==============================] - 4s 14ms/step - loss: 0.1835 - accuracy: 0.9473\n",
            "Epoch 6/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1651 - accuracy: 0.9515\n",
            "Epoch 7/200\n",
            "308/308 [==============================] - 4s 14ms/step - loss: 0.1468 - accuracy: 0.9564\n",
            "Epoch 8/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1364 - accuracy: 0.9596\n",
            "Epoch 9/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1185 - accuracy: 0.9647\n",
            "Epoch 10/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1149 - accuracy: 0.9656\n",
            "Epoch 11/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1048 - accuracy: 0.9677\n",
            "Epoch 12/200\n",
            "308/308 [==============================] - 4s 14ms/step - loss: 0.1100 - accuracy: 0.9666\n",
            "Epoch 13/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0998 - accuracy: 0.9690\n",
            "Epoch 14/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0969 - accuracy: 0.9699\n",
            "Epoch 15/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0959 - accuracy: 0.9697\n",
            "Epoch 16/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0915 - accuracy: 0.9715\n",
            "Epoch 17/200\n",
            "308/308 [==============================] - 4s 14ms/step - loss: 0.0873 - accuracy: 0.9738\n",
            "Epoch 18/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0804 - accuracy: 0.9748\n",
            "Epoch 19/200\n",
            "308/308 [==============================] - 4s 14ms/step - loss: 0.0836 - accuracy: 0.9736\n",
            "Epoch 20/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0827 - accuracy: 0.9740\n",
            "Epoch 21/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0746 - accuracy: 0.9777\n",
            "Epoch 22/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0769 - accuracy: 0.9751\n",
            "Epoch 23/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0843 - accuracy: 0.9725\n",
            "Epoch 24/200\n",
            "308/308 [==============================] - 4s 14ms/step - loss: 0.0616 - accuracy: 0.9803\n",
            "Epoch 25/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0710 - accuracy: 0.9767\n",
            "Epoch 26/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0838 - accuracy: 0.9738\n",
            "Epoch 27/200\n",
            "308/308 [==============================] - 4s 14ms/step - loss: 0.0763 - accuracy: 0.9764\n",
            "Epoch 28/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0671 - accuracy: 0.9784\n",
            "Epoch 29/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0696 - accuracy: 0.9786\n",
            "Score for fold 3: loss of 0.11356059461832047; accuracy of 97.11821675300598%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/200\n",
            "308/308 [==============================] - 6s 13ms/step - loss: 0.8975 - accuracy: 0.7445\n",
            "Epoch 2/200\n",
            "308/308 [==============================] - 4s 14ms/step - loss: 0.3041 - accuracy: 0.9155\n",
            "Epoch 3/200\n",
            "308/308 [==============================] - 4s 14ms/step - loss: 0.2355 - accuracy: 0.9360\n",
            "Epoch 4/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1920 - accuracy: 0.9459\n",
            "Epoch 5/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1708 - accuracy: 0.9513\n",
            "Epoch 6/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1561 - accuracy: 0.9549\n",
            "Epoch 7/200\n",
            "308/308 [==============================] - 4s 14ms/step - loss: 0.1413 - accuracy: 0.9591\n",
            "Epoch 8/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1231 - accuracy: 0.9631\n",
            "Epoch 9/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1245 - accuracy: 0.9623\n",
            "Epoch 10/200\n",
            "308/308 [==============================] - 4s 14ms/step - loss: 0.1153 - accuracy: 0.9647\n",
            "Epoch 11/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1033 - accuracy: 0.9688\n",
            "Epoch 12/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1027 - accuracy: 0.9680\n",
            "Epoch 13/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0989 - accuracy: 0.9695\n",
            "Epoch 14/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0957 - accuracy: 0.9711\n",
            "Epoch 15/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0952 - accuracy: 0.9698\n",
            "Epoch 16/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0878 - accuracy: 0.9722\n",
            "Epoch 17/200\n",
            "308/308 [==============================] - 4s 14ms/step - loss: 0.0787 - accuracy: 0.9748\n",
            "Epoch 18/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0876 - accuracy: 0.9735\n",
            "Epoch 19/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0782 - accuracy: 0.9753\n",
            "Epoch 20/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0878 - accuracy: 0.9730\n",
            "Epoch 21/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0844 - accuracy: 0.9728\n",
            "Epoch 22/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0672 - accuracy: 0.9786\n",
            "Epoch 23/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0667 - accuracy: 0.9790\n",
            "Epoch 24/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0850 - accuracy: 0.9734\n",
            "Epoch 25/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0668 - accuracy: 0.9794\n",
            "Epoch 26/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0625 - accuracy: 0.9815\n",
            "Epoch 27/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0698 - accuracy: 0.9780\n",
            "Epoch 28/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0731 - accuracy: 0.9770\n",
            "Epoch 29/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0668 - accuracy: 0.9787\n",
            "Epoch 30/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0670 - accuracy: 0.9785\n",
            "Epoch 31/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0661 - accuracy: 0.9790\n",
            "Score for fold 4: loss of 0.14728029072284698; accuracy of 96.47894501686096%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/200\n",
            "308/308 [==============================] - 6s 13ms/step - loss: 0.9164 - accuracy: 0.7390\n",
            "Epoch 2/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.3074 - accuracy: 0.9159\n",
            "Epoch 3/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.2348 - accuracy: 0.9353\n",
            "Epoch 4/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.2008 - accuracy: 0.9447\n",
            "Epoch 5/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1708 - accuracy: 0.9512\n",
            "Epoch 6/200\n",
            "308/308 [==============================] - 4s 14ms/step - loss: 0.1603 - accuracy: 0.9531\n",
            "Epoch 7/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1490 - accuracy: 0.9560\n",
            "Epoch 8/200\n",
            "308/308 [==============================] - 4s 14ms/step - loss: 0.1309 - accuracy: 0.9607\n",
            "Epoch 9/200\n",
            "308/308 [==============================] - 4s 14ms/step - loss: 0.1261 - accuracy: 0.9614\n",
            "Epoch 10/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1185 - accuracy: 0.9645\n",
            "Epoch 11/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1090 - accuracy: 0.9664\n",
            "Epoch 12/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.1070 - accuracy: 0.9672\n",
            "Epoch 13/200\n",
            "308/308 [==============================] - 4s 14ms/step - loss: 0.0982 - accuracy: 0.9700\n",
            "Epoch 14/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0978 - accuracy: 0.9690\n",
            "Epoch 15/200\n",
            "308/308 [==============================] - 4s 14ms/step - loss: 0.0926 - accuracy: 0.9719\n",
            "Epoch 16/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0856 - accuracy: 0.9735\n",
            "Epoch 17/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0874 - accuracy: 0.9725\n",
            "Epoch 18/200\n",
            "308/308 [==============================] - 4s 14ms/step - loss: 0.0871 - accuracy: 0.9734\n",
            "Epoch 19/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0831 - accuracy: 0.9735\n",
            "Epoch 20/200\n",
            "308/308 [==============================] - 4s 14ms/step - loss: 0.0793 - accuracy: 0.9754\n",
            "Epoch 21/200\n",
            "308/308 [==============================] - 4s 14ms/step - loss: 0.0842 - accuracy: 0.9738\n",
            "Epoch 22/200\n",
            "308/308 [==============================] - 4s 14ms/step - loss: 0.0724 - accuracy: 0.9775\n",
            "Epoch 23/200\n",
            "308/308 [==============================] - 4s 14ms/step - loss: 0.0713 - accuracy: 0.9781\n",
            "Epoch 24/200\n",
            "308/308 [==============================] - 4s 14ms/step - loss: 0.0704 - accuracy: 0.9773\n",
            "Epoch 25/200\n",
            "308/308 [==============================] - 4s 14ms/step - loss: 0.0716 - accuracy: 0.9773\n",
            "Epoch 26/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0625 - accuracy: 0.9807\n",
            "Epoch 27/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0731 - accuracy: 0.9773\n",
            "Epoch 28/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0680 - accuracy: 0.9786\n",
            "Epoch 29/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0660 - accuracy: 0.9790\n",
            "Epoch 30/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0638 - accuracy: 0.9797\n",
            "Epoch 31/200\n",
            "308/308 [==============================] - 4s 13ms/step - loss: 0.0668 - accuracy: 0.9780\n",
            "Score for fold 5: loss of 0.11257883906364441; accuracy of 97.22983241081238%\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " transformer_block_4 (Transf  (None, 6, 128)           297344    \n",
            " ormerBlock)                                                     \n",
            "                                                                 \n",
            " global_average_pooling1d_4   (None, 128)              0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 20)                2580      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 316,436\n",
            "Trainable params: 316,436\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Define per-fold score containers \n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "num_folds = 5\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "fold_no = 1\n",
        "input_shape = x_train.shape[1:]\n",
        "input = Input(shape=x_train.shape[1:])\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "  model = keras.Sequential()\n",
        "  model.add(layers.Input(shape= x_train.shape[1:]))\n",
        "  model.add(TransformerBlock(embed_dim, num_heads, ff_dim))\n",
        "  model.add(layers.GlobalAveragePooling1D())\n",
        "  model.add(layers.Dropout(0.2))\n",
        "  model.add(layers.Dense(ff_dim, activation='relu'))\n",
        "  model.add(layers.Dropout(0.2))\n",
        "  model.add(layers.Dense(20, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  history = model.fit(inputs[train], targets[train], epochs=200, verbose=True, batch_size=128,callbacks=[keras.callbacks.EarlyStopping(monitor='loss', patience=5,restore_best_weights=True)])\n",
        "    # Generate generalization metrics\n",
        "  scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaWCy3YG9Vlu",
        "outputId": "8654ead9-bd29-43a9-a445-b6e036c6cd5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.17365725338459015 - Accuracy: 96.19482755661011%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.15621525049209595 - Accuracy: 96.77321314811707%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.11356059461832047 - Accuracy: 97.11821675300598%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.14728029072284698 - Accuracy: 96.47894501686096%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.11257883906364441 - Accuracy: 97.22983241081238%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 96.7590069770813 (+- 0.3866893294387845)\n",
            "> Loss: 0.1406584456562996\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Average scores \n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "KFOLD_TCN_dataset_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}