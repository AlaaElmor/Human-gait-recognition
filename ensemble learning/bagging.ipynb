{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bagging.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "sIoK2Y9u501o"
      },
      "outputs": [],
      "source": [
        "from numpy import save, load\n",
        "from pandas import read_csv\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras import Model\n",
        "from keras.layers import Conv1D, SpatialDropout1D\n",
        "from keras.layers import Convolution1D, Dense,Activation\n",
        "from keras.models import Input, Model\n",
        "from keras.layers import LSTM, Conv1D, concatenate,GlobalMaxPooling1D,GlobalAveragePooling1D,TimeDistributed, MaxPooling1D\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "import keras.layers\n",
        "from tensorflow import keras\n",
        "from keras import optimizers\n",
        "from keras.layers import Activation, Lambda\n",
        "from keras.layers import Convolution1D, Dense\n",
        "from keras.models import Input, Model\n",
        "from typing import List, Tuple\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from sklearn.ensemble import BaggingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/'  #change dir to your project folder\n",
        "\n",
        "import numpy as np\n",
        "x_train = np.load('gdrive/My Drive/d1/trainX.npy')\n",
        "y_train = np.load('gdrive/My Drive/d1/trainy.npy')\n",
        "x_test = np.load('gdrive/My Drive/d1/testX.npy')\n",
        "y_test = np.load('gdrive/My Drive/d1/testy.npy')\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
        "y_train = (np.argmax(y_train, axis=1)).reshape(-1,1)\n",
        "y_test = (np.argmax(y_test, axis=1)).reshape(-1,1)\n",
        "nsamples, nx, ny = x_train.shape\n",
        "x_train = x_train.reshape((nsamples,nx*ny))\n",
        "nsamples, nx, ny = x_test.shape\n",
        "x_test = x_test.reshape((nsamples,nx*ny))\n",
        "print(x_train.shape, x_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adLw1yd_5_dL",
        "outputId": "8f0fa007-eacd-49fc-9742-1c18ee4e0e09"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Mounted at /content/gdrive\n",
            "(33104, 128, 6) (33104, 118) (3740, 128, 6) (3740, 118)\n",
            "(33104, 768) (3740, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=0.20, random_state=42)\n",
        "#y_validation = (np.argmax(y_validation, axis=1)).reshape(-1,1)"
      ],
      "metadata": {
        "id": "QgT9idPy6DTh"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def channel_normalization(x):\n",
        "    max_values = K.max(K.abs(x), 2, keepdims=True) + 1e-5\n",
        "    out = x / max_values\n",
        "    return out\n",
        "\n",
        "def residual_block(x, s, i, activation, nb_filters, kernel_size, padding, dropout_rate=0, name=''):\n",
        "    original_x = x\n",
        "    conv = Conv1D(filters=nb_filters, kernel_size=kernel_size,\n",
        "                  dilation_rate=i, padding=padding,\n",
        "                  name=name + '_dilated_conv_%d_tanh_s%d' % (i, s))(x)\n",
        "    if activation == 'norm_relu':\n",
        "        x = Activation('relu')(conv)\n",
        "        x = Lambda(channel_normalization)(x)\n",
        "    else:\n",
        "        x = Activation(activation)(conv)\n",
        "\n",
        "    x = SpatialDropout1D(dropout_rate, name=name + '_spatial_dropout1d_%d_s%d_%f' % (i, s, dropout_rate))(x)\n",
        "\n",
        "    # 1x1 conv.\n",
        "    x = Convolution1D(nb_filters, 1, padding='same')(x)\n",
        "    res_x = keras.layers.add([original_x, x])\n",
        "    return res_x, x"
      ],
      "metadata": {
        "id": "b4CTb60I6HON"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TCN:\n",
        "\n",
        "    def __init__(self,\n",
        "                 nb_filters=64,\n",
        "                 kernel_size=2,\n",
        "                 nb_stacks=1,\n",
        "                 dilations=None,\n",
        "                 activation='norm_relu',\n",
        "                 padding='causal',\n",
        "                 use_skip_connections=True,\n",
        "                 dropout_rate=0.0,\n",
        "                 return_sequences=True,\n",
        "                 name='tcn'):\n",
        "        self.name = name\n",
        "        self.return_sequences = return_sequences\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.use_skip_connections = use_skip_connections\n",
        "        self.activation = activation\n",
        "        self.dilations = dilations\n",
        "        self.nb_stacks = nb_stacks\n",
        "        self.kernel_size = kernel_size\n",
        "        self.nb_filters = nb_filters\n",
        "        self.padding = padding\n",
        "        \n",
        "        if padding != 'causal' and padding != 'same':\n",
        "            raise ValueError(\"Only 'causal' or 'same' paddings are compatible for this layer.\")\n",
        "\n",
        "        if not isinstance(nb_filters, int):\n",
        "            print('An interface change occurred after the version 2.1.2.')\n",
        "            print('Before: tcn.TCN(i, return_sequences=False, ...)')\n",
        "            print('Now should be: tcn.TCN(return_sequences=False, ...)(i)')\n",
        "            print('Second solution is to pip install keras-tcn==2.1.2 to downgrade.')\n",
        "            raise Exception()\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        if self.dilations is None:\n",
        "            self.dilations = [1, 2, 4, 8, 16, 32]\n",
        "        x = inputs\n",
        "        x = Convolution1D(self.nb_filters, 1, padding=self.padding, name=self.name + '_initial_conv')(x)\n",
        "        skip_connections = []\n",
        "        for s in range(self.nb_stacks):\n",
        "            for i in self.dilations:\n",
        "                x, skip_out = residual_block(x, s, i, self.activation, self.nb_filters,\n",
        "                                             self.kernel_size, self.padding, self.dropout_rate, name=self.name)\n",
        "                skip_connections.append(skip_out)\n",
        "        if self.use_skip_connections:\n",
        "            x = keras.layers.add(skip_connections)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "        if not self.return_sequences:\n",
        "            output_slice_index = -1\n",
        "            x = Lambda(lambda tt: tt[:, output_slice_index, :])(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "7cvQlCYC6KVA"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = x_train.shape[1:]\n",
        "input = Input(shape=x_train.shape)\n",
        "x = SpatialDropout1D(0.2)(input)\n",
        "x = TCN(128,dilations = [1, 2, 4, 8, 16],kernel_size = 3, return_sequences=True, name = 'tnc1')(x)\n",
        "x = TCN(128,dilations = [1, 2, 4, 8, 16],kernel_size = 3, return_sequences=True, name = 'tnc2')(x)\n",
        "max_pool = GlobalMaxPooling1D()(x)\n",
        "x = Dense(128, activation=\"relu\")(max_pool)\n",
        "x = Dropout(0.2)(x)\n",
        "output = Dense(1, activation=\"softmax\")(x)    \n",
        "model = Model(inputs=input, outputs=output)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]"
      ],
      "metadata": {
        "id": "kb-OKXJ56O0H"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = x_train.shape[-1]  # Embedding size for each token\n",
        "num_heads = 4  # Number of attention heads\n",
        "ff_dim = x_train.shape[-1]  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads,\n",
        "                                             key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)  # self-attention layer\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)  # layer norm\n",
        "        ffn_output = self.ffn(out1)  #feed-forward layer\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)  # layer norm\n",
        "model1 = keras.Sequential()\n",
        "model1.add(layers.Input(shape= x_train.shape))\n",
        "model1.add(TransformerBlock(embed_dim, num_heads, ff_dim))\n",
        "model1.add(layers.GlobalAveragePooling1D(data_format=\"channels_first\"))\n",
        "model1.add(layers.Dropout(0.2))\n",
        "model1.add(layers.Dense(ff_dim, activation='relu'))\n",
        "model1.add(layers.Dropout(0.2))\n",
        "model1.add(layers.Dense(118, activation='softmax'))\n",
        "\n",
        "model1.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "model1.summary()\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUv1t09U6Xy4",
        "outputId": "9d976e55-647f-44ca-9eab-cee505974c15"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_37\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " transformer_block_19 (Trans  (None, 26483, 768)       10631424  \n",
            " formerBlock)                                                    \n",
            "                                                                 \n",
            " global_average_pooling1d_3   (None, 26483)            0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dropout_50 (Dropout)        (None, 26483)             0         \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 768)               20339712  \n",
            "                                                                 \n",
            " dropout_51 (Dropout)        (None, 768)               0         \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 118)               90742     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,061,878\n",
            "Trainable params: 31,061,878\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_models():\n",
        "\tmodels = list()\n",
        "\tmodels.append(('tcn1', model))\n",
        "\tmodels.append(('transformer', model1))\n",
        "\treturn models"
      ],
      "metadata": {
        "id": "R7ADUCyn6YmT"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models= get_models()\n",
        "clf = BaggingClassifier(base_estimator= model, n_estimators=2, random_state=0).get_params(deep=True)\n",
        "clf.fit(x_train, y_train)\n",
        "yhat=clf.predict(x_test)\n",
        "score = accuracy_score(y_test, yhat)\n",
        "print('score: %.3f' % (score*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "6cEbhDSO63Ck",
        "outputId": "a8a9de0c-6439-4831-c4cb-d237c275f052"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-71b2ff98ae1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mget_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaggingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'fit'"
          ]
        }
      ]
    }
  ]
}